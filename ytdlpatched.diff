diff --git a/.devcontainer/devcontainer.json b/.devcontainer/devcontainer.json
new file mode 100644
index 000000000..77e596cb0
--- /dev/null
+++ b/.devcontainer/devcontainer.json
@@ -0,0 +1,28 @@
+{
+  "name": "Python 3",
+  "image": "nao20010128nao/ytdl-patched:codespaces",
+  "settings": {
+    "terminal.integrated.shell.linux": "/bin/bash",
+    "python.pythonPath": "/home/lesmi/.linuxbrew/bin/python3.9",
+    "python.linting.enabled": true,
+    "python.linting.pylintEnabled": true,
+    "python.formatting.autopep8Path": "/usr/local/py-utils/bin/autopep8",
+    "python.formatting.blackPath": "/usr/local/py-utils/bin/black",
+    "python.formatting.yapfPath": "/usr/local/py-utils/bin/yapf",
+    "python.linting.banditPath": "/usr/local/py-utils/bin/bandit",
+    "python.linting.flake8Path": "/usr/local/py-utils/bin/flake8",
+    "python.linting.mypyPath": "/usr/local/py-utils/bin/mypy",
+    "python.linting.pycodestylePath": "/usr/local/py-utils/bin/pycodestyle",
+    "python.linting.pydocstylePath": "/usr/local/py-utils/bin/pydocstyle",
+    "python.linting.pylintPath": "/usr/local/py-utils/bin/pylint",
+    "editor.codeLens": false,
+    "editor.fontSize": 12,
+    "markdown.preview.fontSize": 12
+  },
+  "extensions": ["ms-python.python", "ms-python.vscode-pylance", "ms-azuretools.vscode-docker", "eamodio.gitlens", "ms-vscode.powershell"],
+  "remoteUser": "codespace",
+  "overrideCommand": false,
+  "mounts": ["source=/var/run/docker.sock,target=/var/run/docker-host.sock,type=bind"],
+  "runArgs": ["--privileged", "--security-opt", "seccomp=unconfined"],
+  "postCreateCommand": "./devscripts/init_hooks.sh"
+}
diff --git a/.github/workflows/build.yml b/.github/workflows/build.yml
new file mode 100644
index 000000000..8a9838e64
--- /dev/null
+++ b/.github/workflows/build.yml
@@ -0,0 +1,581 @@
+name: 'URGENT: Please respond #1126012 before stopping Actions!'
+# name: Build Patched YTDL
+
+on:
+  push:
+    paths:
+      - youtube_dl/**
+      - test/**
+      - docker/**
+      - .github/workflows/build.yml
+      - Makefile
+      - setup.py
+    branches-ignore:
+      - gh-pages
+  pull_request:
+    types: [ opened, synchronize, ready_for_review ]
+  workflow_dispatch:
+    inputs:
+
+env:
+  python_version: 3.9
+  windows_icon: red
+  ZOPFLI_ITERATIONS: 40
+
+jobs:
+  prepare:
+    runs-on: ubuntu-latest
+    outputs:
+      latest_version: ${{ steps.prepare.outputs.latest_version }}
+      latest_version_normalized: ${{ steps.prepare.outputs.latest_version_normalized }}
+      latest_version_numeric: ${{ steps.prepare.outputs.latest_version_numeric }}
+    steps:
+      - uses: actions/checkout@v2
+        with:
+          fetch-depth: 0
+      - name: Set up Python ${{ env.python_version }}
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ env.python_version }}
+      - name: Install deps
+        run: |
+          sudo ln -fs /usr/share/zoneinfo/Etc/UTC /etc/localtime
+          sudo apt update
+          sudo env DEBIAN_FRONTEND=noninteractive apt install -y tzdata
+      - name: Prepare files
+        id: prepare
+        run: |
+          mkdir artifacts
+          export NOW_RAW="$(date)"
+          export LATEST_VERSION="$(date --date="${NOW_RAW}" +'%Y.%m.%d.%s')"
+          echo "${LATEST_VERSION}" > artifacts/LATEST_VERSION
+          echo "::set-output name=latest_version::${LATEST_VERSION}"
+          echo "::set-output name=latest_version_normalized::$(python3 devscripts/normalize_version.py "${LATEST_VERSION}")"
+          echo "::set-output name=latest_version_numeric::$(date --date="${NOW_RAW}" +'%s')"
+
+          echo "from __future__ import unicode_literals
+          __version__ = '${LATEST_VERSION}'
+          " > youtube_dl/version.py
+      - name: Upload artifacts for the next steps
+        uses: actions/upload-artifact@v2
+        with:
+          name: ytdl-base
+          path: |
+            youtube_dl/version.py
+            youtube_dl/build_config.py
+            artifacts/
+  build-linux:
+    runs-on: ubuntu-latest
+    needs: prepare
+    steps:
+      - uses: actions/checkout@v2
+      - name: Download base
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-base
+      - name: Create buildconfig.py
+        run: python ./devscripts/make_buildconfig.py
+      - name: Set up Python ${{ env.python_version }}
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ env.python_version }}
+      - name: Install deps
+        run: |
+          sudo ln -fs /usr/share/zoneinfo/Etc/UTC /etc/localtime
+          sudo apt update
+          sudo env DEBIAN_FRONTEND=noninteractive apt install -y curl zip make git gawk pandoc tzdata p7zip-full
+          brew install advancecomp
+      - name: Build patched YTDL for Linux
+        run: |
+          make youtube-dl youtube-dl.tar.gz
+          mv youtube-dl artifacts/
+          mv youtube-dl.tar.gz artifacts/
+      - name: Upload artifacts for the next steps
+        uses: actions/upload-artifact@v2
+        with:
+          name: ytdl-linux
+          # README.md (or any other files in the repo) is required
+          # to prevent upload-artifacts to find LCA
+          path: |
+            artifacts/
+            README.md
+  build-wheel:
+    runs-on: ubuntu-latest
+    needs: prepare
+    steps:
+      - uses: actions/checkout@v2
+      - name: Download base
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-base
+      - name: Set up Python ${{ env.python_version }}
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ env.python_version }}
+      - name: Install deps
+        run: pip3 install -U pip build wheel setuptools
+      - name: Build wheel
+        run: python3 -m build
+      - name: Upload artifacts for the next steps
+        uses: actions/upload-artifact@v2
+        with:
+          name: ytdl-wheel
+          # README.md (or any other files in the repo) is required
+          # to prevent upload-artifacts to find LCA
+          path: |
+            dist/
+            README.md
+  # build-eggs:
+  #   runs-on: ${{ matrix.os }}
+  #   needs: prepare
+  #   strategy:
+  #     fail-fast: false
+  #     matrix:
+  #       os: [
+  #         ubuntu-latest,
+  #         windows-latest,
+  #         macos-latest,
+  #       ]
+  #       python_version: [
+  #         2.7,
+  #         3.5, 3.6, 3.7, 3.8, 3.9
+  #       ]
+  #   steps:
+  #     - uses: actions/checkout@v2
+  #     - name: Download base
+  #       uses: actions/download-artifact@v2
+  #       with:
+  #         name: ytdl-base
+  #     - name: Set up Python ${{ matrix.python_version }}
+  #       uses: actions/setup-python@v2
+  #       with:
+  #         python-version: ${{ matrix.python_version }}
+  #     - name: Install deps
+  #       run: python3 -m pip install -U pip build wheel setuptools
+  #     - name: Build egg
+  #       run: python3 setup.py bdist_egg
+  #     - name: Upload artifacts for the next steps
+  #       uses: actions/upload-artifact@v2
+  #       with:
+  #         name: ytdl-egg-${{ matrix.python_version }}-${{ runner.os }}
+  #         # README.md (or any other files in the repo) is required
+  #         # to prevent upload-artifacts to find LCA
+  #         path: |
+  #           dist/
+  #           README.md
+  build-windows:
+    runs-on: windows-latest
+    needs: prepare
+    strategy:
+      fail-fast: false
+      matrix:
+        icon: [red, white]
+        # :tada: https://github.com/pyinstaller/pyinstaller/releases/tag/v4.2
+        # builder: [PyInstaller, py2exe]
+        # remove_tests: ['-notests', '']  # yes and no
+        builder: [PyInstaller]
+        remove_tests: ['']
+    steps:
+      - uses: actions/checkout@v2
+      - name: Download base
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-base
+      - name: Create buildconfig.py
+        run: python ./devscripts/make_buildconfig.py variant=${{ matrix.icon }}
+      - name: Set up Python ${{ env.python_version }}
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ env.python_version }}
+      - name: Install deps
+        run: pip install -U ${{ matrix.builder }} zopflipy websocket_client pyyaml python-dateutil
+      - name: Install UPX
+        run: choco install upx
+      - name: Remove unneeded tests in InfoExtractor
+        shell: bash
+        run: python3 ./devscripts/reduce_code.py youtube_dl/extractor/*.py
+        if: ${{ matrix.remove_tests == '-notests' }}
+      - name: Build patched YTDL for Windows
+        shell: pwsh
+        run: pwsh ./devscripts/build_windows_exe.ps1 ${{ matrix.builder }} ${{ matrix.icon }}
+      - name: Compress generated EXE
+        run: upx --lzma --best artifacts/youtube-dl.exe
+        if: ${{ matrix.builder == 'PyInstaller' }}
+        continue-on-error: true
+      - name: Upload artifacts for the next steps
+        uses: actions/upload-artifact@v2
+        with:
+          name: "ytdl-windows-${{ matrix.icon }}-${{ matrix.builder }}${{ matrix.remove_tests }}"
+          # README.md (or any other files in the repo) is required
+          # to prevent upload-artifacts to find LCA
+          path: |
+            artifacts/
+            README.md
+  prepare-artifacts:
+    runs-on: ubuntu-latest
+    needs: [prepare, build-linux, build-windows]
+    if: ${{ github.ref == 'refs/heads/master' }}
+    outputs:
+      filehash_bin: ${{ steps.hash-and-versions.outputs.filehash_bin }}
+      filehash_exe: ${{ steps.hash-and-versions.outputs.filehash_exe }}
+      filehash_tar: ${{ steps.hash-and-versions.outputs.filehash_tar }}
+    steps:
+      - name: Download builds for Linux
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-linux
+      - name: Download builds for Windows
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-windows-${{ env.windows_icon }}-PyInstaller
+      - name: Install deps
+        run: |
+          sudo apt update
+          sudo env DEBIAN_FRONTEND=noninteractive apt install -y jq
+      - name: Prepare artifacts
+        id: hash-and-versions
+        run: |
+          FILEHASH_BIN="$(sha256sum artifacts/youtube-dl        | awk '{print $1}')"
+          FILEHASH_TAR="$(sha256sum artifacts/youtube-dl.tar.gz | awk '{print $1}')"
+          FILEHASH_EXE="$(sha256sum artifacts/youtube-dl.exe    | awk '{print $1}')"
+          echo "::set-output name=filehash_bin::${FILEHASH_BIN}"
+          echo "::set-output name=filehash_tar::${FILEHASH_TAR}"
+          echo "::set-output name=filehash_exe::${FILEHASH_EXE}"
+          echo '{"versions":{}}' | jq ".latest=\"${{ needs.prepare.outputs.latest_version }}\"" \
+            | jq ".versions[\"${{ needs.prepare.outputs.latest_version }}\"].bin=[\"https://github.com/nao20010128nao/ytdl-patched/releases/download/${{ needs.prepare.outputs.latest_version_numeric }}/youtube-dl\",\"${FILEHASH_BIN}\"]" \
+            | jq ".versions[\"${{ needs.prepare.outputs.latest_version }}\"].tar=[\"https://github.com/nao20010128nao/ytdl-patched/releases/download/${{ needs.prepare.outputs.latest_version_numeric }}/youtube-dl.tar.gz\",\"${FILEHASH_TAR}\"]" \
+            | jq ".versions[\"${{ needs.prepare.outputs.latest_version }}\"].exe=[\"https://github.com/nao20010128nao/ytdl-patched/releases/download/${{ needs.prepare.outputs.latest_version_numeric }}/youtube-dl-${{ env.windows_icon }}.exe\",\"${FILEHASH_EXE}\"]" \
+            | jq ".versions[\"${{ needs.prepare.outputs.latest_version }}\"][\"exe-${{ env.windows_icon }}\"]=[\"https://github.com/nao20010128nao/ytdl-patched/releases/download/${{ needs.prepare.outputs.latest_version_numeric }}/youtube-dl-${{ env.windows_icon }}.exe\",\"${FILEHASH_EXE}\"]" \
+            | tee artifacts/versions.json
+      - name: Upload artifacts for the next steps
+        uses: actions/upload-artifact@v2
+        with:
+          name: ytdl-artifacts
+          # README.md (or any other files in the repo) is required
+          # to prevent upload-artifacts to find LCA
+          path: |
+            artifacts/
+            README.md
+
+  upload-gh-pages:
+    needs: prepare-artifacts
+    runs-on: ubuntu-latest
+    if: ${{ github.event_name != 'pull_request' && github.ref == 'refs/heads/master' }}
+    steps:
+      - uses: actions/checkout@v2
+      - name: Download artifacts
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-artifacts
+      - name: Deploy to gh-pages
+        uses: JamesIves/github-pages-deploy-action@4.1.1
+        with:
+          token: ${{ secrets.GITHUB_TOKEN }}
+          branch: gh-pages
+          folder: artifacts
+          single-commit: true
+  upload-homebrew:
+    needs: [prepare, prepare-artifacts]
+    runs-on: ubuntu-latest
+    if: ${{ github.event_name != 'pull_request' && github.ref == 'refs/heads/master' }}
+    env:
+      FILEHASH_TAR: ${{ needs.prepare-artifacts.outputs.filehash_tar }}
+      VERSION: ${{ needs.prepare.outputs.latest_version_numeric }}
+    steps:
+      - name: Upload to my homebrew repository
+        run: |
+          git config --global user.name nao20010128nao
+          git config --global user.email nao20010128@gmail.com
+          git clone https://nao20010128nao:${{ secrets.GH_PAT }}@github.com/nao20010128nao/homebrew-my.git
+          cd homebrew-my
+          echo $FILEHASH_TAR $VERSION
+          sed -E -i "s/sha256 \"[0-9a-f]*\"/sha256 \"$FILEHASH_TAR\"/g" Formula/ytdl-patched.rb
+          sed -E -i "s/version \".*\"/version \"$VERSION\"/g" Formula/ytdl-patched.rb
+          sed -E -i "s^url \".*\"^url \"https://github.com/nao20010128nao/ytdl-patched/releases/download/$VERSION/youtube-dl.tar.gz\"^g" Formula/ytdl-patched.rb
+          git add .
+          git commit -m "ytdl-patched: $VERSION"
+          git push
+  upload-releases:
+    needs: [prepare, prepare-artifacts]
+    runs-on: ubuntu-latest
+    if: ${{ github.event_name != 'pull_request' && github.ref == 'refs/heads/master' }}
+    steps:
+      - name: Download artifacts
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-artifacts
+      - name: Create release
+        id: create_release
+        uses: actions/create-release@v1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        with:
+          tag_name: ${{ needs.prepare.outputs.latest_version_numeric }}
+          release_name: ${{ needs.prepare.outputs.latest_version }}
+          draft: false
+          prerelease: false
+          body: |
+            What to download?
+            - for Linux/macOS - `youtube-dl` (requires Python 3.x installed)
+            - for Windows (no Python 3.x required)
+                - red icon - `...-red.exe` (**default** in -U)
+                - white icon - `...-white.exe`
+      - name: Upload bin
+        id: upload-1
+        uses: actions/upload-release-asset@v1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        with:
+          upload_url: ${{ steps.create_release.outputs.upload_url }} 
+          asset_path: ./artifacts/youtube-dl
+          asset_name: youtube-dl
+          asset_content_type: application/zip
+      - name: Upload source tar
+        id: upload-2
+        uses: actions/upload-release-asset@v1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        with:
+          upload_url: ${{ steps.create_release.outputs.upload_url }} 
+          asset_path: ./artifacts/youtube-dl.tar.gz
+          asset_name: youtube-dl.tar.gz
+          asset_content_type: application/gzip
+      - name: Download builds for Windows (red icon)
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-windows-red-PyInstaller
+      - name: Upload exe
+        id: upload-3
+        uses: actions/upload-release-asset@v1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        with:
+          upload_url: ${{ steps.create_release.outputs.upload_url }} 
+          asset_path: ./artifacts/youtube-dl.exe
+          asset_name: youtube-dl-red.exe
+          asset_content_type: application/vnd.microsoft.portable-executable
+      - name: Download builds for Windows (white icon)
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-windows-white-PyInstaller
+      - name: Upload exe
+        id: upload-4
+        uses: actions/upload-release-asset@v1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        with:
+          upload_url: ${{ steps.create_release.outputs.upload_url }} 
+          asset_path: ./artifacts/youtube-dl.exe
+          asset_name: youtube-dl-white.exe
+          asset_content_type: application/vnd.microsoft.portable-executable
+      - name: Download wheel
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-wheel
+      - name: Upload wheel
+        id: upload-5
+        uses: actions/upload-release-asset@v1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        with:
+          upload_url: ${{ steps.create_release.outputs.upload_url }} 
+          asset_path: ./dist/youtube_dl-${{ needs.prepare.outputs.latest_version_normalized }}-py2.py3-none-any.whl
+          asset_name: youtube_dl-${{ needs.prepare.outputs.latest_version }}-py2.py3-none-any.whl
+          asset_content_type: application/zip
+      - name: Upload wheel tar
+        id: upload-6
+        uses: actions/upload-release-asset@v1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+        with:
+          upload_url: ${{ steps.create_release.outputs.upload_url }} 
+          asset_path: ./dist/youtube_dl-${{ needs.prepare.outputs.latest_version_normalized }}.tar.gz
+          asset_name: youtube_dl-${{ needs.prepare.outputs.latest_version }}-wheel.tar.gz
+          asset_content_type: application/gzip
+
+  # upload-pull-request:
+  #   needs: prepare-artifacts
+  #   runs-on: ubuntu-latest
+  #   if: ${{ github.event_name == 'pull_request' }}
+  #   steps:
+  #     - name: Download artifacts
+  #       uses: actions/download-artifact@v2
+  #       with:
+  #         name: ytdl-artifacts
+  #     - name: Download builds for Windows (red icon)
+  #       uses: actions/download-artifact@v2
+  #       with:
+  #         name: ytdl-windows-red-PyInstaller
+  #     - name: Download builds for Windows (white icon)
+  #       uses: actions/download-artifact@v2
+  #       with:
+  #         name: ytdl-windows-white-PyInstaller
+  #     - name: Download wheel
+  #       uses: actions/download-artifact@v2
+  #       with:
+  #         name: ytdl-wheel
+  #     - uses: actions/github-script@v4
+  #       with:
+  #         github-token: ${{secrets.GITHUB_TOKEN}}
+  #         script: |
+  #           const texts = ['Built binary for this PR can be found here:']
+  #           texts.append(`- Commit hash: \`${context.sha}\``)
+  #           texts.append(`- Sorry, but it's WIP`)
+  #           await github.issues.createComment({
+  #             issue_number: context.issue.number,
+  #             owner: context.repo.owner,
+  #             repo: context.repo.repo,
+  #             body: texts.join('\n'),
+  #           })
+  # pull-request-failed:
+  #   runs-on: ubuntu-latest
+  #   needs: [prepare, prepare-artifacts, build-windows, build-linux]
+  #   if: ${{ github.event_name == 'pull_request' && failure() }}
+  #   steps:
+  #     - uses: actions/github-script@v4
+  #       with:
+  #         github-token: ${{secrets.GITHUB_TOKEN}}
+  #         script: |
+  #           const texts = ['Build Failed!']
+  #           texts.append(`- Commit hash: \`${context.sha}\``)
+  #           await github.issues.createComment({
+  #             issue_number: context.issue.number,
+  #             owner: context.repo.owner,
+  #             repo: context.repo.repo,
+  #             body: texts.join('\n'),
+  #           })
+
+  binary-test:
+    needs: [build-linux, build-windows]
+    runs-on: ${{ matrix.environ.os }}
+    continue-on-error: true
+    strategy:
+      fail-fast: false
+      matrix:
+        environ: [
+          { os: "ubuntu-20.04", artifact: linux },
+          { os: "ubuntu-18.04", artifact: linux },
+
+          # { os: "macos-11.0", artifact: linux },
+          # { os: "macos-10.15", artifact: linux },
+
+          { os: "windows-2016", artifact: windows-white-PyInstaller },
+          # { os: "windows-2016", artifact: windows-white-py2exe },
+          { os: "windows-2019", artifact: windows-white-PyInstaller },
+          # { os: "windows-2019", artifact: windows-white-py2exe },
+        ]
+        python_version: ["3.9", "3.8", "3.7"]
+    steps:
+      - name: Download artifacts
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-${{ matrix.environ.artifact }}
+      - name: Set up Python ${{ matrix.python_version }}
+        uses: actions/setup-python@v1
+        if: ${{ startsWith(runner.os, 'ubuntu') || startsWith(runner.os, 'macos') }}
+        with:
+          python-version: ${{ matrix.python_version }}
+      - name: youtube-dl --help and --version
+        shell: bash
+        run: |
+          chmod a+x ./artifacts/youtube-dl || true
+          ./artifacts/youtube-dl --help
+          ./artifacts/youtube-dl --version
+      - name: Download some videos
+        shell: bash
+        continue-on-error: true
+        run: |
+          ./artifacts/youtube-dl \
+              https://www.youtube.com/watch?v=XEY7UQJxw-o \
+              https://twitter.com/twetchapp/status/1311686520793829376
+  docker-linux:
+    runs-on: ubuntu-latest
+    needs: [prepare, build-linux]
+    continue-on-error: true
+    if: ${{ github.event_name != 'pull_request' && github.ref == 'refs/heads/master' }}
+    steps:
+      - uses: actions/checkout@v2
+      - name: Download artifacts
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-linux
+      - run: ls artifacts/
+      - name: Login
+        uses: docker/login-action@v1
+        with:
+          username: nao20010128nao
+          password: ${{ secrets.DHUB_TOKEN }}
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v1
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v1
+      - name: Build and Push (alpine)
+        uses: docker/build-push-action@v2
+        with:
+          push: true
+          platforms: linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64,linux/i386,linux/ppc64le
+          file: ./docker/linux.Dockerfile
+          context: .
+          build-args: |
+            base_tag=3-alpine
+          tags: |
+            nao20010128nao/ytdl-patched:latest
+            nao20010128nao/ytdl-patched:alpine
+            nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}
+            nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}-alpine
+      - name: Build and Push (slim)
+        uses: docker/build-push-action@v2
+        with:
+          push: true
+          platforms: linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64,linux/i386,linux/ppc64le
+          file: ./docker/linux.Dockerfile
+          context: .
+          build-args: |
+            base_tag=3-slim
+          tags: |
+            nao20010128nao/ytdl-patched:slim
+            nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}-slim
+  docker-windows:
+    runs-on: ${{ matrix.windows.os }}
+    needs: [prepare, build-windows]
+    continue-on-error: true
+    if: ${{ github.event_name != 'pull_request' && github.ref == 'refs/heads/master' }}
+    strategy:
+      matrix:
+        windows: [
+          {os: windows-2019, tag: '1809'},
+          {os: windows-2016, tag: ltsc2016},
+        ]
+    steps:
+      - uses: actions/checkout@v2
+      - name: Download artifacts
+        uses: actions/download-artifact@v2
+        with:
+          name: ytdl-windows-${{ env.windows_icon }}-PyInstaller
+      - run: ls artifacts/
+      - name: Login
+        run: docker login --username nao20010128nao --password ${{ secrets.DHUB_TOKEN }}
+      - name: Build and Push (1809)
+        if: ${{ matrix.windows.tag == '1809' }}
+        run: |
+          docker build --build-arg base_tag=3-windowsservercore-1809 `
+              -f ./docker/windows.Dockerfile `
+              -t nao20010128nao/ytdl-patched:latest `
+              -t nao20010128nao/ytdl-patched:windowsservercore-1809 `
+              -t nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }} `
+              -t nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}-windowsservercore-1809 `
+              .
+          docker push nao20010128nao/ytdl-patched:latest
+          docker push nao20010128nao/ytdl-patched:windowsservercore-1809
+          docker push nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}
+          docker push nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}-windowsservercore-1809
+      - name: Build and Push (ltsc2016)
+        if: ${{ matrix.windows.tag == 'ltsc2016' }}
+        run: |
+          docker build --build-arg base_tag=3-windowsservercore-ltsc2016 `
+              -f ./docker/windows.Dockerfile `
+              -t nao20010128nao/ytdl-patched:windowsservercore-ltsc2016 `
+              -t nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}-windowsservercore-ltsc2016 `
+              .
+          docker push nao20010128nao/ytdl-patched:windowsservercore-ltsc2016
+          docker push nao20010128nao/ytdl-patched:${{ needs.prepare.outputs.latest_version_numeric }}-windowsservercore-ltsc2016
diff --git a/.github/workflows/ci.yml b/.github/workflows/ci.yml
index 90bd63c32..72cfbf2c1 100644
--- a/.github/workflows/ci.yml
+++ b/.github/workflows/ci.yml
@@ -43,7 +43,7 @@ jobs:
         python-version: ${{ matrix.python-version }}
     - name: Set up Java 8
       if: ${{ matrix.python-impl == 'jython' }}
-      uses: actions/setup-java@v1
+      uses: actions/setup-java@v2
       with:
         java-version: 8
     - name: Install Jython
diff --git a/.github/workflows/codeql-analysis.yml b/.github/workflows/codeql-analysis.yml
new file mode 100644
index 000000000..355543418
--- /dev/null
+++ b/.github/workflows/codeql-analysis.yml
@@ -0,0 +1,67 @@
+# For most projects, this workflow file will not need changing; you simply need
+# to commit it to your repository.
+#
+# You may wish to alter this file to override the set of languages analyzed,
+# or to provide custom queries or build logic.
+name: "CodeQL"
+
+on:
+  push:
+    branches: [master]
+    paths:
+      - youtube_dl/**
+      - test/**
+  workflow_dispatch:
+    inputs: {}
+  pull_request:
+    # The branches below must be a subset of the branches above
+    branches: [master]
+  schedule:
+    - cron: '0 19 * * 6'
+
+jobs:
+  analyze:
+    name: Analyze
+    runs-on: ubuntu-latest
+
+    strategy:
+      fail-fast: false
+      matrix:
+        # Override automatic language detection by changing the below list
+        # Supported options are ['csharp', 'cpp', 'go', 'java', 'javascript', 'python']
+        language: ['python']
+        # Learn more...
+        # https://docs.github.com/en/github/finding-security-vulnerabilities-and-errors-in-your-code/configuring-code-scanning#overriding-automatic-language-detection
+
+    steps:
+    - name: Checkout repository
+      uses: actions/checkout@v2
+
+    # Initializes the CodeQL tools for scanning.
+    - name: Initialize CodeQL
+      uses: github/codeql-action/init@v1
+      with:
+        languages: ${{ matrix.language }}
+        # If you wish to specify custom queries, you can do so here or in a config file.
+        # By default, queries listed here will override any specified in a config file.
+        # Prefix the list here with "+" to use these queries and those in the config file.
+        # queries: ./path/to/local/query, your-org/your-repo/queries@main
+
+    # Autobuild attempts to build any compiled languages  (C/C++, C#, or Java).
+    # If this step fails, then you should remove it and run the build manually (see below)
+    - name: Autobuild
+      uses: github/codeql-action/autobuild@v1
+
+    # ‚ÑπÔ∏è Command-line programs to run using the OS shell.
+    # üìö https://git.io/JvXDl
+
+    # ‚úèÔ∏è If the Autobuild fails above, remove it and uncomment the following three lines
+    #    and modify them (or add more) to build your code if your project
+    #    uses a compiled language
+
+    #- run: |
+    #   make bootstrap
+    #   make release
+
+    - name: Perform CodeQL Analysis
+      uses: github/codeql-action/analyze@v1
diff --git a/.github/workflows/deploy-cdn.yml b/.github/workflows/deploy-cdn.yml
new file mode 100644
index 000000000..327afe887
--- /dev/null
+++ b/.github/workflows/deploy-cdn.yml
@@ -0,0 +1,43 @@
+name: Deploy repository to CDNs
+
+on:
+  push:
+    branches:
+      - master
+  workflow_dispatch:
+    inputs: {}
+
+jobs:
+  netlify:
+    runs-on: ubuntu-latest
+    env:
+      NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
+      NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
+    steps:
+      - uses: actions/checkout@v2
+      - name: Set up Node.js 14
+        uses: actions/setup-node@v1
+        with:
+          node-version: 14
+      - name: Set up Netlify
+        run: npm install netlify-cli -g
+      - name: Build and deploy
+        run: |
+          netlify build
+          netlify deploy --prod
+  vercel:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v2
+      - name: Set up Node.js 14
+        uses: actions/setup-node@v1
+        with:
+          node-version: 14
+      - uses: amondnet/vercel-action@v20
+        with:
+          vercel-token: ${{ secrets.VERCEL_TOKEN }}
+          github-token: ${{ secrets.GITHUB_TOKEN }}
+          vercel-args: '--prod'
+          vercel-org-id: ${{ secrets.ORG_ID }}
+          vercel-project-id: ${{ secrets.PROJECT_ID }}
+          github-comment: false 
diff --git a/.github/workflows/docker-codespaces.yml b/.github/workflows/docker-codespaces.yml
new file mode 100644
index 000000000..166e4895b
--- /dev/null
+++ b/.github/workflows/docker-codespaces.yml
@@ -0,0 +1,29 @@
+name: Docker image for Codespaces
+
+on:
+  push:
+    paths:
+      - docker/codespaces.Dockerfile
+      - .github/workflows/docker-codespaces.yml
+      - .devcontainer/**
+    branches:
+      - master
+  workflow_dispatch:
+    inputs: {}
+  schedule:
+    - cron: "0 4 * * 0"
+
+jobs:
+  docker-codespaces:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v2
+      - name: Login
+        uses: docker/login-action@v1
+        with:
+          username: ${{ github.repository_owner }}
+          password: ${{ secrets.DHUB_TOKEN }}
+      - name: Build and Push
+        run: |
+          docker build -t nao20010128nao/ytdl-patched:codespaces -f ./docker/codespaces.Dockerfile .
+          docker push nao20010128nao/ytdl-patched:codespaces
diff --git a/.github/workflows/generate-files-automatic.yml b/.github/workflows/generate-files-automatic.yml
new file mode 100644
index 000000000..43f49d946
--- /dev/null
+++ b/.github/workflows/generate-files-automatic.yml
@@ -0,0 +1,61 @@
+name: Automatically regenerate files
+
+on:
+  push:
+    paths:
+      - .github/workflows/generate-files-automatic.yml
+      - devscripts/make_mastodon_instance_list.py
+      - devscripts/make_peertube_instance_list.py
+      - devscripts/make_chrome_version_list.py
+    branches:
+      - master
+  schedule:
+    - cron: "0 2 * * 0"
+  workflow_dispatch:
+    inputs: {}
+
+jobs:
+  generate:
+    runs-on: ubuntu-latest
+    env:
+      INSTANCE_SOCIAL_API_SECRET: ${{ secrets.INSTANCE_SOCIAL_API_SECRET }}
+    steps:
+    - uses: actions/checkout@v2
+      with:
+        ref: master
+        fetch-depth: 0
+    - name: Set up Python 3.9
+      uses: actions/setup-python@v2
+      with:
+        python-version: 3.9
+    - name: Configure git
+      run: |
+        git config --global pull.rebase false
+        git config --global core.editor true
+        git config --global user.name nao20010128nao
+        git config --global user.email nao20010128@gmail.com
+        git config --unset  http.https://github.com/.extraheader
+        git remote set-url origin https://nao20010128nao:${{ secrets.GH_PAT }}@github.com/nao20010128nao/ytdl-patched.git
+
+    - name: Generate youtube_dl/extractor/mastodon/instances.py
+      run: python ./devscripts/make_mastodon_instance_list.py
+      continue-on-error: true
+    - name: Generate youtube_dl/extractor/peertube/instances.py
+      run: python ./devscripts/make_peertube_instance_list.py
+      continue-on-error: true
+    - name: Generate youtube_dl/chrome_versions.py
+      run: python ./devscripts/make_chrome_version_list.py
+      continue-on-error: true
+
+    - name: Commit youtube_dl/extractor/mastodon/instances.py
+      run: git commit -m"[automatic] regenerate youtube_dl/extractor/mastodon/instances.py @ $(date +'%Y%m%d')" youtube_dl/extractor/mastodon/instances.py
+      continue-on-error: true
+    - name: Commit youtube_dl/extractor/peertube/instances.py
+      run: git commit -m"[automatic] regenerate youtube_dl/extractor/peertube/instances.py @ $(date +'%Y%m%d')" youtube_dl/extractor/peertube/instances.py
+      continue-on-error: true
+    - name: Commit youtube_dl/chrome_versions.py
+      run: git commit -m"[automatic] regenerate youtube_dl/chrome_versions.py @ $(date +'%Y%m%d')" youtube_dl/chrome_versions.py
+      continue-on-error: true
+
+    - name: Push to here
+      run: git push origin HEAD:master
diff --git a/.github/workflows/merge.yml b/.github/workflows/merge.yml
new file mode 100644
index 000000000..e965e9a18
--- /dev/null
+++ b/.github/workflows/merge.yml
@@ -0,0 +1,42 @@
+name: Merge upstream
+
+on:
+  schedule:
+    - cron: "0 */6 * * *"
+  workflow_dispatch:
+    inputs:
+
+jobs:
+  merge:
+    runs-on: ubuntu-latest
+    steps:
+    - uses: actions/checkout@v2
+      with:
+        ref: master
+        fetch-depth: 0
+    - name: Configure git
+      run: |
+        set +e -x
+        git config --global pull.rebase false
+        git config --global core.editor "$(realpath ./devscripts/remove_ci_skip.py)"
+        git config --global user.name nao20010128nao
+        git config --global user.email nao20010128@gmail.com
+        git config --unset  http.https://github.com/.extraheader
+        git config --unset  credential.helper
+        git remote add self https://nao20010128nao:${{ secrets.GH_PAT }}@github.com/nao20010128nao/ytdl-patched.git
+        git remote add upstream https://github.com/ytdl-org/youtube-dl.git
+    - name: Pull from ytdl-org/youtube-dl
+      run: git pull upstream master --log
+    - name: Regenerate docs/supportedsites.md
+      continue-on-error: true
+      run: |
+        python ./devscripts/make_supportedsites.py ./docs/supportedsites.md 
+        git commit -m"[automatic] regenerate docs/supportedsites.md @ $(date +'%Y%m%d')" ./docs/supportedsites.md
+    - name: Push to here
+      run: git push self HEAD:master
+    - name: Create tag
+      continue-on-error: true
+      run: |
+        git push self :refs/tags/youtube-dl || true
+        git tag -f youtube-dl upstream/master || true
+        git push self -f --tags || true
diff --git a/.github/workflows/tests.yml b/.github/workflows/tests.yml
new file mode 100644
index 000000000..4f4c10c89
--- /dev/null
+++ b/.github/workflows/tests.yml
@@ -0,0 +1,82 @@
+name: Run tests
+
+on:
+  push:
+    paths:
+      - youtube_dl/**
+      - test/**
+      - .github/workflows/tests.yml
+      - devscripts/**
+    branches-ignore:
+      - gh-pages
+  workflow_dispatch:
+    inputs: {}
+
+jobs:
+  test:
+    runs-on: ubuntu-latest
+    continue-on-error: ${{ matrix.ytdl_test_set.may_fail || matrix.python.may_fail }}
+    strategy:
+      matrix:
+        python: [
+          { name: '2.7', may_fail: false, major: 2, special: '' },
+          { name: '3.9', may_fail: false, major: 3, special: '' },
+          { name: '3.8', may_fail: false, major: 3, special: '' },
+          { name: '3.7', may_fail: false, major: 3, special: '' },
+          { name: 'pypy-3.7', may_fail: false, major: 3, special: '' },
+          { name: 'pypy-2.7', may_fail: true, major: 2, special: '' },
+          { name: 'jython', may_fail: true, major: 2, special: 'jython' },
+          # { name: 'ironpython', may_fail: true, major: 2, special: 'iron' },
+        ]
+        ytdl_test_set: [
+          { name: 'download', may_fail: true },
+          { name: 'core', may_fail: false },
+        ]
+    env:
+      YTDL_TEST_SET: ${{ matrix.ytdl_test_set.name }}
+    steps:
+      - uses: actions/checkout@v2
+      # standard CPython provided by GitHub Actions
+      - name: Set up Python ${{ matrix.python.name }}
+        uses: actions/setup-python@v2
+        if: ${{ !matrix.python.special }}
+        with:
+          python-version: ${{ matrix.python.name }}
+      - name: Install deps
+        if: ${{ !matrix.python.special }}
+        run: pip install nose
+      # Jython
+      - name: Install Java
+        if: ${{ matrix.python.special == 'jython' }}
+        uses: actions/setup-java@v2
+        with:
+          distribution: 'zulu'
+          java-version: '11'
+      - name: Install Jython
+        if: ${{ matrix.python.special == 'jython' }}
+        run: ./devscripts/install_jython.sh
+      # IronPython
+      - name: Install .NET
+        uses: actions/setup-dotnet@v1
+        if: ${{ matrix.python.special == 'iron' }}
+        with:
+          dotnet-version: '3.1.x'
+      - name: Install IronPython
+        if: ${{ matrix.python.special == 'iron' }}
+        run: ./devscripts/install_ironpython.sh
+      # main script
+      - name: Run test
+        continue-on-error: ${{ matrix.ytdl_test_set.may_fail || matrix.python.may_fail }}
+        run: ./devscripts/run_tests.sh
+  flake8:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v2
+      - name: Set up Python 3.8
+        uses: actions/setup-python@v2
+        with:
+          python-version: 3.8
+      - name: Install deps
+        run: pip3 install flake8
+      - name: Run flake8 .
+        run: flake8 .
diff --git a/.gitignore b/.gitignore
index c4870a6ba..2ab2537aa 100644
--- a/.gitignore
+++ b/.gitignore
@@ -14,12 +14,17 @@ youtube-dl.1
 youtube-dl.bash-completion
 youtube-dl.fish
 youtube_dl/extractor/lazy_extractors.py
+youtube_dl/build_config.py
 youtube-dl
 youtube-dl.exe
 youtube-dl.tar.gz
 .coverage
 cover/
 updates_key.pem
+.jython_cache/
+.pytest_cache/
+__pycache__/
+**/__pycache__/
 *.egg-info
 *.srt
 *.ttml
@@ -38,9 +43,20 @@ updates_key.pem
 *.part
 *.ytdl
 *.swp
+*.jpg
+*.jpeg
+*.png
+*.gif
+*.aria2c
+*.lock
+*.dump
 test/local_parameters.json
 .tox
 youtube-dl.zsh
+public/
+bin/
+.vercel
+zip/
 
 # IntelliJ related files
 .idea
@@ -51,3 +67,7 @@ venv/
 
 # VS Code related files
 .vscode
+
+# Added by GH Codespaces
+pythonenv3.8/
+.venv/
diff --git a/.gitlab-ci.yml b/.gitlab-ci.yml
new file mode 100644
index 000000000..1217ff1f0
--- /dev/null
+++ b/.gitlab-ci.yml
@@ -0,0 +1,14 @@
+stages:
+  - test
+
+variables:
+  GIT_SUBMODULE_STRATEGY: recursive
+
+test:
+  stage: test
+  image: python
+  variables:
+    YTDL_TEST_SET: core
+  script:
+    - pip3 install nose
+    - ./devscripts/run_tests.sh
diff --git a/.lintstagedrc.json b/.lintstagedrc.json
new file mode 100644
index 000000000..97c46ec63
--- /dev/null
+++ b/.lintstagedrc.json
@@ -0,0 +1,5 @@
+{
+  "*.py": ["flake8"],
+  "*.{js,json}": ["npx prettier --print-width 200 --write"],
+  "*.sh": ["shellcheck"]
+}
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 58ab3a4b8..11db73724 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -3,7 +3,7 @@
 $ youtube-dl -v <your command line>
 [debug] System config: []
 [debug] User config: []
-[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
+[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=Li_hJB_TCqY']
 [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
 [debug] youtube-dl version 2015.12.06
 [debug] Git HEAD: 135392e
@@ -34,7 +34,7 @@ For bug reports, this means that your report should contain the *complete* outpu
 
 If your server has multiple IPs or you suspect censorship, adding `--call-home` may be a good idea to get more diagnostics. If the error is `ERROR: Unable to extract ...` and you cannot reproduce it from multiple countries, add `--dump-pages` (warning: this will yield a rather large output, redirect it to the file `log.txt` by adding `>log.txt 2>&1` to your command-line) or upload the `.dump` files you get when you add `--write-pages` [somewhere](https://gist.github.com/).
 
-**Site support requests must contain an example URL**. An example URL is a URL you might want to download, like `https://www.youtube.com/watch?v=BaW_jenozKc`. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. `https://www.youtube.com/`) is *not* an example URL.
+**Site support requests must contain an example URL**. An example URL is a URL you might want to download, like `https://www.youtube.com/watch?v=Li_hJB_TCqY`. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. `https://www.youtube.com/`) is *not* an example URL.
 
 ###  Are you using the latest version?
 
@@ -70,7 +70,7 @@ It may sound strange, but some bug reports we receive are completely unrelated t
 
 # DEVELOPER INSTRUCTIONS
 
-Most users do not need to build youtube-dl and can [download the builds](https://ytdl-org.github.io/youtube-dl/download.html) or get them from their distribution.
+Most users do not need to build youtube-dl and can [download the builds](https://github.com/nao20010128nao/ytdl-patched/releases/latest) or get them from their distribution.
 
 To run youtube-dl as a developer, you don't need to build anything either. Simply execute
 
@@ -329,14 +329,14 @@ For example, you should **never** split long string literals like URLs or some o
 Correct:
 
 ```python
-'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
+'https://www.youtube.com/watch?v=aSVvL2rgW8g&list=PL129CD8FEF7ED1E38'
 ```
 
 Incorrect:
 
 ```python
-'https://www.youtube.com/watch?v=FqZTN594JQw&list='
-'PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
+'https://www.youtube.com/watch?v=aSVvL2rgW8g&list='
+'PL129CD8FEF7ED1E38'
 ```
 
 ### Inline values
@@ -432,3 +432,55 @@ duration = float_or_none(video.get('durationMs'), scale=1000)
 view_count = int_or_none(video.get('views'))
 ```
 
+# BUILDING INSTRUCTIONS
+_Note: Most of you don't have to read this section. Please go to [releases page](https://github.com/nao20010128nao/ytdl-patched/releases/latest) for built binaries. This section is for advanced users._
+
+## Linux/macOS builds
+### Dependencies
+
+- Python 3.x (most distro have this)
+- One of: `zip` or `7z` command. `7z` will be preferred when both available.
+- Optional: If you need to improve compression rate, install `advzip` (included in `advancecomp`)
+- `pandoc` (required to build [`OPTIONS`](#options) section of this README.md)
+
+### Instruction
+Just run:
+
+```bash
+make
+```
+
+To build `youtube-dl` binary:
+
+```bash
+make youtube-dl
+```
+
+You'll find binary at the same directory you run this.
+
+## Windows builds
+### Dependencies
+
+- [Genuine Windows operating system](https://developer.microsoft.com/microsoft-edge/tools/vms/) (Windows 7 or above)
+- Python 3.6 or later (**CAREFULLY READ INDENTED SECTIONS BELOW**)
+    - **DO NOT** install Python from **Windows Store**, uninstall it before building. [This is due to permission errors especially to Windows Store Apps.](https://github.com/pyinstaller/pyinstaller/pull/4702)
+    - [**USE** Python 3.x found at python.org](https://www.python.org/downloads/windows/)
+    - Instead, you can use CI services with Windows support.
+- Install `PyInstaller` to Python above with: `python3 -m pip install pyinstaller`
+    - ytdl-patched doesn't support py2exe.
+
+### Instruction
+_Note: there is no GUI tool to build this._
+
+Open PowerShell, cd to root of this repository, then type:
+
+```powershell
+python3 -m PyInstaller `
+    --onefile --console --distpath . `
+    -n youtube-dl youtube_dl\__main__.py
+```
+
+This command is minimum requirement needed to build. (see [devscripts/build_windows_exe.ps1](https://github.com/nao20010128nao/ytdl-patched/blob/master/devscripts/build_windows_exe.ps1) for current build flags used)
+
+You'll find binary at the same directory you run this.
+
diff --git a/MANIFEST.in b/MANIFEST.in
index 4e43e99f3..13138d067 100644
--- a/MANIFEST.in
+++ b/MANIFEST.in
@@ -4,6 +4,7 @@ include AUTHORS
 include ChangeLog
 include youtube-dl.bash-completion
 include youtube-dl.fish
+include _youtube-dl
 include youtube-dl.1
 recursive-include docs Makefile conf.py *.rst
 recursive-include test *
diff --git a/Makefile b/Makefile
index 3e17365b8..94f92a770 100644
--- a/Makefile
+++ b/Makefile
@@ -1,23 +1,24 @@
-all: youtube-dl README.md CONTRIBUTING.md README.txt youtube-dl.1 youtube-dl.bash-completion youtube-dl.zsh youtube-dl.fish supportedsites
+all: youtube-dl README.md CONTRIBUTING.md README.txt youtube-dl.1 youtube-dl.bash-completion _youtube-dl youtube-dl.fish supportedsites
 
 clean:
-	rm -rf youtube-dl.1.temp.md youtube-dl.1 youtube-dl.bash-completion README.txt MANIFEST build/ dist/ .coverage cover/ youtube-dl.tar.gz youtube-dl.zsh youtube-dl.fish youtube_dl/extractor/lazy_extractors.py *.dump *.part* *.ytdl *.info.json *.mp4 *.m4a *.flv *.mp3 *.avi *.mkv *.webm *.3gp *.wav *.ape *.swf *.jpg *.png CONTRIBUTING.md.tmp youtube-dl youtube-dl.exe
+	rm -rf youtube-dl.1.temp.md youtube-dl.1 youtube-dl.bash-completion README.txt MANIFEST build/ dist/ .coverage cover/ youtube-dl.tar.gz _youtube-dl youtube-dl.fish youtube_dl/extractor/lazy_extractors.py *.dump *.part* *.ytdl *.info.json *.mp4 *.m4a *.flv *.mp3 *.avi *.mkv *.webm *.3gp *.wav *.ape *.swf *.jpg *.png CONTRIBUTING.md.tmp youtube-dl youtube-dl.exe
 	find . -name "*.pyc" -delete
 	find . -name "*.class" -delete
+	find . -name "*~~" -exec rm -rf {} +
 
 PREFIX ?= /usr/local
 BINDIR ?= $(PREFIX)/bin
 MANDIR ?= $(PREFIX)/man
 SHAREDIR ?= $(PREFIX)/share
-PYTHON ?= /usr/bin/env python
+PYTHON ?= /usr/bin/env python3
 
 # set SYSCONFDIR to /etc if PREFIX=/usr or PREFIX=/usr/local
 SYSCONFDIR = $(shell if [ $(PREFIX) = /usr -o $(PREFIX) = /usr/local ]; then echo /etc; else echo $(PREFIX)/etc; fi)
 
 # set markdown input format to "markdown-smart" for pandoc version 2 and to "markdown" for pandoc prior to version 2
-MARKDOWN = $(shell if [ `pandoc -v | head -n1 | cut -d" " -f2 | head -c1` = "2" ]; then echo markdown-smart; else echo markdown; fi)
+MARKDOWN = $(shell if [ "$(pandoc -v | head -n1 | cut -d" " -f2 | head -c1)" = "2" ]; then echo markdown-smart; else echo markdown; fi)
 
-install: youtube-dl youtube-dl.1 youtube-dl.bash-completion youtube-dl.zsh youtube-dl.fish
+install: youtube-dl youtube-dl.1 youtube-dl.bash-completion _youtube-dl youtube-dl.fish
 	install -d $(DESTDIR)$(BINDIR)
 	install -m 755 youtube-dl $(DESTDIR)$(BINDIR)
 	install -d $(DESTDIR)$(MANDIR)/man1
@@ -25,7 +26,7 @@ install: youtube-dl youtube-dl.1 youtube-dl.bash-completion youtube-dl.zsh youtu
 	install -d $(DESTDIR)$(SYSCONFDIR)/bash_completion.d
 	install -m 644 youtube-dl.bash-completion $(DESTDIR)$(SYSCONFDIR)/bash_completion.d/youtube-dl
 	install -d $(DESTDIR)$(SHAREDIR)/zsh/site-functions
-	install -m 644 youtube-dl.zsh $(DESTDIR)$(SHAREDIR)/zsh/site-functions/_youtube-dl
+	install -m 644 _youtube-dl $(DESTDIR)$(SHAREDIR)/zsh/site-functions/_youtube-dl
 	install -d $(DESTDIR)$(SYSCONFDIR)/fish/completions
 	install -m 644 youtube-dl.fish $(DESTDIR)$(SYSCONFDIR)/fish/completions/youtube-dl.fish
 
@@ -49,28 +50,32 @@ offlinetest: codetest
 		--exclude test_subtitles.py \
 		--exclude test_write_annotations.py \
 		--exclude test_youtube_lists.py \
-		--exclude test_youtube_signature.py
+		--exclude test_youtube_signature.py \
+		--exclude test_websocket.py
 
 tar: youtube-dl.tar.gz
 
 .PHONY: all clean install test tar bash-completion pypi-files zsh-completion fish-completion ot offlinetest codetest supportedsites
 
-pypi-files: youtube-dl.bash-completion README.txt youtube-dl.1 youtube-dl.fish
-
-youtube-dl: youtube_dl/*.py youtube_dl/*/*.py
-	mkdir -p zip
-	for d in youtube_dl youtube_dl/downloader youtube_dl/extractor youtube_dl/postprocessor ; do \
-	  mkdir -p zip/$$d ;\
-	  cp -pPR $$d/*.py zip/$$d/ ;\
-	done
-	touch -t 200001010101 zip/youtube_dl/*.py zip/youtube_dl/*/*.py
-	mv zip/youtube_dl/__main__.py zip/
-	cd zip ; zip -q ../youtube-dl youtube_dl/*.py youtube_dl/*/*.py __main__.py
-	rm -rf zip
-	echo '#!$(PYTHON)' > youtube-dl
-	cat youtube-dl.zip >> youtube-dl
-	rm youtube-dl.zip
-	chmod a+x youtube-dl
+pypi-files: youtube-dl.bash-completion README.txt youtube-dl.1 youtube-dl.fish _youtube-dl
+
+# youtube-dl: youtube_dl/*.py youtube_dl/*/*.py
+# 	mkdir -p zip
+# 	for d in youtube_dl youtube_dl/downloader youtube_dl/extractor youtube_dl/extractor/*/ youtube_dl/postprocessor youtube_dl/websocket ; do \
+# 	  mkdir -p zip/$$d ;\
+# 	  cp -pPR $$d/*.py zip/$$d/ ;\
+# 	done
+# 	touch -t 200001010101 zip/youtube_dl/*.py zip/youtube_dl/*/*.py
+# 	mv zip/youtube_dl/__main__.py zip/
+# 	cd zip ; 7z a -mm=Deflate -mfb=258 -mpass=15 -mtc- ../youtube-dl.zip youtube_dl/*.py youtube_dl/*/*.py youtube_dl/*/*/*.py __main__.py
+# 	rm -rf zip
+# 	echo '#!$(PYTHON)' > youtube-dl
+# 	cat youtube-dl.zip >> youtube-dl
+# 	rm youtube-dl.zip
+# 	chmod a+x youtube-dl
+
+youtube-dl: youtube_dl/*.py youtube_dl/*/*.py youtube_dl/*/*/*.py devscripts/make_zipfile.py
+	$(PYTHON) devscripts/make_zipfile.py "$(PYTHON)"
 
 README.md: youtube_dl/*.py youtube_dl/*/*.py
 	COLUMNS=80 $(PYTHON) youtube_dl/__main__.py --help | $(PYTHON) devscripts/make_readme.py
@@ -101,10 +106,10 @@ youtube-dl.bash-completion: youtube_dl/*.py youtube_dl/*/*.py devscripts/bash-co
 
 bash-completion: youtube-dl.bash-completion
 
-youtube-dl.zsh: youtube_dl/*.py youtube_dl/*/*.py devscripts/zsh-completion.in
+_youtube-dl: youtube_dl/*.py youtube_dl/*/*.py devscripts/zsh-completion.in
 	$(PYTHON) devscripts/zsh-completion.py
 
-zsh-completion: youtube-dl.zsh
+zsh-completion: _youtube-dl
 
 youtube-dl.fish: youtube_dl/*.py youtube_dl/*/*.py devscripts/fish-completion.in
 	$(PYTHON) devscripts/fish-completion.py
@@ -117,7 +122,7 @@ _EXTRACTOR_FILES = $(shell find youtube_dl/extractor -iname '*.py' -and -not -in
 youtube_dl/extractor/lazy_extractors.py: devscripts/make_lazy_extractors.py devscripts/lazy_load_template.py $(_EXTRACTOR_FILES)
 	$(PYTHON) devscripts/make_lazy_extractors.py $@
 
-youtube-dl.tar.gz: youtube-dl README.md README.txt youtube-dl.1 youtube-dl.bash-completion youtube-dl.zsh youtube-dl.fish ChangeLog AUTHORS
+youtube-dl.tar.gz: youtube-dl README.md README.txt youtube-dl.1 youtube-dl.bash-completion _youtube-dl youtube-dl.fish ChangeLog AUTHORS
 	@tar -czf youtube-dl.tar.gz --transform "s|^|youtube-dl/|" --owner 0 --group 0 \
 		--exclude '*.DS_Store' \
 		--exclude '*.kate-swp' \
@@ -131,5 +136,6 @@ youtube-dl.tar.gz: youtube-dl README.md README.txt youtube-dl.1 youtube-dl.bash-
 		bin devscripts test youtube_dl docs \
 		ChangeLog AUTHORS LICENSE README.md README.txt \
 		Makefile MANIFEST.in youtube-dl.1 youtube-dl.bash-completion \
-		youtube-dl.zsh youtube-dl.fish setup.py setup.cfg \
+		_youtube-dl youtube-dl.fish setup.py setup.cfg \
 		youtube-dl
+	advdef -z -4 -i 30 youtube-dl.tar.gz
diff --git a/README.md b/README.md
index 059141611..c4d4b70db 100644
--- a/README.md
+++ b/README.md
@@ -1,8 +1,44 @@
-[![Build Status](https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg)](https://github.com/ytdl-org/youtube-dl/actions?query=workflow%3ACI)
-
-
-youtube-dl - download videos from youtube.com or other video platforms
-
+## MIRRORS
+- https://github.com/nao20010128nao/ytdl-patched (primary)
+- https://bitbucket.org/nao20010128nao/ytdl-patched (secondary)
+- https://forge.tedomum.net/nao20010128nao/ytdl-patched (secondary)
+- https://gitlab.com/lesmi_the_goodness/ytdl-patched (secondary)
+- https://gitea.com/nao20010128nao/ytdl-patched (secondary)
+- https://git.sr.ht/~nao20010128nao/ytdl-patched (secondary)
+  - manually mirrored at every git push
+- https://codeberg.org/nao20010128nao/ytdl-patched (secondary)
+- [In my Keybase account](https://book.keybase.io/git) (secondary)
+  - spoiler: you can clone it with `git clone keybase://public/nao20010128nao/ytdl-patched`
+- https://ytdl-patched.vercel.app/ (just for fun)
+- https://ytdl-patched.netlify.app/ (just for fun)
+
+<!-- MARKER BEGIN -->
+
+| Service | Type | Status | Note |
+|:-------:|:----:|:------:|:----:|
+| Travis CI | Tests | [![Build Status](https://travis-ci.org/nao20010128nao/ytdl-patched.svg?branch=master)](https://travis-ci.org/nao20010128nao/ytdl-patched.svg) | Stopped because of lack of credits |
+| GitHub Actions | Tests | ![Run tests](https://github.com/nao20010128nao/ytdl-patched/workflows/Run%20tests/badge.svg) | |
+| GitHub Actions | Build and release | ![Build Patched YTDL](https://github.com/nao20010128nao/ytdl-patched/workflows/Build%20Patched%20YTDL/badge.svg) | |
+| GitHub Actions | Merging commits from upstream | ![Merge upstream](https://github.com/nao20010128nao/ytdl-patched/workflows/Merge%20upstream/badge.svg) | There's conflict if it fails |
+
+<!-- MARKER END -->
+
+# ytdl-patched
+ytdl-patched - download videos from youtube.com or other video platforms
+
+ytdl-patched is not intended modify heavily, but just to "patch" the ytdl-org one.
+This is where "patched" is come from.
+
+## GOALS
+- keep merging with [`ytdl-org/youtube-dl`](https://github.com/ytdl-org/youtube-dl)
+- implement miscellaneous extractors as possible
+- make `-U` work (yes, really works)
+- do anything best
+
+## NOT TO DO
+- don't change very much from ytdl-org
+
+## TOC
 - [INSTALLATION](#installation)
 - [DESCRIPTION](#description)
 - [OPTIONS](#options)
@@ -12,6 +48,7 @@ youtube-dl - download videos from youtube.com or other video platforms
 - [VIDEO SELECTION](#video-selection)
 - [FAQ](#faq)
 - [DEVELOPER INSTRUCTIONS](#developer-instructions)
+- [BUILDING INSTRUCTIONS](#building-instructions)
 - [EMBEDDING YOUTUBE-DL](#embedding-youtube-dl)
 - [BUGS](#bugs)
 - [COPYRIGHT](#copyright)
@@ -20,27 +57,27 @@ youtube-dl - download videos from youtube.com or other video platforms
 
 To install it right away for all UNIX users (Linux, macOS, etc.), type:
 
-    sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
+    sudo curl -L https://ytdl-patched.vercel.app/api/release/latest/youtube-dl -o /usr/local/bin/youtube-dl
     sudo chmod a+rx /usr/local/bin/youtube-dl
 
 If you do not have curl, you can alternatively use a recent wget:
 
-    sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
+    sudo wget https://ytdl-patched.vercel.app/api/release/latest/youtube-dl -O /usr/local/bin/youtube-dl
     sudo chmod a+rx /usr/local/bin/youtube-dl
 
 Windows users can [download an .exe file](https://yt-dl.org/latest/youtube-dl.exe) and place it in any location on their [PATH](https://en.wikipedia.org/wiki/PATH_%28variable%29) except for `%SYSTEMROOT%\System32` (e.g. **do not** put in `C:\Windows\System32`).
 
-You can also use pip:
+~~You can also use pip:~~
 
     sudo -H pip install --upgrade youtube-dl
     
-This command will update youtube-dl if you have already installed it. See the [pypi page](https://pypi.python.org/pypi/youtube_dl) for more information.
+~~This command will update youtube-dl if you have already installed it. See the [pypi page](https://pypi.python.org/pypi/youtube_dl) for more information.~~
 
-macOS users can install youtube-dl with [Homebrew](https://brew.sh/):
+macOS/Linux users can install youtube-dl with [Homebrew](https://brew.sh/):
 
-    brew install youtube-dl
+    brew install nao20010128nao/my/youtube-dl
 
-Or with [MacPorts](https://www.macports.org/):
+~~Or with [MacPorts](https://www.macports.org/):~~
 
     sudo port install youtube-dl
 
@@ -53,7 +90,7 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
 
 # OPTIONS
     -h, --help                           Print this help text and exit
-    --version                            Print program version and exit
+    -V, --version                        Print program version and exit
     -U, --update                         Update this program to latest version.
                                          Make sure that you have sufficient
                                          permissions (run with sudo if needed)
@@ -96,12 +133,24 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
     --no-mark-watched                    Do not mark videos watched (YouTube
                                          only)
     --no-color                           Do not emit color codes in output
+    --check-mastodon-instance            Always perform online checks for
+                                         Mastodon-like URL
+    --check-peertube-instance            Always perform online checks for
+                                         PeerTube-like URL
+    --test-filename CMD                  Like --exec option, but used for
+                                         testing if downloading should be
+                                         started. You can begin with "re:" to
+                                         use regex instead of commands
+    --print-infojson-types               DO NOT USE. IT'S MEANINGLESS FOR MOST
+                                         PEOPLE. Prints types of object in info
+                                         json. Use this for extractors that
+                                         --print-json won' work.
 
 ## Network Options:
     --proxy URL                          Use the specified HTTP/HTTPS/SOCKS
                                          proxy. To enable SOCKS proxy, specify a
                                          proper scheme. For example
-                                         socks5://127.0.0.1:1080/. Pass in an
+                                         socks5h://127.0.0.1:1080/. Pass in an
                                          empty string (--proxy "") for direct
                                          connection
     --socket-timeout SECONDS             Time to wait before giving up, in
@@ -190,9 +239,15 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
     --download-archive FILE              Download only videos not listed in the
                                          archive file. Record the IDs of all
                                          downloaded videos in it.
+    --failed-archive FILE                Record the URLs or IDs of all
+                                         downloading-failed videos in it.
     --include-ads                        Download advertisements as well
                                          (experimental)
 
+## Extractor Options:
+    --extractor-retries RETRIES          Number of retries for known extractor
+                                         errors (default is 3), or "infinite"
+
 ## Download Options:
     -r, --limit-rate RATE                Maximum download rate in bytes per
                                          second (e.g. 50K or 4.2M)
@@ -286,6 +341,8 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
                                          but that may change.
     --no-cache-dir                       Disable filesystem caching
     --rm-cache-dir                       Delete all filesystem cache files
+    --rm-long-name-dir                   Deletes all filename-splitting-related
+                                         empty directories in working directory
 
 ## Thumbnail Options:
     --write-thumbnail                    Write thumbnail image to disk
@@ -364,6 +421,21 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
                                          possible number of seconds to sleep).
                                          Must only be used along with --min-
                                          sleep-interval.
+    --sleep-before-extract SECONDS       Number of seconds to sleep before each
+                                         extraction when used alone or a lower
+                                         bound of a range for randomized sleep
+                                         before each extraction (minimum
+                                         possible number of seconds to sleep)
+                                         when used along with --max-sleep-
+                                         before-extract.
+    --max-sleep-before-extract SECONDS   Upper bound of a range for randomized
+                                         sleep before each extraction (maximum
+                                         possible number of seconds to sleep).
+                                         Must only be used along with --min-
+                                         sleep-before-extract.
+    --escape-long-names                  Split filename longer than 255 bytes
+                                         into few path segments. This may create
+                                         dumb directories.
 
 ## Video Format Options:
     -f, --format FORMAT                  Video format code, see the "FORMAT
@@ -371,6 +443,8 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
     --all-formats                        Download all available video formats
     --prefer-free-formats                Prefer free video formats unless a
                                          specific one is requested
+    --prefer-smaller-formats             Prefer smaller video formats unless a
+                                         specific one is requested
     -F, --list-formats                   List all available formats of requested
                                          videos
     --youtube-skip-dash-manifest         Do not download the DASH manifests and
@@ -380,6 +454,10 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
                                          container format. One of mkv, mp4, ogg,
                                          webm, flv. Ignored if no merge is
                                          required
+    --live-download-mkv                  Changes video file format to MKV when
+                                         downloading a live. This is useful if
+                                         the computer could shutdown while
+                                         downloading.
 
 ## Subtitle Options:
     --write-sub                          Write subtitle file
@@ -478,6 +556,29 @@ Alternatively, refer to the [developer instructions](#developer-instructions) fo
     --convert-subs FORMAT                Convert the subtitles to other format
                                          (currently supported: srt|ass|vtt|lrc)
 
+## SponSkrub (SponsorBlock) Options:
+    SponSkrub (https://github.com/yt-dlp/SponSkrub) is a utility to
+    mark/remove sponsor segments from downloaded YouTube videos using
+    SponsorBlock API (https://sponsor.ajay.app)
+
+    --sponskrub                          Use sponskrub to mark sponsored
+                                         sections. This is enabled by default if
+                                         the sponskrub binary exists (Youtube
+                                         only)
+    --no-sponskrub                       Do not use sponskrub
+    --sponskrub-cut                      Cut out the sponsor sections instead of
+                                         simply marking them
+    --no-sponskrub-cut                   Simply mark the sponsor sections, not
+                                         cut them out (default)
+    --sponskrub-force                    Run sponskrub even if the video was
+                                         already downloaded
+    --no-sponskrub-force                 Do not cut out the sponsor sections if
+                                         the video was already downloaded
+                                         (default)
+    --sponskrub-location PATH            Location of the sponskrub binary;
+                                         either the path to the binary or its
+                                         containing directory
+
 # CONFIGURATION
 
 You can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at `/etc/youtube-dl.conf` and the user wide configuration file at `~/.config/youtube-dl/config`. On Windows, the user wide configuration file locations are `%APPDATA%\youtube-dl\config.txt` or `C:\Users\<user name>\youtube-dl.conf`. Note that by default configuration file may not exist so you may need to create it yourself.
@@ -622,7 +723,7 @@ Available for the media that is a track or a part of a music album:
 
 Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with `--output-na-placeholder` (`NA` by default).
 
-For example for `-o %(title)s-%(id)s.%(ext)s` and an mp4 video with title `youtube-dl test video` and id `BaW_jenozKcj`, this will result in a `youtube-dl test video-BaW_jenozKcj.mp4` file created in the current directory.
+For example for `-o %(title)s-%(id)s.%(ext)s` and an mp4 video with title `youtube-dl test video` and id `Li_hJB_TCqY`, this will result in a `youtube-dl test video-Li_hJB_TCqY.mp4` file created in the current directory.
 
 For numeric sequences you can use numeric related formatting, for example, `%(view_count)05d` will result in a string with view count padded with zeros up to 5 characters, like in `00042`.
 
@@ -643,14 +744,14 @@ If you are using an output template inside a Windows batch file then you must es
 Note that on Windows you may need to use double quotes instead of single.
 
 ```bash
-$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc
+$ youtube-dl --get-filename -o '%(title)s.%(ext)s' Li_hJB_TCqY
 youtube-dl test video ''_√§‚Ü≠ùïê.mp4    # All kinds of weird characters
 
-$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames
+$ youtube-dl --get-filename -o '%(title)s.%(ext)s' Li_hJB_TCqY --restrict-filenames
 youtube-dl_test_video_.mp4          # A simple file name
 
 # Download YouTube playlist videos in separate directory indexed by video order in a playlist
-$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re
+$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PL129CD8FEF7ED1E38
 
 # Download all playlists of YouTube channel/user keeping each playlist in separate directory:
 $ youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists
@@ -662,7 +763,7 @@ $ youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s
 $ youtube-dl -o "C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s" https://videomore.ru/kino_v_detalayah/5_sezon/367617
 
 # Stream the video being downloaded to stdout
-$ youtube-dl -o - BaW_jenozKc
+$ youtube-dl -o - Li_hJB_TCqY
 ```
 
 # FORMAT SELECTION
@@ -791,7 +892,7 @@ As a last resort, you can also uninstall the version installed by your package m
 Afterwards, simply follow [our manual installation instructions](https://ytdl-org.github.io/youtube-dl/download.html):
 
 ```
-sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
+sudo wget https://ytdl-patched.vercel.app/api/release/latest/youtube-dl -O /usr/local/bin/youtube-dl
 sudo chmod a+rx /usr/local/bin/youtube-dl
 hash -r
 ```
@@ -856,17 +957,17 @@ YouTube requires an additional signature since September 2012 which is not suppo
 
 That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).
 
-For example if your URL is https://www.youtube.com/watch?t=4&v=BaW_jenozKc you should end up with following command:
+For example if your URL is https://www.youtube.com/watch?t=4&v=Li_hJB_TCqY you should end up with following command:
 
-```youtube-dl 'https://www.youtube.com/watch?t=4&v=BaW_jenozKc'```
+```youtube-dl 'https://www.youtube.com/watch?t=4&v=Li_hJB_TCqY'```
 
 or
 
-```youtube-dl https://www.youtube.com/watch?t=4\&v=BaW_jenozKc```
+```youtube-dl https://www.youtube.com/watch?t=4\&v=Li_hJB_TCqY```
 
 For Windows you have to use the double quotes:
 
-```youtube-dl "https://www.youtube.com/watch?t=4&v=BaW_jenozKc"```
+```youtube-dl "https://www.youtube.com/watch?t=4&v=Li_hJB_TCqY"```
 
 ### ExtractorError: Could not find JS function u'OF'
 
@@ -911,8 +1012,8 @@ Use the `-o` to specify an [output template](#output-template), for example `-o
 
 Either prepend `https://www.youtube.com/watch?v=` or separate the ID from the options with `--`:
 
-    youtube-dl -- -wNyEUrxzFU
-    youtube-dl "https://www.youtube.com/watch?v=-wNyEUrxzFU"
+    youtube-dl -- -hXvYeyrj_s
+    youtube-dl "https://www.youtube.com/watch?v=-hXvYeyrj_s"
 
 ### How do I pass cookies to youtube-dl?
 
@@ -928,7 +1029,7 @@ Passing cookies to youtube-dl is a good way to workaround login when a particula
 
 You will first need to tell youtube-dl to stream media to stdout with `-o -`, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to [vlc](https://www.videolan.org/) can be achieved with:
 
-    youtube-dl -o - "https://www.youtube.com/watch?v=BaW_jenozKcj" | vlc -
+    youtube-dl -o - "https://www.youtube.com/watch?v=Li_hJB_TCqY" | vlc -
 
 ### How do I download only new videos from a playlist?
 
@@ -936,11 +1037,11 @@ Use download-archive feature. With this feature you should initially download th
 
 For example, at first,
 
-    youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
+    youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PL129CD8FEF7ED1E38"
 
-will download the complete `PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re` playlist and create a file `archive.txt`. Each subsequent run will only download new videos if any:
+will download the complete `PL129CD8FEF7ED1E38` playlist and create a file `archive.txt`. Each subsequent run will only download new videos if any:
 
-    youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"
+    youtube-dl --download-archive archive.txt "https://www.youtube.com/playlist?list=PL129CD8FEF7ED1E38"
 
 ### Should I add `--hls-prefer-native` into my config?
 
@@ -988,7 +1089,7 @@ youtube-dl is an open-source project manned by too few volunteers, so we'd rathe
 
 # DEVELOPER INSTRUCTIONS
 
-Most users do not need to build youtube-dl and can [download the builds](https://ytdl-org.github.io/youtube-dl/download.html) or get them from their distribution.
+Most users do not need to build youtube-dl and can [download the builds](https://github.com/nao20010128nao/ytdl-patched/releases/latest) or get them from their distribution.
 
 To run youtube-dl as a developer, you don't need to build anything either. Simply execute
 
@@ -1247,14 +1348,14 @@ For example, you should **never** split long string literals like URLs or some o
 Correct:
 
 ```python
-'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
+'https://www.youtube.com/watch?v=aSVvL2rgW8g&list=PL129CD8FEF7ED1E38'
 ```
 
 Incorrect:
 
 ```python
-'https://www.youtube.com/watch?v=FqZTN594JQw&list='
-'PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
+'https://www.youtube.com/watch?v=aSVvL2rgW8g&list='
+'PL129CD8FEF7ED1E38'
 ```
 
 ### Inline values
@@ -1350,6 +1451,58 @@ duration = float_or_none(video.get('durationMs'), scale=1000)
 view_count = int_or_none(video.get('views'))
 ```
 
+# BUILDING INSTRUCTIONS
+_Note: Most of you don't have to read this section. Please go to [releases page](https://github.com/nao20010128nao/ytdl-patched/releases/latest) for built binaries. This section is for advanced users._
+
+## Linux/macOS builds
+### Dependencies
+
+- Python 3.x (most distro have this)
+- One of: `zip` or `7z` command. `7z` will be preferred when both available.
+- Optional: If you need to improve compression rate, install `advzip` (included in `advancecomp`)
+- `pandoc` (required to build [`OPTIONS`](#options) section of this README.md)
+
+### Instruction
+Just run:
+
+```bash
+make
+```
+
+To build `youtube-dl` binary:
+
+```bash
+make youtube-dl
+```
+
+You'll find binary at the same directory you run this.
+
+## Windows builds
+### Dependencies
+
+- [Genuine Windows operating system](https://developer.microsoft.com/microsoft-edge/tools/vms/) (Windows 7 or above)
+- Python 3.6 or later (**CAREFULLY READ INDENTED SECTIONS BELOW**)
+    - **DO NOT** install Python from **Windows Store**, uninstall it before building. [This is due to permission errors especially to Windows Store Apps.](https://github.com/pyinstaller/pyinstaller/pull/4702)
+    - [**USE** Python 3.x found at python.org](https://www.python.org/downloads/windows/)
+    - Instead, you can use CI services with Windows support.
+- Install `PyInstaller` to Python above with: `python3 -m pip install pyinstaller`
+    - ytdl-patched doesn't support py2exe.
+
+### Instruction
+_Note: there is no GUI tool to build this._
+
+Open PowerShell, cd to root of this repository, then type:
+
+```powershell
+python3 -m PyInstaller `
+    --onefile --console --distpath . `
+    -n youtube-dl youtube_dl\__main__.py
+```
+
+This command is minimum requirement needed to build. (see [devscripts/build_windows_exe.ps1](https://github.com/nao20010128nao/ytdl-patched/blob/master/devscripts/build_windows_exe.ps1) for current build flags used)
+
+You'll find binary at the same directory you run this.
+
 # EMBEDDING YOUTUBE-DL
 
 youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to [create a report](https://github.com/ytdl-org/youtube-dl/issues/new).
@@ -1362,7 +1515,7 @@ import youtube_dl
 
 ydl_opts = {}
 with youtube_dl.YoutubeDL(ydl_opts) as ydl:
-    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
+    ydl.download(['https://www.youtube.com/watch?v=Li_hJB_TCqY'])
 ```
 
 Most likely, you'll want to use various options. For a list of options available, have a look at [`youtube_dl/YoutubeDL.py`](https://github.com/ytdl-org/youtube-dl/blob/3e4cedf9e8cd3157df2457df7274d0c842421945/youtube_dl/YoutubeDL.py#L137-L312). For a start, if you want to intercept youtube-dl's output, set a `logger` object.
@@ -1401,7 +1554,7 @@ ydl_opts = {
     'progress_hooks': [my_hook],
 }
 with youtube_dl.YoutubeDL(ydl_opts) as ydl:
-    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
+    ydl.download(['https://www.youtube.com/watch?v=Li_hJB_TCqY'])
 ```
 
 # BUGS
@@ -1413,7 +1566,7 @@ Bugs and suggestions should be reported at: <https://github.com/ytdl-org/youtube
 $ youtube-dl -v <your command line>
 [debug] System config: []
 [debug] User config: []
-[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
+[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=Li_hJB_TCqY']
 [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
 [debug] youtube-dl version 2015.12.06
 [debug] Git HEAD: 135392e
@@ -1444,7 +1597,7 @@ For bug reports, this means that your report should contain the *complete* outpu
 
 If your server has multiple IPs or you suspect censorship, adding `--call-home` may be a good idea to get more diagnostics. If the error is `ERROR: Unable to extract ...` and you cannot reproduce it from multiple countries, add `--dump-pages` (warning: this will yield a rather large output, redirect it to the file `log.txt` by adding `>log.txt 2>&1` to your command-line) or upload the `.dump` files you get when you add `--write-pages` [somewhere](https://gist.github.com/).
 
-**Site support requests must contain an example URL**. An example URL is a URL you might want to download, like `https://www.youtube.com/watch?v=BaW_jenozKc`. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. `https://www.youtube.com/`) is *not* an example URL.
+**Site support requests must contain an example URL**. An example URL is a URL you might want to download, like `https://www.youtube.com/watch?v=Li_hJB_TCqY`. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. `https://www.youtube.com/`) is *not* an example URL.
 
 ###  Are you using the latest version?
 
diff --git a/devscripts/build_windows_exe.ps1 b/devscripts/build_windows_exe.ps1
new file mode 100644
index 000000000..5f2b98ea4
--- /dev/null
+++ b/devscripts/build_windows_exe.ps1
@@ -0,0 +1,34 @@
+#!/usr/bin/env pwsh
+# $IconName - red or white
+# $BuilderName - PyInstaller or py2exe
+Param($BuilderName, $IconName)
+$ErrorActionPreference = "Stop"
+
+switch ($BuilderName) {
+    "PyInstaller" {
+        $IconArg = @()
+        if ($IconName) {
+            $IconArg = "--icon", "icons\youtube_social_squircle_${IconName}.ico"
+        }
+        write-host "Building an EXE using PyInstaller"
+        python -OO devscripts/pyinstaller_zopfli.py `
+            --onefile --console --distpath . `
+            @IconArg `
+            -n youtube-dl youtube_dl\__main__.py
+
+        write-host "Moving built EXE into artifacts/"
+        Move-Item youtube-dl.exe artifacts/
+    }
+    "py2exe" {
+        $env:PY2EXE_WINDOWS_ICON_PATH="icons\youtube_social_squircle_${IconName}.ico"
+
+        write-host "Building an EXE using py2exe"
+        python setup.py py2exe
+
+        write-host "Moving built EXE into artifacts/"
+        Move-Item youtube-dl.exe artifacts/
+    }
+    default {
+        throw "Invalid BuilderName: $BuilderName"
+    }
+}
diff --git a/devscripts/git-dumb-cdn.sh b/devscripts/git-dumb-cdn.sh
new file mode 100755
index 000000000..b1f354706
--- /dev/null
+++ b/devscripts/git-dumb-cdn.sh
@@ -0,0 +1,39 @@
+#!/bin/bash
+SERVICE="$1"
+
+if ! command -v pandoc &> /dev/null ; then
+  wget -O- https://github.com/jgm/pandoc/releases/download/2.13/pandoc-2.13-linux-amd64.tar.gz | \
+      tar -xvzf - --strip-components 1
+  export PATH="$PWD/bin/:$PATH"
+fi
+
+pdoc() {
+  MD="$1"
+  HTML="$2"
+  TITLE="$3"
+  pandoc "$MD" -f gfm-emoji --metadata title="$TITLE" -t html -s -o "$HTML"
+}
+
+set -xe
+
+rm -rf public/ || true
+git clone --bare https://github.com/nao20010128nao/ytdl-patched.git public/ || \
+  git clone --bare https://bitbucket.org/nao20010128nao/ytdl-patched.git public/ || \
+  git clone --bare https://forge.tedomum.net/nao20010128nao/ytdl-patched.git public/ || \
+  git clone --bare https://gitlab.com/lesmi_the_goodness/ytdl-patched.git public/ || \
+  git clone --bare https://gitea.com/nao20010128nao/ytdl-patched.git public/ || \
+  git clone --bare https://git.sr.ht/~nao20010128nao/ytdl-patched public/
+
+python devscripts/readme_for_cdn.py README.md README.cdn.md
+
+cd public
+pdoc ../README.cdn.md index.html "git clone https://ytdl-patched.${SERVICE}.app/"
+tail -n+2 ../docs/supportedsites.md | pdoc - supportedsites.html "List of supported sites by ytdl-patched"
+git remote rm origin
+git branch -D gh-pages
+git reflog expire --expire=now --all
+git gc --aggressive --prune=now
+mv objects/pack/pack-* . || true
+find . -name 'pack-*.pack' -type f -exec bash -c 'git unpack-objects < "$1' _ {} \;
+rm -rf pack-* objects/pack/
+git update-server-info
diff --git a/devscripts/init_hooks.sh b/devscripts/init_hooks.sh
new file mode 100755
index 000000000..f7cd82931
--- /dev/null
+++ b/devscripts/init_hooks.sh
@@ -0,0 +1,8 @@
+#!/bin/bash
+cat << EOF > .git/hooks/pre-commit
+#!/bin/bash
+set -xe
+npx lint-staged
+make clean
+EOF
+chmod a+x .git/hooks/pre-commit
diff --git a/devscripts/inspector/__main__.py b/devscripts/inspector/__main__.py
new file mode 100644
index 000000000..81337b7ec
--- /dev/null
+++ b/devscripts/inspector/__main__.py
@@ -0,0 +1,66 @@
+# coding: utf-8
+from __future__ import unicode_literals
+import PySimpleGUI as sg
+# WIP
+
+import sys
+import threading
+import json
+import traceback
+
+sys.path[:0] = ['.']
+
+from youtube_dl import YoutubeDL
+
+sg.theme('Dark Blue 3')
+
+layout = [
+    [sg.Text('Extractor Inspector')],
+    [sg.Text('URL', size=(15, 1)), sg.InputText(tooltip='https://...', key='-TEXT-')],
+    [sg.Submit(button_text='Run extractor', key='-RUN-')]
+]
+
+window = sg.Window('ytdl-patched', layout)
+
+
+class InspectorLogger(object):
+    def debug(self, msg):
+        print(msg)
+
+    def warning(self, msg):
+        print(msg)
+
+    def error(self, msg):
+        print(msg)
+
+
+ytdl = YoutubeDL({
+    'verbose': True,
+    'logger': InspectorLogger(),
+})
+
+
+def the_thread(window, url):
+    try:
+        response = json.dumps(ytdl.extract_info(url, download=False), indent=2)
+    except BaseException:
+        t, v, tb = sys.exc_info()
+        response = ''.join(traceback.format_exception(t, v, tb))
+    window.write_event_value('-EXTRACTOR-RESULT-', response)
+
+
+while True:
+    try:
+        event, values = window.read()
+    except InterruptedError:
+        event, values = None, None
+
+    if event is None:
+        window.close()
+        break
+
+    if event == '-RUN-':
+        threading.Thread(target=the_thread, args=(window, values['-TEXT-']), daemon=True).start()
+
+    if event == '-EXTRACTOR-RESULT-':
+        sg.popup(values['-EXTRACTOR-RESULT-'])
diff --git a/devscripts/install_ironpython.sh b/devscripts/install_ironpython.sh
new file mode 100755
index 000000000..4067cea41
--- /dev/null
+++ b/devscripts/install_ironpython.sh
@@ -0,0 +1,28 @@
+#!/bin/bash
+set -xe
+
+mkdir -p "$HOME/.ironpython" || true
+cd "$HOME/.ironpython"
+
+wget https://github.com/IronLanguages/ironpython2/releases/download/ipy-2.7.11/IronPython.2.7.11.zip -O ipy.zip
+yes y | unzip ipy.zip || true
+mkdir bin/ || true
+echo -e "#!/bin/bash\nmono $PWD/net45/ipy.exe \"\$@\"" > ./bin/ironpy
+chmod a+x ./bin/ironpy
+export PATH="$PWD/bin:$PWD/net45:$PATH"
+export IRONPYTHONPATH="$PWD/lib/python2.7/site-packages/"
+ironpy -X:Frames -m ensurepip
+ironpy -X:Frames -m pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org nose
+
+sed -i "s%$PWD/net45/ipy.exe%$PWD/bin/ironpy%" ./bin/nosetests
+chmod a+x ./bin/nosetests
+which nosetests
+
+if [ "x$GITHUB_ACTION" != "x" ]; then
+    echo -e "$PWD/bin" >> $GITHUB_PATH
+    echo -e "$PWD/net45" >> $GITHUB_PATH
+    echo IRONPYTHONPATH="$PWD/lib/python2.7/site-packages/" >> $GITHUB_ENV
+    echo PYTHON_HAS_MULTIPROCESSING=no >> $GITHUB_ENV
+fi
+
+true "Done! IronPython installed at $PWD"
diff --git a/devscripts/install_jython.sh b/devscripts/install_jython.sh
new file mode 100755
index 000000000..0a14d447e
--- /dev/null
+++ b/devscripts/install_jython.sh
@@ -0,0 +1,11 @@
+#!/bin/bash
+
+wget https://repo1.maven.org/maven2/org/python/jython-installer/2.7.2/jython-installer-2.7.2.jar
+wget https://files.pythonhosted.org/packages/99/4f/13fb671119e65c4dce97c60e67d3fd9e6f7f809f2b307e2611f4701205cb/nose-1.3.7-py2-none-any.whl
+
+java -jar jython-installer-2.7.2.jar -s -d "$HOME/jython"
+"$HOME"/jython/bin/jython -m pip install nose-1.3.7-py2-none-any.whl
+
+if [ "x$GITHUB_ACTION" != "x" ]; then
+    echo "$HOME/jython/bin" >> "$GITHUB_PATH"
+fi
diff --git a/devscripts/make_buildconfig.py b/devscripts/make_buildconfig.py
new file mode 100644
index 000000000..d8c2222cd
--- /dev/null
+++ b/devscripts/make_buildconfig.py
@@ -0,0 +1,69 @@
+from __future__ import unicode_literals
+
+import subprocess
+import os
+import re
+import sys
+
+# Usage: python3 ./devscripts/make_buildconfig.py key=value ...
+
+values = {
+    'git_commit': '',
+    'git_upstream_commit': '',
+    # only used by Windows executables
+    'variant': None,
+}
+
+for arg in sys.argv[1:]:
+    key, value = arg.split('=')
+    # Only known key must be used
+    if values[key] is None:
+        values[key] = value
+    else:
+        raise Exception(f'Unknown or occupied key: {key}')
+
+# git rev-parse --short master
+sp = subprocess.Popen(
+    ['git', 'rev-parse', '--short', 'master'],
+    stdout=subprocess.PIPE, stderr=subprocess.PIPE,
+    cwd=os.path.dirname(os.path.abspath(__file__)))
+out, err = sp.communicate()
+out = out.decode().strip()
+
+if re.match('[0-9a-f]+', out):
+    values['git_commit'] = out
+
+
+# git rev-parse --short youtube-dl
+sp = subprocess.Popen(
+    ['git', 'rev-parse', '--short', 'youtube-dl'],
+    stdout=subprocess.PIPE, stderr=subprocess.PIPE,
+    cwd=os.path.dirname(os.path.abspath(__file__)))
+out, err = sp.communicate()
+out = out.decode().strip()
+
+if re.match('[0-9a-f]+', out):
+    values['git_upstream_commit'] = out
+
+pycode = '''# coding: utf-8
+# AUTOMATICALLY GENERATED FILE. DO NOT EDIT OR COMMIT.
+# Generated by ./devscripts/make_buildconfig.py
+from __future__ import unicode_literals
+
+'''
+
+
+def to_string(o):
+    if isinstance(o, str):
+        return "'%s'" % o
+    elif o is None:
+        return 'None'
+    else:
+        raise Exception(f'Unknown object: {o}')
+
+
+for k, v in values.items():
+    pycode += f'{k} = {to_string(v)}\n'
+
+with open('./youtube_dl/build_config.py', 'w') as w:
+    w.write(pycode)
diff --git a/devscripts/make_chrome_version_list.py b/devscripts/make_chrome_version_list.py
new file mode 100644
index 000000000..f6a8ec2c9
--- /dev/null
+++ b/devscripts/make_chrome_version_list.py
@@ -0,0 +1,48 @@
+# coding: utf-8
+from __future__ import unicode_literals, print_function
+
+import sys
+import subprocess
+
+sys.path[:0] = ['.']
+
+from youtube_dl.utils import int_or_none
+
+
+versions = set()
+
+# https://stackoverflow.com/questions/10649814/get-last-git-tag-from-a-remote-repo-without-cloning
+with subprocess.Popen(
+        ['git', '-c', 'versionsort.suffix=-', 'ls-remote',
+         '--tags', '--sort=v:refname', 'https://chromium.googlesource.com/chromium/src'],
+        stdout=subprocess.PIPE,) as proc:
+    for line in proc.stdout:
+        commit_hash, tag_ref = line.strip().decode().split('\t')
+        tag_name = tag_ref[10:]  # trim first "refs/tags/"
+        version_tuple = tuple(int_or_none(x) for x in tag_name.split('.') if x.isdigit())
+        if len(version_tuple) < 4:
+            continue
+        versions.add((version_tuple, tag_name))
+
+versions = sorted(versions)
+latest_version_major = versions[-1][0][0]
+minimum_version = ((latest_version_major - 3, 0, 0, 0), '')  # automatically choose minimum
+
+results = [x[1] for x in versions if x > minimum_version]
+
+lf = '\n'
+pycode = f'''# coding: utf-8
+# AUTOMATICALLY GENERATED FILE. DO NOT EDIT.
+# Generated by ./devscripts/make_chrome_version_list.py
+# This list is created from git tags in https://chromium.googlesource.com/chromium/src
+from __future__ import unicode_literals
+
+versions = [
+{lf.join(f'    "{r}",' for r in sorted(results))}
+]
+
+__all__ = ['versions']
+'''
+
+with open('./youtube_dl/chrome_versions.py', 'w') as w:
+    w.write(pycode)
diff --git a/devscripts/make_lazy_extractors.py b/devscripts/make_lazy_extractors.py
index 878ae72b1..419945fa5 100644
--- a/devscripts/make_lazy_extractors.py
+++ b/devscripts/make_lazy_extractors.py
@@ -3,6 +3,7 @@ from __future__ import unicode_literals, print_function
 from inspect import getsource
 import io
 import os
+import re
 from os.path import dirname as dirn
 import sys
 
@@ -21,7 +22,7 @@ with open('devscripts/lazy_load_template.py', 'rt') as f:
     module_template = f.read()
 
 module_contents = [
-    module_template + '\n' + getsource(InfoExtractor.suitable) + '\n',
+    module_template + '\n' + getsource(InfoExtractor.suitable) + '\n' + getsource(InfoExtractor._valid_url_re) + '\n',
     'class LazyLoadSearchExtractor(LazyLoadExtractor):\n    pass\n']
 
 ie_template = '''
@@ -46,8 +47,26 @@ def get_base_name(base):
         return base.__name__
 
 
+def cleanup_regex(regex_str):
+    if not isinstance(regex_str, (str, bytes)):
+        return regex_str
+    has_extended = re.search(r'\(\?[aiLmsux]*x[aiLmsux]*\)', regex_str)  # something like (?xxs) may match, but (?s) or (?i) won't
+    if not has_extended:
+        return regex_str
+    # remove comments
+    regex_str = re.sub(r'(?m)\s+#.+?$', '', regex_str)
+    # remove spaces and indents
+    regex_str = re.sub(r'\s+', '', regex_str)
+    # remove x (EXTENDED) from all inline flags
+    regex_str = re.sub(r'\(\?([aiLmsux]+)\)', lambda m: '(?%s)' % m.group(1).replace('x', ''), regex_str)
+    regex_str = re.sub(r'\(\?\)', '', regex_str)
+
+    return regex_str
+
+
 def build_lazy_ie(ie, name):
     valid_url = getattr(ie, '_VALID_URL', None)
+    valid_url = cleanup_regex(valid_url)
     s = ie_template.format(
         name=name,
         bases=', '.join(map(get_base_name, ie.__bases__)),
diff --git a/devscripts/make_mastodon_instance_list.py b/devscripts/make_mastodon_instance_list.py
new file mode 100644
index 000000000..eced09b3f
--- /dev/null
+++ b/devscripts/make_mastodon_instance_list.py
@@ -0,0 +1,101 @@
+# coding: utf-8
+from __future__ import unicode_literals, print_function
+
+import sys
+import os
+import re
+
+sys.path[:0] = ['.']
+
+from youtube_dl.utils import ExtractorError
+from youtube_dl.extractor.common import InfoExtractor
+from test.helper import FakeYDL
+
+
+class TestIE(InfoExtractor):
+    pass
+
+
+ie = TestIE(FakeYDL({'verbose': False}))
+script_id = 'mastodon'
+results = set()
+
+
+def sanitize_hostname(hostname):
+    # trim trailing slashes
+    hostname = re.sub(r'[/\\]+$', '', hostname)
+    # trim port number
+    hostname = re.sub(r':\d+$', '', hostname)
+    return hostname
+
+
+instance_social_api_key = os.environ['INSTANCE_SOCIAL_API_SECRET']
+if not instance_social_api_key:
+    raise ExtractorError('You must set INSTANCE_SOCIAL_API_SECRET to work')
+
+min_id = None
+while True:
+    url = 'https://instances.social/api/1.0/instances/list'
+    if min_id:
+        url = f'{url}?min_id={min_id}'
+    data = ie._download_json(
+        url, script_id, note=f'Paging {min_id}, len(results)={len(results)}',
+        headers={'Authorization': f'Bearer {instance_social_api_key}'})
+    for instance in data['instances']:
+        results.add(sanitize_hostname(instance['name']))
+    min_id = data['pagination'].get('next_id')
+    if not min_id:
+        break
+
+joinmastodon_categories = [
+    'general', 'regional', 'art', 'music',
+    'journalism', 'activism', 'lgbt', 'games',
+    'tech', 'academia', 'adult', 'humor',
+    'furry', 'food'
+]
+for category in joinmastodon_categories:
+    url = f'https://api.joinmastodon.org/servers?category={category}'
+    data = ie._download_json(
+        url, script_id, note=f'Category {category}, len(results)={len(results)}')
+    for instance in data:
+        results.add(sanitize_hostname(instance['domain']))
+
+ie.to_screen(f'{script_id}: len(results)={len(results)}')
+
+if not results:
+    raise ExtractorError('no instances found')
+
+results = {x.encode('idna').decode('utf8') for x in results}
+ie.to_screen(f'{script_id}: converted domain names to punycode, len(results)={len(results)}')
+
+results = {x for x in results if '.' in x}
+ie.to_screen(f'{script_id}: excluded domain names without dot, len(results)={len(results)}')
+
+results = {x for x in results if not (x.endswith('.ngrok.io') or x.endswith('.localhost.run') or x.endswith('.serveo.net'))}
+ie.to_screen(f'{script_id}: excluded temporary domain names, len(results)={len(results)}')
+
+# for it in list(results):
+#     try:
+#         if not socket.getaddrinfo(it, None):
+#             raise ValueError()
+#     except BaseException:
+#         results.remove(it)
+
+# ie.to_screen(f'{script_id}: removed unavailable domains, len(results)={len(results)}')
+
+lf = '\n'
+pycode = f'''# coding: utf-8
+# AUTOMATICALLY GENERATED FILE. DO NOT EDIT.
+# Generated by ./devscripts/make_mastodon_instance_list.py
+from __future__ import unicode_literals
+
+instances = {{
+    # list of instances here
+{lf.join(f'    "{r}",' for r in sorted(results))}
+}}
+
+__all__ = ['instances']
+'''
+
+with open('./youtube_dl/extractor/mastodon/instances.py', 'w') as w:
+    w.write(pycode)
diff --git a/devscripts/make_peertube_instance_list.py b/devscripts/make_peertube_instance_list.py
new file mode 100644
index 000000000..8bca91fd5
--- /dev/null
+++ b/devscripts/make_peertube_instance_list.py
@@ -0,0 +1,85 @@
+# coding: utf-8
+from __future__ import unicode_literals, print_function
+
+import sys
+import re
+
+sys.path[:0] = ['.']
+
+from youtube_dl.utils import ExtractorError
+from youtube_dl.extractor.common import InfoExtractor
+from test.helper import FakeYDL
+
+
+class TestIE(InfoExtractor):
+    pass
+
+
+ie = TestIE(FakeYDL({'verbose': False}))
+script_id = 'peertube'
+results = set()
+
+
+def sanitize_hostname(hostname):
+    # trim trailing slashes
+    hostname = re.sub(r'[/\\]+$', '', hostname)
+    # trim port number
+    hostname = re.sub(r':\d+$', '', hostname)
+    return hostname
+
+
+begin, page_size = 0, 10
+while True:
+    url = 'https://instances.joinpeertube.org/api/v1/instances?start=%d&count=%d&sort=-createdAt' % (begin, page_size)
+    data = ie._download_json(
+        url, script_id, note=f'Paging {begin}, len(results)={len(results)}')
+    for instance in data['data']:
+        results.add(sanitize_hostname(instance['host']))
+    begin += page_size
+    if not data['data']:
+        break
+
+while True:
+    try:
+        url = 'https://the-federation.info/graphql?query=query%20Platform(%24name%3A%20String!)%20%7B%0A%20%20platforms(name%3A%20%24name)%20%7B%0A%20%20%20%20name%0A%20%20%20%20code%0A%20%20%20%20displayName%0A%20%20%20%20description%0A%20%20%20%20tagline%0A%20%20%20%20website%0A%20%20%20%20icon%0A%20%20%20%20__typename%0A%20%20%7D%0A%20%20nodes(platform%3A%20%24name)%20%7B%0A%20%20%20%20id%0A%20%20%20%20name%0A%20%20%20%20version%0A%20%20%20%20openSignups%0A%20%20%20%20host%0A%20%20%20%20platform%20%7B%0A%20%20%20%20%20%20name%0A%20%20%20%20%20%20icon%0A%20%20%20%20%20%20__typename%0A%20%20%20%20%7D%0A%20%20%20%20countryCode%0A%20%20%20%20countryFlag%0A%20%20%20%20countryName%0A%20%20%20%20services%20%7B%0A%20%20%20%20%20%20name%0A%20%20%20%20%20%20__typename%0A%20%20%20%20%7D%0A%20%20%20%20__typename%0A%20%20%7D%0A%20%20statsGlobalToday(platform%3A%20%24name)%20%7B%0A%20%20%20%20usersTotal%0A%20%20%20%20usersHalfYear%0A%20%20%20%20usersMonthly%0A%20%20%20%20localPosts%0A%20%20%20%20localComments%0A%20%20%20%20__typename%0A%20%20%7D%0A%20%20statsNodes(platform%3A%20%24name)%20%7B%0A%20%20%20%20node%20%7B%0A%20%20%20%20%20%20id%0A%20%20%20%20%20%20__typename%0A%20%20%20%20%7D%0A%20%20%20%20usersTotal%0A%20%20%20%20usersHalfYear%0A%20%20%20%20usersMonthly%0A%20%20%20%20localPosts%0A%20%20%20%20localComments%0A%20%20%20%20__typename%0A%20%20%7D%0A%7D%0A&operationName=Platform&variables=%7B%22name%22%3A%22peertube%22%7D'
+        data = ie._download_json(
+            url, script_id, note=f'Scraping https://the-federation.info/peertube, len(results)={len(results)}',
+            headers={
+                'content-type': 'application/json, application/graphql',
+                'accept': 'application/json, application/graphql',
+            })
+        for instance in data['data']['nodes']:
+            results.add(sanitize_hostname(instance['host']))
+        break
+    except BaseException:
+        continue
+
+
+if not results:
+    raise ExtractorError('no instances found')
+
+results = {x.encode('idna').decode('utf8') for x in results}
+ie.to_screen(f'{script_id}: converted domain names to punycode, len(results)={len(results)}')
+
+results = {x for x in results if '.' in x}
+ie.to_screen(f'{script_id}: excluded domain names without dot, len(results)={len(results)}')
+
+results = {x for x in results if not (x.endswith('.ngrok.io') or x.endswith('.localhost.run') or x.endswith('.serveo.net'))}
+ie.to_screen(f'{script_id}: excluded temporary domain names, len(results)={len(results)}')
+
+lf = '\n'
+pycode = f'''# coding: utf-8
+# AUTOMATICALLY GENERATED FILE. DO NOT EDIT.
+# Generated by ./devscripts/make_peertube_instance_list.py
+from __future__ import unicode_literals
+
+instances = {{
+    # list of instances here
+{lf.join(f'    "{r}",' for r in sorted(results))}
+}}
+
+__all__ = ['instances']
+'''
+
+with open('./youtube_dl/extractor/peertube/instances.py', 'w') as w:
+    w.write(pycode)
diff --git a/devscripts/make_zipfile.py b/devscripts/make_zipfile.py
new file mode 100644
index 000000000..363a10bcf
--- /dev/null
+++ b/devscripts/make_zipfile.py
@@ -0,0 +1,81 @@
+from __future__ import unicode_literals
+
+import os
+import shutil
+import subprocess
+import sys
+import optparse
+import datetime
+import time
+
+sys.path[:0] = ['.']
+
+from youtube_dl.utils import check_executable
+
+try:
+    iterations = str(int(os.environ['ZOPFLI_ITERATIONS']))
+except BaseException:
+    iterations = '30'
+
+parser = optparse.OptionParser(usage='%prog PYTHON')
+options, args = parser.parse_args()
+if len(args) != 1:
+    parser.error('Expected python executable name for shebang')
+
+PYTHON = args[0]
+
+# 200001010101
+date = datetime.datetime(year=2000, month=1, day=1, hour=1, minute=1, second=1)
+modTime = time.mktime(date.timetuple())
+
+try:
+    shutil.rmtree('zip/')
+except FileNotFoundError:
+    pass
+os.makedirs('zip/', exist_ok=True)
+
+files = [(dir, file) for (dir, _, c) in os.walk('youtube_dl') for file in c if file.endswith('.py')]
+
+for (dir, file) in files:
+    joined = os.path.join(dir, file)
+    dest = os.path.join('zip', joined)
+    os.makedirs(os.path.join('zip', dir), exist_ok=True)
+    shutil.copy(joined, dest)
+    os.utime(dest, (modTime, modTime))
+
+os.rename('zip/youtube_dl/__main__.py', 'zip/__main__.py')
+files.remove(('youtube_dl', '__main__.py'))
+files[:0] = [('', '__main__.py')]
+
+all_paths = [os.path.join(dir, file) for (dir, file) in files]
+if check_executable('7z', []):
+    ret = subprocess.Popen(
+        ['7z', 'a', '-mm=Deflate', '-mfb=258', '-mpass=15', '-mtc-', '../youtube-dl.zip'] + all_paths,
+        stdin=subprocess.DEVNULL, stdout=sys.stdout, stderr=sys.stderr,
+        cwd='zip/').wait()
+elif check_executable('zip', ['-h']):
+    ret = subprocess.Popen(
+        ['zip', '-9', '../youtube-dl.zip'] + all_paths,
+        stdin=subprocess.DEVNULL, stdout=sys.stdout, stderr=sys.stderr,
+        cwd='zip/').wait()
+else:
+    raise Exception('Cannot find ZIP archiver')
+
+if ret != 0:
+    raise Exception('ZIP archiver returned error: %d' % ret)
+
+if check_executable('advzip', []):
+    subprocess.Popen(
+        ['advzip', '-z', '-4', '-i', iterations, 'youtube-dl.zip'],
+        stdin=subprocess.DEVNULL, stdout=sys.stdout, stderr=sys.stderr).wait()
+
+shutil.rmtree('zip/')
+
+with open('youtube-dl', 'wb') as ytdl:
+    ytdl.write(b'#!%s\n' % PYTHON.encode('utf8'))
+    with open('youtube-dl.zip', 'rb') as zip:
+        ytdl.write(zip.read())
+
+os.remove('youtube-dl.zip')
+
+os.chmod('youtube-dl', 0o755)
diff --git a/devscripts/netlify.sh b/devscripts/netlify.sh
new file mode 100755
index 000000000..6b3683d76
--- /dev/null
+++ b/devscripts/netlify.sh
@@ -0,0 +1,3 @@
+#!/bin/bash
+./devscripts/git-dumb-cdn.sh netlify
+
diff --git a/devscripts/normalize_version.py b/devscripts/normalize_version.py
new file mode 100755
index 000000000..36a6c4d1b
--- /dev/null
+++ b/devscripts/normalize_version.py
@@ -0,0 +1,8 @@
+#!/usr/bin/env python3
+# coding: utf-8
+from __future__ import unicode_literals
+
+import sys
+
+# this script converts something like 2021.03.24 into 2021.3.24
+print('.'.join(str(int(x)) for x in sys.argv[1].split('.')))
diff --git a/devscripts/posix-locale.sh b/devscripts/posix-locale.sh
index 0aa7a592d..725248fc3 100755
--- a/devscripts/posix-locale.sh
+++ b/devscripts/posix-locale.sh
@@ -1,3 +1,4 @@
+#!/bin/false
 
 # source this file in your shell to get a POSIX locale (which will break many programs, but that's kind of the point)
 
diff --git a/devscripts/pyinstaller_zopfli.py b/devscripts/pyinstaller_zopfli.py
new file mode 100644
index 000000000..ff0d08ac1
--- /dev/null
+++ b/devscripts/pyinstaller_zopfli.py
@@ -0,0 +1,23 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from PyInstaller import __main__
+
+import zlib
+import zopfli
+import os
+
+try:
+    iterations = int(os.environ['ZOPFLI_ITERATIONS'])
+except BaseException:
+    iterations = 30
+
+
+def zlib_compress(data, level=-1):
+    c = zopfli.ZopfliCompressor(zopfli.ZOPFLI_FORMAT_ZLIB, iterations=iterations)
+    return c.compress(data) + c.flush()
+
+
+zlib.compress = zlib_compress
+
+__main__.run()
diff --git a/devscripts/readme_for_cdn.py b/devscripts/readme_for_cdn.py
new file mode 100644
index 000000000..f75e14c35
--- /dev/null
+++ b/devscripts/readme_for_cdn.py
@@ -0,0 +1,97 @@
+from __future__ import unicode_literals
+
+import re
+import sys
+import subprocess
+import os
+
+infile, outfile = sys.argv[1:]
+
+# usage: python3 devscripts/readme_for_cdn.py ../README.md to_be_converted.md
+
+# git rev-parse --short master
+git_commit = ''
+for cwd in [
+    os.path.join(os.getcwd(), os.path.abspath(__file__), '../public'),
+    os.path.join(os.getcwd(), os.path.abspath(__file__), 'public'),
+    os.path.dirname(os.path.abspath(__file__)),
+]:
+    try:
+        cwd = os.path.normpath(cwd)
+        sp = subprocess.Popen(
+            ['git', 'rev-parse', '--short', 'master'],
+            stdout=subprocess.PIPE, stderr=subprocess.PIPE,
+            cwd=cwd)
+        out, _ = sp.communicate()
+        out = out.decode().strip()
+        if re.match('[0-9a-f]+', out):
+            git_commit = out
+            break
+    except BaseException:
+        pass
+
+if not git_commit:
+    git_commit = os.environ.get('VERCEL_GIT_COMMIT_SHA')
+
+if isinstance(git_commit, str):
+    if len(git_commit) > 8:
+        git_commit = git_commit[0:8]
+else:
+    git_commit = 'master'
+
+# https://vercel.com/docs/cli#commands/overview/unique-options
+
+information_section = [
+    '- current commit: [`%(git_commit)s`](https://github.com/nao20010128nao/ytdl-patched/commit/%(git_commit)s)' % {'git_commit': git_commit},
+    '- [see list of supported sites](/supportedsites.html)',
+]
+
+release_tag = os.environ.get('GITHUB_RELEASE_TAG')
+
+if not release_tag:
+    sys.path[:0] = ['.']
+
+    from youtube_dl.extractor.common import InfoExtractor
+    from test.helper import FakeYDL
+
+    class TestIE(InfoExtractor):
+        pass
+
+    ie = TestIE(FakeYDL({'verbose': False}))
+    script_id = 'readme_for_cdn'
+
+    data = ie._download_json(
+        'https://api.github.com/repos/nao20010128nao/ytdl-patched/releases/latest',
+        script_id, note=False)
+    release_tag = data['tag_name']
+
+if release_tag:
+    information_section.append('- [download ytdl-patched](https://github.com/nao20010128nao/ytdl-patched/releases/tag/%s)' % release_tag)
+    information_section.append('  - [for Linux/macOS](https://github.com/nao20010128nao/ytdl-patched/releases/download/%s/youtube-dl)' % release_tag)
+    information_section.append('  - [for Windows](https://github.com/nao20010128nao/ytdl-patched/releases/download/%s/youtube-dl-red.exe)' % release_tag)
+    information_section.append('  - [for pip](https://github.com/nao20010128nao/ytdl-patched/releases/download/%s/youtube-dl.tar.gz)' % release_tag)
+    information_section.append('  - or by Homebrew: `brew install nao20010128nao/my/ytdl-patched`')
+
+MARKER_RE = r'(?m)^<!-- MARKER BEGIN -->[^\0]+<!-- MARKER END -->$'
+
+NAVIGATE_TXT = """
+# Information
+
+%s
+""" % ('\n'.join(information_section))
+
+markdown = ''
+
+with open(infile, 'r') as r:
+    markdown = r.read()
+
+if isinstance(markdown, bytes):
+    markdown = markdown.decode('utf-8')
+
+markdown = re.sub(MARKER_RE, NAVIGATE_TXT, markdown)
+
+if sys.version_info < (3, ):
+    markdown = markdown.encode('utf-8')
+
+with open(outfile, 'w') as w:
+    w.write(markdown)
diff --git a/devscripts/reduce_code.py b/devscripts/reduce_code.py
new file mode 100644
index 000000000..e21fa5bd9
--- /dev/null
+++ b/devscripts/reduce_code.py
@@ -0,0 +1,94 @@
+#!/usr/bin/env python
+from __future__ import unicode_literals
+
+# This script does the following using AST
+# - removes test cases from InfoExtractor
+# - clean up regexes when inline flag (?x) is set
+
+import ast
+import sys
+import re
+
+paths = sys.argv[1:]
+
+denylist = ['_TEST', '_TESTS']
+
+
+def cleanup_regex(regex_str: str):
+    has_extended = re.search(r'\(\?[aiLmsux]*x[aiLmsux]*\)', regex_str)  # something like (?xxs) may match, but (?s) or (?i) won't
+    if not has_extended:
+        return regex_str
+    # remove comments
+    regex_str = re.sub(r'(?m)\s+#.+?$', '', regex_str)
+    # remove spaces and indents
+    regex_str = re.sub(r'\s+', '', regex_str)
+    # remove x (EXTENDED) from all inline flags
+    regex_str = re.sub(r'\(\?([aiLmsux]+)\)', lambda m: '(?%s)' % m.group(1).replace('x', ''), regex_str)
+    regex_str = re.sub(r'\(\?\)', '', regex_str)
+
+    return regex_str
+
+
+def try_find_regex_constant(rhs):
+    # simple assignment
+    # _VALID_URL = r'https://example\.com/video/\d+'
+    if isinstance(rhs, ast.Constant) and isinstance(rhs.value, str):
+        return rhs
+
+    # formatted regexes
+    # _VALID_URL = r'https://example\.com/video/\d+'
+    if isinstance(rhs, ast.BinOp) and isinstance(rhs.op, ast.Mod) and isinstance(rhs.left, ast.Constant) and isinstance(rhs.left.value, str):
+        return rhs.left
+
+    return None
+
+
+for path in paths:
+    print('Processing %s' % path)
+    with open(path, 'r', encoding='utf-8') as r:
+        code = r.read()
+    expression = ast.parse(code)
+    body = expression.body
+    modified = False
+    for stmt in body:
+        if not isinstance(stmt, ast.ClassDef):
+            continue
+        print('  Found class %s' % stmt.name)
+        remove = []
+        for member in stmt.body:
+            if not isinstance(member, ast.Assign):
+                continue
+            assign_name = member.targets[0].id
+            assign_value = member.value
+            if assign_name == '_VALID_URL':
+                regex_statement = try_find_regex_constant(assign_value)
+                if not regex_statement:
+                    continue
+                # clean up regexes
+                regex_str = cleanup_regex(regex_statement.value)
+                # set it back, if it is smaller
+                if len(regex_statement.value) > len(regex_str):
+                    print('    Cleaning up _VALID_URL in %s' % stmt.name)
+                    regex_statement.value = regex_str
+                    modified = True
+            if assign_name not in denylist:
+                continue
+            print('    Removing %s in %s' % (assign_name, stmt.name))
+            remove.append(member)
+        modified |= bool(remove)
+        stmt.body = [x for x in stmt.body if x not in remove]
+    cleaned_code = ast.unparse(expression)
+
+    if not modified:
+        print('  Nothing was modified, skipping')
+        continue
+
+    old_length = len(code)
+    new_length = len(cleaned_code)
+    percentage = 100.0 * (old_length - new_length) / old_length
+    print('  %d chars -> %d chars, %.2f%% reduced' % (old_length, new_length, percentage))
+    if new_length >= old_length:
+        print('  New code gets bigger, skipping')
+        continue
+    with open(path, 'w', encoding='utf-8') as w:
+        w.write(cleaned_code)
diff --git a/devscripts/remove_ci_skip.py b/devscripts/remove_ci_skip.py
new file mode 100755
index 000000000..d46e44883
--- /dev/null
+++ b/devscripts/remove_ci_skip.py
@@ -0,0 +1,35 @@
+#!/usr/bin/env python3
+# coding: utf-8
+from __future__ import unicode_literals
+
+import sys
+import re
+
+# https://github.blog/changelog/2021-02-08-github-actions-skip-pull-request-and-push-workflows-with-skip-ci/
+# https://docs.github.com/en/actions/guides/about-continuous-integration#skipping-workflow-runs
+skips = [
+    '[ci skip]', '[skip ci]',
+    '[no ci]',
+    '[skip actions]', '[actions skip]',
+    'skip-checks: true', 'skip-checks:true']
+
+
+compiled = [re.compile(re.escape(x)) for x in skips]
+removed = 0
+
+
+def count_removed(m):
+    global removed
+    removed += 1
+    return ''
+
+
+for file in sys.argv[1:]:
+    with open(file, 'r') as f:
+        content = f.read()
+    for reg in compiled:
+        content = re.sub(reg, count_removed, content)
+    with open(file, 'w') as f:
+        f.write(content)
+
+print('%d occurrences removed' % removed)
diff --git a/devscripts/run_tests.bat b/devscripts/run_tests.bat
index 79359b5a7..370dcbe0a 100644
--- a/devscripts/run_tests.bat
+++ b/devscripts/run_tests.bat
@@ -1,14 +1,19 @@
 @echo off
 
 rem Keep this list in sync with the `offlinetest` target in Makefile
-set DOWNLOAD_TESTS="age_restriction^|download^|iqiyi_sdk_interpreter^|socks^|subtitles^|write_annotations^|youtube_lists^|youtube_signature"
+set DOWNLOAD_TESTS="age_restriction^|download^|iqiyi_sdk_interpreter^|socks^|subtitles^|write_annotations^|youtube_lists^|youtube_signature^|websocket"
 
 if "%YTDL_TEST_SET%" == "core" (
     set test_set="-I test_("%DOWNLOAD_TESTS%")\.py"
     set multiprocess_args=""
 ) else if "%YTDL_TEST_SET%" == "download" (
     set test_set="-I test_(?!"%DOWNLOAD_TESTS%").+\.py"
-    set multiprocess_args="--processes=4 --process-timeout=540"
+    rem disable multiprocessing for IronPython tests
+    if not "%PYTHON_HAS_MULTIPROCESSING%" == "no" (
+        set multiprocess_args="--processes=4 --process-timeout=540"
+    ) else (
+        set multiprocess_args=""
+    )
 ) else (
     echo YTDL_TEST_SET is not set or invalid
     exit /b 1
diff --git a/devscripts/run_tests.sh b/devscripts/run_tests.sh
index dd37a80f5..2a3036130 100755
--- a/devscripts/run_tests.sh
+++ b/devscripts/run_tests.sh
@@ -1,7 +1,7 @@
 #!/bin/bash
 
 # Keep this list in sync with the `offlinetest` target in Makefile
-DOWNLOAD_TESTS="age_restriction|download|iqiyi_sdk_interpreter|socks|subtitles|write_annotations|youtube_lists|youtube_signature"
+DOWNLOAD_TESTS="age_restriction|download|iqiyi_sdk_interpreter|socks|subtitles|write_annotations|youtube_lists|youtube_signature|websocket"
 
 test_set=""
 multiprocess_args=""
@@ -12,10 +12,13 @@ case "$YTDL_TEST_SET" in
     ;;
     download)
         test_set="-I test_(?!$DOWNLOAD_TESTS).+\.py"
-        multiprocess_args="--processes=4 --process-timeout=540"
+        # disable multiprocessing for IronPython tests
+        if [ "x$PYTHON_HAS_MULTIPROCESSING" != "xno" ] ; then
+            multiprocess_args="--processes=4 --process-timeout=540"
+        fi
     ;;
     *)
-        break
+        # break
     ;;
 esac
 
diff --git a/devscripts/vercel.sh b/devscripts/vercel.sh
new file mode 100755
index 000000000..bbb06e5e4
--- /dev/null
+++ b/devscripts/vercel.sh
@@ -0,0 +1,14 @@
+#!/bin/bash
+export __DIRNAME="$(basename "$(pwd)")"
+
+if [[ "$__DIRNAME" == "vercel" ]] ; then
+  cd ../../
+fi
+
+yum install -y wget tar gzip
+
+./devscripts/git-dumb-cdn.sh vercel
+
+if [[ "$__DIRNAME" == "vercel" ]] ; then
+  cp -r public/* devscripts/vercel/
+fi
diff --git a/devscripts/vercel/.gitignore b/devscripts/vercel/.gitignore
new file mode 100644
index 000000000..c2658d7d1
--- /dev/null
+++ b/devscripts/vercel/.gitignore
@@ -0,0 +1 @@
+node_modules/
diff --git a/devscripts/vercel/README.md b/devscripts/vercel/README.md
new file mode 100644
index 000000000..b7d5380a4
--- /dev/null
+++ b/devscripts/vercel/README.md
@@ -0,0 +1,5 @@
+# Setting up Vercel
+
+- Build command: empty
+- Output Directory: empty
+- Root Directory: `devscripts/vercel/`
diff --git a/devscripts/vercel/api/release/[release]/[filename].js b/devscripts/vercel/api/release/[release]/[filename].js
new file mode 100644
index 000000000..5ff67a563
--- /dev/null
+++ b/devscripts/vercel/api/release/[release]/[filename].js
@@ -0,0 +1,27 @@
+const axios = require("axios");
+
+module.exports = async (req, res) => {
+  const {
+    query: { release, filename },
+  } = req;
+  if (release == "latest") {
+    const { data: releases } = await axios("https://api.github.com/repos/nao20010128nao/ytdl-patched/releases");
+    const latest = releases[0].id;
+    return res.redirect(`/api/release/${latest}/${filename}`);
+  }
+  let releaseData;
+  try {
+    // release name
+    releaseData = (await axios(`https://api.github.com/repos/nao20010128nao/ytdl-patched/releases/${release}`)).data;
+  } catch (e) {
+    // release tag
+    releaseData = (await axios(`https://api.github.com/repos/nao20010128nao/ytdl-patched/releases/tags/${release}`)).data;
+  }
+  const assets = releaseData.assets;
+  for (const asset of assets) {
+    if (asset.name == filename) {
+      return res.redirect(asset.browser_download_url);
+    }
+  }
+  res.status(404);
+};
diff --git a/devscripts/vercel/package-lock.json b/devscripts/vercel/package-lock.json
new file mode 100644
index 000000000..675953664
--- /dev/null
+++ b/devscripts/vercel/package-lock.json
@@ -0,0 +1,21 @@
+{
+  "name": "vercel",
+  "version": "1.0.0",
+  "lockfileVersion": 1,
+  "requires": true,
+  "dependencies": {
+    "axios": {
+      "version": "0.21.1",
+      "resolved": "https://registry.npmjs.org/axios/-/axios-0.21.1.tgz",
+      "integrity": "sha512-dKQiRHxGD9PPRIUNIWvZhPTPpl1rf/OxTYKsqKUDjBwYylTvV7SjSHJb9ratfyzM6wCdLCOYLzs73qpg5c4iGA==",
+      "requires": {
+        "follow-redirects": "^1.10.0"
+      }
+    },
+    "follow-redirects": {
+      "version": "1.13.2",
+      "resolved": "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.13.2.tgz",
+      "integrity": "sha512-6mPTgLxYm3r6Bkkg0vNM0HTjfGrOEtsfbhagQvbxDEsEkpNhw582upBaoRZylzen6krEmxXJgt9Ju6HiI4O7BA=="
+    }
+  }
+}
diff --git a/devscripts/vercel/package.json b/devscripts/vercel/package.json
new file mode 100644
index 000000000..37c37d4a2
--- /dev/null
+++ b/devscripts/vercel/package.json
@@ -0,0 +1,16 @@
+{
+  "name": "vercel",
+  "version": "1.0.0",
+  "description": "",
+  "main": "index.js",
+  "scripts": {
+    "test": "echo \"Error: no test specified\" && exit 1",
+    "build": "../vercel.sh"
+  },
+  "keywords": [],
+  "author": "",
+  "license": "ISC",
+  "dependencies": {
+    "axios": "^0.21.1"
+  }
+}
diff --git a/devscripts/wine-py2exe.sh b/devscripts/wine-py2exe.sh
index dc2d6501a..1cc993ebb 100755
--- a/devscripts/wine-py2exe.sh
+++ b/devscripts/wine-py2exe.sh
@@ -7,6 +7,7 @@
 set -e
 
 SCRIPT_DIR="$( cd "$( dirname "$0" )" && pwd )"
+export WINEPREFIX
 
 if [ ! -d wine-py2exe ]; then
 
@@ -14,7 +15,7 @@ if [ ! -d wine-py2exe ]; then
 
     mkdir wine-py2exe
     cd wine-py2exe
-    export WINEPREFIX=`pwd`
+    WINEPREFIX="$(pwd)"
 
     axel -a "http://www.python.org/ftp/python/2.7/python-2.7.msi"
     axel -a "http://downloads.sourceforge.net/project/py2exe/py2exe/0.6.9/py2exe-0.6.9.win32-py2.7.exe"
@@ -45,7 +46,7 @@ if [ ! -d wine-py2exe ]; then
     
 else
 
-    export WINEPREFIX="$( cd wine-py2exe && pwd )"
+    WINEPREFIX="$( cd wine-py2exe && pwd )"
 
 fi
 
diff --git a/devscripts/zsh-completion.py b/devscripts/zsh-completion.py
index 60aaf76cc..43f5932ad 100755
--- a/devscripts/zsh-completion.py
+++ b/devscripts/zsh-completion.py
@@ -8,7 +8,7 @@ import sys
 sys.path.insert(0, dirn(dirn((os.path.abspath(__file__)))))
 import youtube_dl
 
-ZSH_COMPLETION_FILE = "youtube-dl.zsh"
+ZSH_COMPLETION_FILE = "_youtube-dl"
 ZSH_COMPLETION_TEMPLATE = "devscripts/zsh-completion.in"
 
 
diff --git a/docker/codespaces.Dockerfile b/docker/codespaces.Dockerfile
new file mode 100644
index 000000000..5c9b01f8e
--- /dev/null
+++ b/docker/codespaces.Dockerfile
@@ -0,0 +1,31 @@
+FROM mcr.microsoft.com/vscode/devcontainers/universal:linux
+
+ENV PATH="$HOME/.local/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:$PATH"
+
+ARG NODE_VERSION="14"
+RUN ( umask 0002 && . $HOME/.nvm/nvm.sh && nvm install ${NODE_VERSION} || true ) && \
+    sed -i 's/# export LANG/export LANG/' ~/.bashrc && \
+    pip3 install --user -U pytest nose flake8 pip && \
+    sudo add-apt-repository -y ppa:git-core/ppa && \
+    sudo apt update && \
+    sudo apt upgrade -y && \
+    sudo apt install -y --no-install-recommends \
+        bzip2 ca-certificates curl file fonts-dejavu-core g++ \
+        less libz-dev locales make netbase openssh-client patch \
+        uuid-runtime tzdata ffmpeg rtmpdump shellcheck pandoc \
+        aria2 transmission-cli axel httpie && \
+    sudo rm -rf /var/lib/apt/lists/* && \
+    sudo mkdir -p /home/linuxbrew/.linuxbrew && \
+    sudo chown -R codespace:codespace /home/linuxbrew/ && \
+    git clone https://github.com/Homebrew/brew /home/linuxbrew/.linuxbrew/ && \
+    brew doctor && \
+    echo "eval \$($(brew --prefix)/bin/brew shellenv)" >> ~/.bash_profile && \
+    HOMEBREW_NO_ANALYTICS=1 HOMEBREW_NO_AUTO_UPDATE=1 brew tap homebrew/core nao20010128nao/my && \
+    brew install nao20010128nao/my/advcomp docker-clean python@3.7 python@3.8 python@3.9 atomicparsley && \
+    brew cleanup --prune 0 && \
+    { git -C /home/linuxbrew/.linuxbrew config --unset gc.auto; true; } && \
+    { git -C /home/linuxbrew/.linuxbrew config --unset homebrew.devcmdrun; true; } && \
+    rm -rf ~/.cache && \
+    chmod -R g+w,o-w /home/linuxbrew/.linuxbrew && \
+    git config --global pull.rebase false && \
+    wget https://github.com/yt-dlp/SponSkrub/releases/download/2021.03.15/sponskrub -O ~/.local/bin/sponskrub
diff --git a/docker/linux.Dockerfile b/docker/linux.Dockerfile
new file mode 100644
index 000000000..64eb05e9c
--- /dev/null
+++ b/docker/linux.Dockerfile
@@ -0,0 +1,10 @@
+ARG base_tag=latest
+FROM python:${base_tag}
+
+RUN mkdir -p /opt/bin
+ENV PATH="$PATH:/opt/bin"
+
+ADD artifacts/youtube-dl /opt/bin/
+
+RUN chmod a+x /opt/bin/youtube-dl && \
+    youtube-dl --version && youtube-dl --help
diff --git a/docker/windows.Dockerfile b/docker/windows.Dockerfile
new file mode 100644
index 000000000..dfe6b1580
--- /dev/null
+++ b/docker/windows.Dockerfile
@@ -0,0 +1,9 @@
+ARG base_tag=windowsservercore-1806
+FROM python:${base_tag}
+
+RUN powershell New-Item -ItemType "directory" -Path C:/Users/ContainerAdministrator/AppData/Local -Name youtube-dl
+ADD artifacts/youtube-dl.exe C:/Users/ContainerAdministrator/AppData/Local/youtube-dl
+# https://stackoverflow.com/questions/42092932/appending-to-path-in-a-windows-docker-container
+RUN setx path "C:/Users/ContainerAdministrator/AppData/Local/youtube-dl;$env:path"
+
+RUN youtube-dl --version ; youtube-dl --help
diff --git a/docs/supportedsites.md b/docs/supportedsites.md
index ed0d5e9d9..00c75c81d 100644
--- a/docs/supportedsites.md
+++ b/docs/supportedsites.md
@@ -1,4 +1,6 @@
 # Supported sites
+ - **17live**
+ - **17live:clip**
  - **1tv**: –ü–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª
  - **20min**
  - **220.ro**
@@ -70,6 +72,9 @@
  - **ArteTVPlaylist**
  - **AsianCrush**
  - **AsianCrushPlaylist**
+ - **ask.fm**
+ - **askmona**
+ - **askmona3**
  - **AtresPlayer**
  - **ATTTechChannel**
  - **ATVAt**
@@ -111,6 +116,7 @@
  - **BiliBili**
  - **BilibiliAudio**
  - **BilibiliAudioAlbum**
+ - **BilibiliChannel**
  - **BiliBiliPlayer**
  - **BioBioChileTV**
  - **Biography**
@@ -186,6 +192,7 @@
  - **CloserToTruth**
  - **CloudflareStream**
  - **Cloudy**
+ - **clubdam:damtomo**
  - **Clubic**
  - **Clyp**
  - **cmt.com**
@@ -240,8 +247,10 @@
  - **DiscoveryPlus**
  - **DiscoveryVR**
  - **Disney**
+ - **disneychris**
  - **dlive:stream**
  - **dlive:vod**
+ - **dnatube**
  - **Dotsub**
  - **DouyuShow**
  - **DouyuTV**: ÊñóÈ±º
@@ -290,6 +299,8 @@
  - **faz.net**
  - **fc2**
  - **fc2:embed**
+ - **fc2:live** (Currently broken)
+ - **fc2:user**
  - **Fczenit**
  - **filmon**
  - **filmon:channel**
@@ -321,7 +332,6 @@
  - **FrontendMasters**
  - **FrontendMastersCourse**
  - **FrontendMastersLesson**
- - **FujiTVFODPlus7**
  - **Funimation**
  - **Funk**
  - **Fusion**
@@ -354,6 +364,7 @@
  - **hbo**
  - **HearThisAt**
  - **Heise**
+ - **HelloNewDream**
  - **HellPorno**
  - **Helsinki**: helsinki.fi
  - **HentaiStigma**
@@ -409,10 +420,12 @@
  - **ivi**: ivi.ru
  - **ivi:compilation**: ivi.ru compilations
  - **ivideon**: Ivideon TV
- - **Iwara**
+ - **iwara**
+ - **iwara:user**
  - **Izlesene**
  - **Jamendo**
  - **JamendoAlbum**
+ - **javhub**
  - **JeuxVideo**
  - **Joj**
  - **Jove**
@@ -494,7 +507,11 @@
  - **MaoriTV**
  - **Markiza**
  - **MarkizaPage**
+ - **marshmallow-qa**
  - **massengeschmack.tv**
+ - **mastodon**
+ - **mastodon:user**
+ - **mastodon:user:numeric_id**
  - **MatchTV**
  - **MDR**: MDR.DE and KiKA
  - **MedalTV**
@@ -516,12 +533,17 @@
  - **Mgoon**
  - **MGTV**: ËäíÊûúTV
  - **MiaoPai**
+ - **mildom**: Record ongoing live by specific user in Mildom
+ - **mildom:user:vod**: Download all VODs from specific user in Mildom
+ - **mildom:vod**: Download a VOD in Mildom
  - **minds**
  - **minds:channel**
  - **minds:group**
  - **MinistryGrid**
  - **Minoto**
  - **miomio.tv**
+ - **mirrativ**
+ - **mirrativ:user**
  - **MiTele**: mitele.es
  - **mixcloud**
  - **mixcloud:playlist**
@@ -538,6 +560,7 @@
  - **Motherless**
  - **MotherlessGroup**
  - **Motorsport**: motorsport.com
+ - **mottohomete**
  - **MovieClips**
  - **MovieFap**
  - **Moviezine**
@@ -613,7 +636,11 @@
  - **nickelodeonru**
  - **nicknight**
  - **niconico**: „Éã„Ç≥„Éã„Ç≥ÂãïÁîª
- - **NiconicoPlaylist**
+ - **niconico:live**: „Éã„Ç≥„Éã„Ç≥ÁîüÊîæÈÄÅ
+ - **niconico:playlist**
+ - **niconico:series**
+ - **niconico:smile** (Currently broken)
+ - **niconico:user**
  - **Nintendo**
  - **njoy**: N-JOY
  - **njoy:embed**
@@ -663,6 +690,8 @@
  - **OnionStudios**
  - **Ooyala**
  - **OoyalaExternal**
+ - **openrec**
+ - **openrec:capture**
  - **OraTV**
  - **orf:burgenland**: Radio Burgenland
  - **orf:fm4**: radio FM4
@@ -693,6 +722,7 @@
  - **pbs**: Public Broadcasting Service (PBS) and member stations: PBS: Public Broadcasting Service, APT - Alabama Public Television (WBIQ), GPB/Georgia Public Broadcasting (WGTV), Mississippi Public Broadcasting (WMPN), Nashville Public Television (WNPT), WFSU-TV (WFSU), WSRE (WSRE), WTCI (WTCI), WPBA/Channel 30 (WPBA), Alaska Public Media (KAKM), Arizona PBS (KAET), KNME-TV/Channel 5 (KNME), Vegas PBS (KLVX), AETN/ARKANSAS ETV NETWORK (KETS), KET (WKLE), WKNO/Channel 10 (WKNO), LPB/LOUISIANA PUBLIC BROADCASTING (WLPB), OETA (KETA), Ozarks Public Television (KOZK), WSIU Public Broadcasting (WSIU), KEET TV (KEET), KIXE/Channel 9 (KIXE), KPBS San Diego (KPBS), KQED (KQED), KVIE Public Television (KVIE), PBS SoCal/KOCE (KOCE), ValleyPBS (KVPT), CONNECTICUT PUBLIC TELEVISION (WEDH), KNPB Channel 5 (KNPB), SOPTV (KSYS), Rocky Mountain PBS (KRMA), KENW-TV3 (KENW), KUED Channel 7 (KUED), Wyoming PBS (KCWC), Colorado Public Television / KBDI 12 (KBDI), KBYU-TV (KBYU), Thirteen/WNET New York (WNET), WGBH/Channel 2 (WGBH), WGBY (WGBY), NJTV Public Media NJ (WNJT), WLIW21 (WLIW), mpt/Maryland Public Television (WMPB), WETA Television and Radio (WETA), WHYY (WHYY), PBS 39 (WLVT), WVPT - Your Source for PBS and More! (WVPT), Howard University Television (WHUT), WEDU PBS (WEDU), WGCU Public Media (WGCU), WPBT2 (WPBT), WUCF TV (WUCF), WUFT/Channel 5 (WUFT), WXEL/Channel 42 (WXEL), WLRN/Channel 17 (WLRN), WUSF Public Broadcasting (WUSF), ETV (WRLK), UNC-TV (WUNC), PBS Hawaii - Oceanic Cable Channel 10 (KHET), Idaho Public Television (KAID), KSPS (KSPS), OPB (KOPB), KWSU/Channel 10 & KTNW/Channel 31 (KWSU), WILL-TV (WILL), Network Knowledge - WSEC/Springfield (WSEC), WTTW11 (WTTW), Iowa Public Television/IPTV (KDIN), Nine Network (KETC), PBS39 Fort Wayne (WFWA), WFYI Indianapolis (WFYI), Milwaukee Public Television (WMVS), WNIN (WNIN), WNIT Public Television (WNIT), WPT (WPNE), WVUT/Channel 22 (WVUT), WEIU/Channel 51 (WEIU), WQPT-TV (WQPT), WYCC PBS Chicago (WYCC), WIPB-TV (WIPB), WTIU (WTIU), CET  (WCET), ThinkTVNetwork (WPTD), WBGU-TV (WBGU), WGVU TV (WGVU), NET1 (KUON), Pioneer Public Television (KWCM), SDPB Television (KUSD), TPT (KTCA), KSMQ (KSMQ), KPTS/Channel 8 (KPTS), KTWU/Channel 11 (KTWU), East Tennessee PBS (WSJK), WCTE-TV (WCTE), WLJT, Channel 11 (WLJT), WOSU TV (WOSU), WOUB/WOUC (WOUB), WVPB (WVPB), WKYU-PBS (WKYU), KERA 13 (KERA), MPBN (WCBB), Mountain Lake PBS (WCFE), NHPTV (WENH), Vermont PBS (WETK), witf (WITF), WQED Multimedia (WQED), WMHT Educational Telecommunications (WMHT), Q-TV (WDCQ), WTVS Detroit Public TV (WTVS), CMU Public Television (WCMU), WKAR-TV (WKAR), WNMU-TV Public TV 13 (WNMU), WDSE - WRPT (WDSE), WGTE TV (WGTE), Lakeland Public Television (KAWE), KMOS-TV - Channels 6.1, 6.2 and 6.3 (KMOS), MontanaPBS (KUSM), KRWG/Channel 22 (KRWG), KACV (KACV), KCOS/Channel 13 (KCOS), WCNY/Channel 24 (WCNY), WNED (WNED), WPBS (WPBS), WSKG Public TV (WSKG), WXXI (WXXI), WPSU (WPSU), WVIA Public Media Studios (WVIA), WTVI (WTVI), Western Reserve PBS (WNEO), WVIZ/PBS ideastream (WVIZ), KCTS 9 (KCTS), Basin PBS (KPBT), KUHT / Channel 8 (KUHT), KLRN (KLRN), KLRU (KLRU), WTJX Channel 12 (WTJX), WCVE PBS (WCVE), KBTC Public Television (KBTC)
  - **PearVideo**
  - **PeerTube**
+ - **peing**
  - **People**
  - **PerformGroup**
  - **periscope**: Periscope
@@ -706,6 +736,8 @@
  - **Pinkbike**
  - **Pinterest**
  - **PinterestCollection**
+ - **pixiv:sketch**
+ - **pixiv:sketch:user**
  - **Pladform**
  - **Platzi**
  - **PlatziCourse**
@@ -731,6 +763,7 @@
  - **PornHub**: PornHub and Thumbzilla
  - **PornHubPagedVideoList**
  - **PornHubUser**
+ - **PornHubUserLive**
  - **PornHubUserVideosUpload**
  - **Pornotube**
  - **PornoVoisines**
@@ -753,6 +786,7 @@
  - **QuicklineLive**
  - **R7**
  - **R7Article**
+ - **Radiko**
  - **radio.de**
  - **radiobremen**
  - **radiocanada**
@@ -840,6 +874,7 @@
  - **Shahid**
  - **ShahidShow**
  - **Shared**: shared.sx
+ - **sharevideos**
  - **ShowRoomLive**
  - **simplecast**
  - **simplecast:episode**
@@ -961,11 +996,16 @@
  - **TikTok**
  - **TikTokUser** (Currently broken)
  - **tinypic**: tinypic.com videos
+ - **tktube** (Currently broken)
  - **TMZ**
  - **TMZArticle**
  - **TNAFlix**
  - **TNAFlixNetworkEmbed**
  - **toggle**
+ - **tokyomotion**
+ - **tokyomotion:searches**
+ - **tokyomotion:user**
+ - **tokyomotion:user:favs**
  - **ToonGoggles**
  - **tou.tv**
  - **Toypics**: Toypics video
@@ -1019,6 +1059,7 @@
  - **TVPlayHome**
  - **Tweakers**
  - **TwitCasting**
+ - **TwitCastingUser**
  - **twitch:clips**
  - **twitch:stream**
  - **twitch:vod**
@@ -1116,6 +1157,8 @@
  - **VODPl**
  - **VODPlatform**
  - **VoiceRepublic**
+ - **voicy**
+ - **voicy:channel**
  - **Voot**
  - **VoxMedia**
  - **VoxMediaVolume**
@@ -1153,6 +1196,7 @@
  - **Weibo**
  - **WeiboMobile**
  - **WeiqiTV**: WQTV
+ - **whowatch**
  - **Wistia**
  - **WistiaPlaylist**
  - **wnl**: npo.nl, ntr.nl, omroepwnl.nl, zapp.nl and npo3.nl
diff --git a/icons/icon.sh b/icons/icon.sh
new file mode 100755
index 000000000..da241d077
--- /dev/null
+++ b/icons/icon.sh
@@ -0,0 +1,4 @@
+#!/bin/bash
+set -xe
+# https://unix.stackexchange.com/questions/89275/how-to-create-ico-file-with-more-than-one-image
+convert -background transparent "$1" -define icon:auto-resize=16,32,48,64,128,192,256 "${1%%.*}.ico"
diff --git a/icons/youtube_social_squircle_red.ico b/icons/youtube_social_squircle_red.ico
new file mode 100644
index 000000000..fb9dca4c2
Binary files /dev/null and b/icons/youtube_social_squircle_red.ico differ
diff --git a/icons/youtube_social_squircle_red.png b/icons/youtube_social_squircle_red.png
new file mode 100644
index 000000000..66c3c5e54
Binary files /dev/null and b/icons/youtube_social_squircle_red.png differ
diff --git a/icons/youtube_social_squircle_white.ico b/icons/youtube_social_squircle_white.ico
new file mode 100644
index 000000000..bc31a1692
Binary files /dev/null and b/icons/youtube_social_squircle_white.ico differ
diff --git a/icons/youtube_social_squircle_white.png b/icons/youtube_social_squircle_white.png
new file mode 100644
index 000000000..13209c68f
Binary files /dev/null and b/icons/youtube_social_squircle_white.png differ
diff --git a/netlify.toml b/netlify.toml
new file mode 100644
index 000000000..c6ee01db9
--- /dev/null
+++ b/netlify.toml
@@ -0,0 +1,3 @@
+[build]
+  publish = "public/"
+  command = "./devscripts/netlify.sh"
diff --git a/renovate.json b/renovate.json
new file mode 100644
index 000000000..43d6ae645
--- /dev/null
+++ b/renovate.json
@@ -0,0 +1,10 @@
+{
+  "extends": ["config:base"],
+  "automerge": true,
+  "packageRules": [
+    {
+      "updateTypes": ["minor", "patch", "pin", "digest"],
+      "automerge": true
+    }
+  ]
+}
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 000000000..5414be8e5
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,11 @@
+# idk
+pycryptodome==3.10.1
+
+# for Python 3.x
+websockets==9.0.1
+
+# for Python 2.x
+websocket_client==0.59.0
+
+# building
+zopflipy==1.5
diff --git a/setup.cfg b/setup.cfg
index da78a9c47..4a2082402 100644
--- a/setup.cfg
+++ b/setup.cfg
@@ -2,5 +2,5 @@
 universal = True
 
 [flake8]
-exclude = youtube_dl/extractor/__init__.py,devscripts/buildserver.py,devscripts/lazy_load_template.py,devscripts/make_issue_template.py,setup.py,build,.git,venv
+exclude = youtube_dl/extractor/__init__.py,devscripts/buildserver.py,devscripts/lazy_load_template.py,devscripts/make_issue_template.py,setup.py,build,.git,venv,.venv
 ignore = E402,E501,E731,E741,W503
diff --git a/setup.py b/setup.py
index af68b485e..b03746537 100644
--- a/setup.py
+++ b/setup.py
@@ -3,6 +3,7 @@
 
 from __future__ import print_function
 
+import os
 import os.path
 import warnings
 import sys
@@ -26,7 +27,7 @@ except ImportError:
 
 py2exe_options = {
     'bundle_files': 1,
-    'compressed': 1,
+    'compressed': True,
     'optimize': 2,
     'dist_dir': '.',
     'dll_excludes': ['w9xpopen.exe', 'crypt32.dll'],
@@ -57,10 +58,16 @@ py2exe_params = {
 
 if len(sys.argv) >= 2 and sys.argv[1] == 'py2exe':
     params = py2exe_params
+
+    icon_path = os.environ.get('PY2EXE_WINDOWS_ICON_PATH')
+
+    if icon_path:
+        py2exe_console[0]['icon_resources'] = [(0, icon_path)]
 else:
     files_spec = [
         ('etc/bash_completion.d', ['youtube-dl.bash-completion']),
         ('etc/fish/completions', ['youtube-dl.fish']),
+        ('share/zsh/site-functions', ['_youtube-dl']),
         ('share/doc/youtube_dl', ['README.txt']),
         ('share/man/man1', ['youtube-dl.1'])
     ]
@@ -113,7 +120,8 @@ setup(
     packages=[
         'youtube_dl',
         'youtube_dl.extractor', 'youtube_dl.downloader',
-        'youtube_dl.postprocessor'],
+        'youtube_dl.postprocessor', 'youtube_dl.websocket',
+        'youtube_dl.extractor.mastodon', 'youtube_dl.extractor.peertube'],
 
     # Provokes warning on most systems (why?!)
     # test_suite = 'nose.collector',
@@ -136,6 +144,7 @@ setup(
         'Programming Language :: Python :: 3.6',
         'Programming Language :: Python :: 3.7',
         'Programming Language :: Python :: 3.8',
+        'Programming Language :: Python :: 3.9',
         'Programming Language :: Python :: Implementation',
         'Programming Language :: Python :: Implementation :: CPython',
         'Programming Language :: Python :: Implementation :: IronPython',
diff --git a/termux/LICENSE b/termux/LICENSE
new file mode 100644
index 000000000..0ad25db4b
--- /dev/null
+++ b/termux/LICENSE
@@ -0,0 +1,661 @@
+                    GNU AFFERO GENERAL PUBLIC LICENSE
+                       Version 3, 19 November 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU Affero General Public License is a free, copyleft license for
+software and other kinds of works, specifically designed to ensure
+cooperation with the community in the case of network server software.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+our General Public Licenses are intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  Developers that use our General Public Licenses protect your rights
+with two steps: (1) assert copyright on the software, and (2) offer
+you this License which gives you legal permission to copy, distribute
+and/or modify the software.
+
+  A secondary benefit of defending all users' freedom is that
+improvements made in alternate versions of the program, if they
+receive widespread use, become available for other developers to
+incorporate.  Many developers of free software are heartened and
+encouraged by the resulting cooperation.  However, in the case of
+software used on network servers, this result may fail to come about.
+The GNU General Public License permits making a modified version and
+letting the public access it on a server without ever releasing its
+source code to the public.
+
+  The GNU Affero General Public License is designed specifically to
+ensure that, in such cases, the modified source code becomes available
+to the community.  It requires the operator of a network server to
+provide the source code of the modified version running there to the
+users of that server.  Therefore, public use of a modified version, on
+a publicly accessible server, gives the public access to the source
+code of the modified version.
+
+  An older license, called the Affero General Public License and
+published by Affero, was designed to accomplish similar goals.  This is
+a different license, not a version of the Affero GPL, but Affero has
+released a new version of the Affero GPL which permits relicensing under
+this license.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  "This License" refers to version 3 of the GNU Affero General Public License.
+
+  "Copyright" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  "The Program" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as "you".  "Licensees" and
+"recipients" may be individuals or organizations.
+
+  To "modify" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a "modified version" of the
+earlier work or a work "based on" the earlier work.
+
+  A "covered work" means either the unmodified Program or a work based
+on the Program.
+
+  To "propagate" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To "convey" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays "Appropriate Legal Notices"
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The "source code" for a work means the preferred form of the work
+for making modifications to it.  "Object code" means any non-source
+form of a work.
+
+  A "Standard Interface" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The "System Libraries" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+"Major Component", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The "Corresponding Source" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    "keep intact all notices".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+"aggregate" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A "User Product" is either (1) a "consumer product", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, "normally used" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  "Installation Information" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  "Additional permissions" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered "further
+restrictions" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An "entity transaction" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A "contributor" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's "contributor version".
+
+  A contributor's "essential patent claims" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, "control" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a "patent license" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To "grant" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  "Knowingly relying" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is "discriminatory" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Remote Network Interaction; Use with the GNU General Public License.
+
+  Notwithstanding any other provision of this License, if you modify the
+Program, your modified version must prominently offer all users
+interacting with it remotely through a computer network (if your version
+supports such interaction) an opportunity to receive the Corresponding
+Source of your version by providing access to the Corresponding Source
+from a network server at no charge, through some standard or customary
+means of facilitating copying of software.  This Corresponding Source
+shall include the Corresponding Source for any work covered by version 3
+of the GNU General Public License that is incorporated pursuant to the
+following paragraph.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the work with which it is combined will remain governed by version
+3 of the GNU General Public License.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU Affero General Public License from time to time.  Such new versions
+will be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU Affero General
+Public License "or any later version" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU Affero General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU Affero General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU Affero General Public License as published
+    by the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU Affero General Public License for more details.
+
+    You should have received a copy of the GNU Affero General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If your software can interact with users remotely through a computer
+network, you should also make sure that it provides a way for users to
+get its source.  For example, if your program is a web application, its
+interface could display a "Source" link that leads users to an archive
+of the code.  There are many ways you could offer source, and different
+solutions will be better for different programs; see section 13 for the
+specific requirements.
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a "copyright disclaimer" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU AGPL, see
+<https://www.gnu.org/licenses/>.
diff --git a/termux/README.md b/termux/README.md
new file mode 100644
index 000000000..9a104f1b2
--- /dev/null
+++ b/termux/README.md
@@ -0,0 +1,10 @@
+# termux/
+ytdl-patched for Android with [Termux](https://termux.com/)
+
+taken from https://github.com/Ashish0804/androidydl and modified to work with ytdl-patched
+
+# How to use and install
+
+```bash
+curl -s -L https://git.io/JYzIv | bash
+```
diff --git a/termux/config b/termux/config
new file mode 100644
index 000000000..3d5ace9b0
--- /dev/null
+++ b/termux/config
@@ -0,0 +1,2 @@
+
+--no-mtime -o /data/data/com.termux/files/home/storage/shared/Youtube/%(title)s.%(ext)s -f "bestvideo[height<=1280][ext=mp4]+bestaudio[ext=m4a]"
diff --git a/termux/youtubedl.sh b/termux/youtubedl.sh
new file mode 100644
index 000000000..43c8e28f9
--- /dev/null
+++ b/termux/youtubedl.sh
@@ -0,0 +1,82 @@
+#!/bin/bash
+# shellcheck disable=SC2034,SC2039
+
+# Colors
+# ----------------------------------------
+BL='\e[01;90m' > /dev/null 2>&1; # Black
+R='\e[01;91m' > /dev/null 2>&1; # Red
+G='\e[01;92m' > /dev/null 2>&1; # Green
+Y='\e[01;93m' > /dev/null 2>&1; # Yellow
+B='\e[01;94m' > /dev/null 2>&1; # Blue
+P='\e[01;95m' > /dev/null 2>&1; # Purple
+C='\e[01;96m' > /dev/null 2>&1; # Cyan
+W='\e[01;97m' > /dev/null 2>&1; # White
+LG='\e[01;37m' > /dev/null 2>&1; # Light Gray
+N='\e[0m' > /dev/null 2>&1; # Null
+L='\033[7m' > /dev/null 2>&1; #Lines
+X='\033[0m' > /dev/null 2>&1; #Closer
+# ----------------------------------------
+
+apt update -y
+clear
+
+
+# see https://ideone.com/OxtAVZ
+echo -e "$R"'        _      _ _                    _       _              _ '"$N"
+sleep 0.3
+echo -e "$R"'       | |    | | |                  | |     | |            | |'"$N"
+sleep 0.3
+echo -e "$R"'  _   _| |_ __| | |______ _ __   __ _| |_ ___| |__   ___  __| |'"$N"
+sleep 0.3
+echo -e "$R"' | | | | __/ _` | |______| '\''_ \ / _` | __/ __| '\''_ \ / _ \/ _` |'"$N"
+sleep 0.3
+echo -e "$R"' | |_| | || (_| | |      | |_) | (_| | || (__| | | |  __/ (_| |'"$N"
+sleep 0.3
+echo -e "$R"'  \__, |\__\__,_|_|      | .__/ \__,_|\__\___|_| |_|\___|\__,_|'"$N"
+sleep 0.3
+echo -e "$R"'   __/ |                 | |                                   '"$N"
+sleep 0.3
+echo -e "$R"'  |___/                  |_|                                   '"$N"
+sleep 0.3
+
+sleep 1.5
+
+echo -e "$Y""$L""ytdl-patched Installer by"  "$R""nao20010128nao""$N"
+
+echo -e "$Y""$L""Please accept permission access...""$N"
+termux-setup-storage
+
+echo -e "$Y""$L""Installing packages...""$N"
+apt install -y python ffmpeg wget
+
+echo -e "$Y""$L""Creating bin folder...""$N"
+mkdir ~/bin
+
+echo -e "$Y""$L""Installing ytdl-patched...""$N" 
+wget https://github.com/nao20010128nao/ytdl-patched/releases/download/1617160331/youtube-dl -O /data/data/com.termux/files/home/bin/ytdl-patched.dled
+echo '#!'"$(command -v python)" > /data/data/com.termux/files/home/bin/ytdl-patched
+cat /data/data/com.termux/files/home/bin/ytdl-patched.dled >> /data/data/com.termux/files/home/bin/ytdl-patched
+rm /data/data/com.termux/files/home/bin/ytdl-patched.dled
+chmod a+x /data/data/com.termux/files/home/bin/ytdl-patched
+/data/data/com.termux/files/home/bin/ytdl-patched -U
+
+echo -e "$Y""$L""Setting up configs...""$N"
+
+echo -e "$Y""$L""Creating Youtube folder...""$N"
+mkdir /data/data/com.termux/files/home/storage/shared/Youtube
+
+echo -e "$Y""$L""Creating youtube-dl config...""$N"
+mkdir -p ~/.config/youtube-dl
+
+echo -e "$Y""$L""Getting config file...""$N"
+if ! [ -e /data/data/com.termux/files/home/.config/youtube-dl/config ] ; then
+	wget https://raw.githubusercontent.com/nao20010128nao/ytdl-patched/master/termux/config -O /data/data/com.termux/files/home/.config/youtube-dl/config
+fi
+
+echo -e "$Y""$L""Creating files...""$N"
+echo '#!/bin/bash' > /data/data/com.termux/files/home/bin/termux-url-opener
+echo '/data/data/com.termux/files/home/bin/ytdl-patched "$@"' > /data/data/com.termux/files/home/bin/termux-url-opener
+
+echo -e "$G""Installation finished.""$N"
+
+kill -1 $PPID
diff --git a/test/filename_test.py b/test/filename_test.py
new file mode 100755
index 000000000..edbca3aaa
--- /dev/null
+++ b/test/filename_test.py
@@ -0,0 +1,8 @@
+#!/usr/bin/env python3
+from __future__ import unicode_literals
+
+import sys
+
+filename = sys.argv[1]
+
+sys.exit(0 if "it's a small world" in filename.lower() else 1)
diff --git a/test/test_YoutubeDL.py b/test/test_YoutubeDL.py
index a35effe0e..15afef535 100644
--- a/test/test_YoutubeDL.py
+++ b/test/test_YoutubeDL.py
@@ -7,6 +7,7 @@ from __future__ import unicode_literals
 import os
 import sys
 import unittest
+import re
 sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
 import copy
@@ -390,6 +391,15 @@ class TestFormatSelection(unittest.TestCase):
         downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]
         self.assertEqual(downloaded_ids, ['248+141'])
 
+        for f in ['248+141', '141+248']:
+            info_dict = _make_result(list(formats_order), extractor='youtube')
+            ydl = YDL({'format': f})
+            yie = YoutubeIE(ydl)
+            yie._sort_formats(info_dict['formats'])
+            ydl.process_ie_result(info_dict)
+            downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]
+            self.assertEqual(downloaded_ids, ['248+141'])
+
         for f1, f2 in zip(formats_order, formats_order[1:]):
             info_dict = _make_result([f1, f2], extractor='youtube')
             ydl = YDL({'format': 'best/bestvideo'})
@@ -998,5 +1008,85 @@ class TestYoutubeDL(unittest.TestCase):
         self.assertEqual(downloaded['extractor_key'], 'Video')
 
 
+class TestFilenameTest(unittest.TestCase):
+    def test_filenames(self):
+        class _YDL(YDL):
+            def __init__(self, *args, **kwargs):
+                super(_YDL, self).__init__(*args, **kwargs)
+
+        ydl = _YDL({
+            'test_filename': 'python3 test/filename_test.py {}',
+        })
+
+        self.assertTrue(ydl.test_filename_external("It's a small world after all"))
+        self.assertTrue(ydl.test_filename_external("It's a small world"))
+        self.assertTrue(ydl.test_filename_external("It's A small world"))
+
+        self.assertFalse(ydl.test_filename_external(''))
+        self.assertFalse(ydl.test_filename_external("Es gibt eine mond"))
+
+    def test_empty_command(self):
+        class _YDL(YDL):
+            def __init__(self, *args, **kwargs):
+                super(_YDL, self).__init__(*args, **kwargs)
+
+        ydl = _YDL({
+            'test_filename': '',
+        })
+
+        self.assertTrue(ydl.test_filename_external("It's a small world after all"))
+        self.assertTrue(ydl.test_filename_external("It's a small world"))
+        self.assertTrue(ydl.test_filename_external("It's A small world"))
+
+        self.assertTrue(ydl.test_filename_external(''))
+        self.assertTrue(ydl.test_filename_external("Es gibt eine mond"))
+
+    def test_unset(self):
+        class _YDL(YDL):
+            def __init__(self, *args, **kwargs):
+                super(_YDL, self).__init__(*args, **kwargs)
+
+        ydl = _YDL({})
+
+        self.assertTrue(ydl.test_filename_external("It's a small world after all"))
+        self.assertTrue(ydl.test_filename_external("It's a small world"))
+        self.assertTrue(ydl.test_filename_external("It's A small world"))
+
+        self.assertTrue(ydl.test_filename_external(''))
+        self.assertTrue(ydl.test_filename_external("Es gibt eine mond"))
+
+    def test_regex_str(self):
+        class _YDL(YDL):
+            def __init__(self, *args, **kwargs):
+                super(_YDL, self).__init__(*args, **kwargs)
+
+        ydl = _YDL({
+            'test_filename': r"re:(?i)It's a small world",
+        })
+
+        self.assertTrue(ydl.test_filename_external("It's a small world after all"))
+        self.assertTrue(ydl.test_filename_external("It's a small world"))
+        self.assertTrue(ydl.test_filename_external("It's A small world"))
+
+        self.assertFalse(ydl.test_filename_external(''))
+        self.assertFalse(ydl.test_filename_external("Es gibt eine mond"))
+
+    def test_compiled_regex(self):
+        class _YDL(YDL):
+            def __init__(self, *args, **kwargs):
+                super(_YDL, self).__init__(*args, **kwargs)
+
+        ydl = _YDL({
+            'test_filename': re.compile(r"(?i)It's a small world"),
+        })
+
+        self.assertTrue(ydl.test_filename_external("It's a small world after all"))
+        self.assertTrue(ydl.test_filename_external("It's a small world"))
+        self.assertTrue(ydl.test_filename_external("It's A small world"))
+
+        self.assertFalse(ydl.test_filename_external(''))
+        self.assertFalse(ydl.test_filename_external("Es gibt eine mond"))
+
+
 if __name__ == '__main__':
     unittest.main()
diff --git a/test/test_http.py b/test/test_http.py
index 3ee0a5dda..7e1769fbe 100644
--- a/test/test_http.py
+++ b/test/test_http.py
@@ -79,9 +79,10 @@ class TestHTTP(unittest.TestCase):
 
         ydl = YoutubeDL({'logger': FakeLogger()})
         r = ydl.extract_info('http://127.0.0.1:%d/302' % self.port)
-        self.assertEqual(r['entries'][0]['url'], 'http://127.0.0.1:%d/vid.mp4' % self.port)
+        self.assertEqual(r['url'], 'http://127.0.0.1:%d/vid.mp4' % self.port)
 
 
+@unittest.skipIf(os.name.startswith('java'), 'unsupported on Jython')
 class TestHTTPS(unittest.TestCase):
     def setUp(self):
         certfn = os.path.join(TEST_DIR, 'testcert.pem')
@@ -103,7 +104,7 @@ class TestHTTPS(unittest.TestCase):
 
         ydl = YoutubeDL({'logger': FakeLogger(), 'nocheckcertificate': True})
         r = ydl.extract_info('https://127.0.0.1:%d/video.html' % self.port)
-        self.assertEqual(r['entries'][0]['url'], 'https://127.0.0.1:%d/vid.mp4' % self.port)
+        self.assertEqual(r['url'], 'https://127.0.0.1:%d/vid.mp4' % self.port)
 
 
 def _build_proxy_handler(name):
diff --git a/test/test_longname.py b/test/test_longname.py
new file mode 100644
index 000000000..6c51a2679
--- /dev/null
+++ b/test/test_longname.py
@@ -0,0 +1,50 @@
+#!/usr/bin/env python
+# coding: utf-8
+from __future__ import unicode_literals
+
+# Allow direct execution
+import os
+import sys
+import unittest
+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+
+from youtube_dl.longname import (
+    split_longname_str,
+    combine_longname_str,
+)
+
+
+# TODO: add downloader testcase for "-fw_4mVz-44"
+# „Åè„Çå„Åê„Çå„ÇÇ‰ΩìË™ø„Å´Ê∞ó„Çí„Å§„Ç±„ÉÜüò≥‚úãü§ëÔºàÔø£‚ñΩÔø£ÔºâYouTube„Å°„ÇÉ„ÇìüòÜüòÉ‚òÄ ‚ùóüòöÂÖÉÊ∞ó„ÄÅ„Å™„ÅÑ„ÅÆ„Åã„Å™„Ç°üò≠üòì(T_T)üí¶Â§ß‰∏àÂ§´‚ùìÔºàÔø£„ÉºÔø£?Ôºâüòú‚ÅâÔ∏èü§î„Åè„Çå„Åê„Çå„ÇÇ‰ΩìË™ø„Å´Ê∞ó„Çí„Å§„Ç±„ÉÜüò≥‚úãü§ëÔºàÔø£‚ñΩÔø£ÔºâYouTube„Å°„ÇÉ„ÇìüòÜ
+class TestLongName(unittest.TestCase):
+
+    def test_split_utf8(self):
+        self.assertEqual(
+            split_longname_str('spade', 'utf8'),
+            'spade'
+        )
+        self.assertEqual(
+            split_longname_str('„ÄêÂ¶ñÊÄ™„Ç¶„Ç©„ÉÉ„ÉÅ„Ç¢„Éã„É°„ÄëÁ¨¨ÔºëÔºìË©±„Äå „Ç≥„Éû„Åï„Çì „Äú„ÅØ„Åò„ÇÅ„Å¶„ÅÆ„Éï„Ç°„Çπ„Éà„Éï„Éº„ÉâÁ∑®„ÄúÔºà#5Ôºâ„Äç„ÄåÂ¶ñÊÄ™ Âè£„Å†„Åë„Åä„Çì„Å™„Äç  „ÄåÂ¶ñÊÄ™ „ÉÄ„É≥„Çµ„Éº„Ç∫‚òÜ„Äç„Äå „Åò„Çì„ÇÅ„ÇìÁä¨„Ç∑„Éº„Ç∫„É≥2 Áä¨ËÑ±Ëµ∞ Episode 2„Äç-cL46Bl96_GQ.f137.mp4.part', 'utf8'),
+            '„ÄêÂ¶ñÊÄ™„Ç¶„Ç©„ÉÉ„ÉÅ„Ç¢„Éã„É°„ÄëÁ¨¨ÔºëÔºìË©±„Äå „Ç≥„Éû„Åï„Çì „Äú„ÅØ„Åò„ÇÅ„Å¶„ÅÆ„Éï„Ç°„Çπ„Éà„Éï„Éº„ÉâÁ∑®„ÄúÔºà#5Ôºâ„Äç„ÄåÂ¶ñÊÄ™ Âè£„Å†„Åë„Åä„Çì„Å™„Äç  „ÄåÂ¶ñÊÄ™ „ÉÄ„É≥„Çµ„Éº„Ç∫‚òÜ„Äç„Äå „Åò„Çì„ÇÅ„ÇìÁä¨„Ç∑„Éº„Ç∫„É≥2 Áä¨ËÑ±Ëµ∞ Episode 2„Äç-cL46Bl96_GQ.f13~~/7.mp4.part'
+        )
+
+    def test_combine_utf8(self):
+        self.assertEqual(
+            combine_longname_str('spade', 'utf8'),
+            'spade'
+        )
+        self.assertEqual(
+            combine_longname_str('„ÄêÂ¶ñÊÄ™„Ç¶„Ç©„ÉÉ„ÉÅ„Ç¢„Éã„É°„ÄëÁ¨¨ÔºëÔºìË©±„Äå „Ç≥„Éû„Åï„Çì „Äú„ÅØ„Åò„ÇÅ„Å¶„ÅÆ„Éï„Ç°„Çπ„Éà„Éï„Éº„ÉâÁ∑®„ÄúÔºà#5Ôºâ„Äç„ÄåÂ¶ñÊÄ™ Âè£„Å†„Åë„Åä„Çì„Å™„Äç  „ÄåÂ¶ñÊÄ™ „ÉÄ„É≥„Çµ„Éº„Ç∫‚òÜ„Äç„Äå „Åò„Çì„ÇÅ„ÇìÁä¨„Ç∑„Éº„Ç∫„É≥2 Áä¨ËÑ±Ëµ∞ Episode 2„Äç-cL46Bl96_GQ.f13~~/7.mp4.part', 'utf8'),
+            '„ÄêÂ¶ñÊÄ™„Ç¶„Ç©„ÉÉ„ÉÅ„Ç¢„Éã„É°„ÄëÁ¨¨ÔºëÔºìË©±„Äå „Ç≥„Éû„Åï„Çì „Äú„ÅØ„Åò„ÇÅ„Å¶„ÅÆ„Éï„Ç°„Çπ„Éà„Éï„Éº„ÉâÁ∑®„ÄúÔºà#5Ôºâ„Äç„ÄåÂ¶ñÊÄ™ Âè£„Å†„Åë„Åä„Çì„Å™„Äç  „ÄåÂ¶ñÊÄ™ „ÉÄ„É≥„Çµ„Éº„Ç∫‚òÜ„Äç„Äå „Åò„Çì„ÇÅ„ÇìÁä¨„Ç∑„Éº„Ç∫„É≥2 Áä¨ËÑ±Ëµ∞ Episode 2„Äç-cL46Bl96_GQ.f137.mp4.part'
+        )
+
+    def test_split_sjis(self):
+        # In SJIS, it is in 255 bytes (!!)
+        self.assertEqual(
+            split_longname_str('„ÄêÂ¶ñÊÄ™„Ç¶„Ç©„ÉÉ„ÉÅ„Ç¢„Éã„É°„ÄëÁ¨¨ÔºëÔºìË©±„Äå „Ç≥„Éû„Åï„Çì „Äú„ÅØ„Åò„ÇÅ„Å¶„ÅÆ„Éï„Ç°„Çπ„Éà„Éï„Éº„ÉâÁ∑®„ÄúÔºà#5Ôºâ„Äç„ÄåÂ¶ñÊÄ™ Âè£„Å†„Åë„Åä„Çì„Å™„Äç  „ÄåÂ¶ñÊÄ™ „ÉÄ„É≥„Çµ„Éº„Ç∫‚òÜ„Äç„Äå „Åò„Çì„ÇÅ„ÇìÁä¨„Ç∑„Éº„Ç∫„É≥2 Áä¨ËÑ±Ëµ∞ Episode 2„Äç-cL46Bl96_GQ.f137.mp4.part', 'sjis'),
+            '„ÄêÂ¶ñÊÄ™„Ç¶„Ç©„ÉÉ„ÉÅ„Ç¢„Éã„É°„ÄëÁ¨¨ÔºëÔºìË©±„Äå „Ç≥„Éû„Åï„Çì „Äú„ÅØ„Åò„ÇÅ„Å¶„ÅÆ„Éï„Ç°„Çπ„Éà„Éï„Éº„ÉâÁ∑®„ÄúÔºà#5Ôºâ„Äç„ÄåÂ¶ñÊÄ™ Âè£„Å†„Åë„Åä„Çì„Å™„Äç  „ÄåÂ¶ñÊÄ™ „ÉÄ„É≥„Çµ„Éº„Ç∫‚òÜ„Äç„Äå „Åò„Çì„ÇÅ„ÇìÁä¨„Ç∑„Éº„Ç∫„É≥2 Áä¨ËÑ±Ëµ∞ Episode 2„Äç-cL46Bl96_GQ.f137.mp4.part'
+        )
+
+    def test_resplit_identical(self):
+        instr = '„ÄêÂ¶ñÊÄ™„Ç¶„Ç©„ÉÉ„ÉÅ„Ç¢„Éã„É°„ÄëÁ¨¨ÔºëÔºìË©±„Äå „Ç≥„Éû„Åï„Çì „Äú„ÅØ„Åò„ÇÅ„Å¶„ÅÆ„Éï„Ç°„Çπ„Éà„Éï„Éº„ÉâÁ∑®„ÄúÔºà#5Ôºâ„Äç„ÄåÂ¶ñÊÄ™ Âè£„Å†„Åë„Åä„Çì„Å™„Äç  „ÄåÂ¶ñÊÄ™ „ÉÄ„É≥„Çµ„Éº„Ç∫‚òÜ„Äç„Äå „Åò„Çì„ÇÅ„ÇìÁä¨„Ç∑„Éº„Ç∫„É≥2 Áä¨ËÑ±Ëµ∞ Episode 2„Äç-cL46Bl96_GQ.f13~~/7.mp4.part'
+        self.assertEqual(split_longname_str(instr, 'utf8'), instr)
diff --git a/test/test_socks.py b/test/test_socks.py
index 1e68eb0da..22bd8a29a 100644
--- a/test/test_socks.py
+++ b/test/test_socks.py
@@ -113,6 +113,9 @@ class TestSocks(unittest.TestCase):
     def test_socks5(self):
         self.assertTrue(isinstance(self._get_ip('socks5'), compat_str))
 
+    def test_socks5h(self):
+        self.assertTrue(isinstance(self._get_ip('socks5h'), compat_str))
+
 
 if __name__ == '__main__':
     unittest.main()
diff --git a/test/test_websocket.py b/test/test_websocket.py
new file mode 100644
index 000000000..ed78747b9
--- /dev/null
+++ b/test/test_websocket.py
@@ -0,0 +1,76 @@
+#!/usr/bin/env python
+
+from __future__ import unicode_literals, with_statement
+
+# Allow direct execution
+import os
+import sys
+import unittest
+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+
+from youtube_dl.websocket import (
+    HAVE_WEBSOCKET,
+    HAVE_WS_WEBSOCKET_CLIENT,
+    HAVE_WS_WEBSOCKETS,
+    HAVE_WS_WEBSOCAT,
+    HAVE_WS_NODEJS_WEBSOCKET_WRAPPER,
+    HAVE_WS_NODEJS_WS_WRAPPER,
+
+    WebSocketClientWrapper,
+    WebSocketsWrapper,
+    WebsocatWrapper,
+    NodeJsWebsocketWrapper,
+    NodeJsWsWrapper,
+)
+
+import logging
+logger = logging.getLogger('websockets')
+logger.setLevel(logging.DEBUG)
+logger.addHandler(logging.StreamHandler())
+
+
+@unittest.skipUnless(HAVE_WEBSOCKET, 'websocket not available')
+class TestWebSocket(unittest.TestCase):
+
+    @staticmethod
+    def _kraken(impl):
+        with impl('wss://ws.kraken.com/') as ws:
+            ws.send('{"event":"subscribe", "subscription":{"name":"trade"}, "pair":["XBT/USD","XRP/USD"]}')
+            for _ in range(5):
+                print(ws.recv())
+
+    @staticmethod
+    def _echo(impl, url):
+        with impl(url) as ws:
+            for i in range(5):
+                text = 'Hello World %d' % i
+                ws.send(text)
+                recv = ws.recv()
+                print(recv, type(recv))
+                assert text == recv
+
+
+for testsuite in ('kraken', 'echo'):
+    for available, impl, name in (
+        (HAVE_WS_WEBSOCKET_CLIENT, WebSocketClientWrapper, 'websocket_client'),
+        (HAVE_WS_WEBSOCKETS, WebSocketsWrapper, 'websockets'),
+        (HAVE_WS_WEBSOCAT, WebsocatWrapper, 'websocat'),
+        (HAVE_WS_NODEJS_WEBSOCKET_WRAPPER, NodeJsWebsocketWrapper, 'nodejs_websocket'),
+        (HAVE_WS_NODEJS_WS_WRAPPER, NodeJsWsWrapper, 'nodejs_ws'),
+    ):
+
+        def create_function(testsuite, impl, available, name):
+            @unittest.skipUnless(available, '%s not installed' % name)
+            def _runtest(self):
+                if testsuite == 'kraken':
+                    self._kraken(impl)
+                elif testsuite == 'echo':
+                    self._echo(impl, 'ws://echo.websocket.org')
+                    self._echo(impl, 'wss://echo.websocket.org')
+
+            return _runtest
+
+        setattr(TestWebSocket, '_'.join(['test', name, testsuite]), create_function(testsuite, impl, available, name))
+
+if __name__ == '__main__':
+    unittest.main()
diff --git a/vercel.json b/vercel.json
new file mode 100644
index 000000000..8601b6b80
--- /dev/null
+++ b/vercel.json
@@ -0,0 +1,6 @@
+{
+  "github": {
+    "silent": true,
+    "enabled": false
+  }
+}
diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py
index fe30758ef..c97232fb1 100755
--- a/youtube_dl/YoutubeDL.py
+++ b/youtube_dl/YoutubeDL.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 # coding: utf-8
 
 from __future__ import absolute_import, unicode_literals
@@ -15,6 +15,7 @@ import json
 import locale
 import operator
 import os
+import os.path
 import platform
 import re
 import shutil
@@ -36,6 +37,7 @@ from .compat import (
     compat_kwargs,
     compat_numeric_types,
     compat_os_name,
+    compat_shlex_quote,
     compat_str,
     compat_tokenize_tokenize,
     compat_urllib_error,
@@ -45,14 +47,17 @@ from .compat import (
 from .utils import (
     age_restricted,
     args_to_str,
+    compiled_regex_type,
     ContentTooShortError,
     date_from_str,
     DateRange,
     DEFAULT_OUTTMPL,
     determine_ext,
     determine_protocol,
+    dig_object_type,
     DownloadError,
     encode_compat_str,
+    encodeArgument,
     encodeFilename,
     error_to_compat_str,
     expand_path,
@@ -73,11 +78,13 @@ from .utils import (
     PostProcessingError,
     preferredencoding,
     prepend_extension,
+    PrintJsonEncoder,
     register_socks_protocols,
     render_table,
     replace_extension,
     SameFileError,
     sanitize_filename,
+    sanitize_open,
     sanitize_path,
     sanitize_url,
     sanitized_Request,
@@ -107,7 +114,26 @@ from .postprocessor import (
     FFmpegPostProcessor,
     get_postprocessor,
 )
+from .longname import (
+    escaped_open,
+    escaped_path_exists,
+    escaped_path_getsize,
+    escaped_path_isfile,
+    escaped_sanitize_open,
+    escaped_stat,
+    escaped_unlink,
+    escaped_utime,
+    escaped_rename,
+    escaped_remove,
+    escaped_basename,
+    escaped_dirname,
+    ensure_directory,
+)
 from .version import __version__
+try:
+    from .build_config import git_commit, git_upstream_commit
+except ImportError:
+    git_commit, git_upstream_commit = None, None
 
 if compat_os_name == 'nt':
     import ctypes
@@ -209,6 +235,7 @@ class YoutubeDL(object):
     download_archive:  File name of a file where all downloads are recorded.
                        Videos already present in the file are not downloaded
                        again.
+    failed_archive:    File name of a file where all failed downloads are recorded.
     cookiefile:        File name where cookies should be read from and dumped to.
     nocheckcertificate:Do not verify SSL certificates
     prefer_insecure:   Use HTTP instead of HTTPS to retrieve information.
@@ -276,6 +303,16 @@ class YoutubeDL(object):
                        Must only be used along with sleep_interval.
                        Actual sleep time will be a random float from range
                        [sleep_interval; max_sleep_interval].
+    sleep_before_extract: Number of seconds to sleep before each extraction when
+                          used alone or a lower bound of a range for randomized
+                          sleep before each extraction (minimum possible number
+                          of seconds to sleep) when used along with
+                          max_sleep_before_extract.
+    max_sleep_before_extract: Upper bound of a range for randomized sleep before each
+                              extraction (maximum possible number of seconds to sleep).
+                              Must only be used along with sleep_before_extract.
+                              Actual sleep time will be a random float from range
+                              [sleep_before_extract; max_sleep_before_extract].
     listformats:       Print an overview of available video formats and exit.
     list_thumbnails:   Print a table of all thumbnails and exit.
     match_filter:      A function that gets called with the info_dict of
@@ -293,6 +330,9 @@ class YoutubeDL(object):
     geo_bypass_ip_block:
                        IP range in CIDR notation that will be used similarly to
                        geo_bypass_country
+    lock_exclusive:    When True, downloading will be locked exclusively to
+                       this process by creating .lock file.
+                       It'll be removed after download.
 
     The following options determine which downloader is picked:
     external_downloader: Executable of the external downloader to call.
@@ -300,6 +340,16 @@ class YoutubeDL(object):
     hls_prefer_native: Use the native HLS downloader instead of ffmpeg/avconv
                        if True, otherwise use ffmpeg/avconv if False, otherwise
                        use downloader suggested by extractor if None.
+    live_download_mkv: If True, live will be recorded in MKV format instead of MP4.
+    printjsontypes:    DO NOT USE THIS. If True, it shows type of each elements in info_dict.
+    check_peertube_instance: If True, generic extractor tests if it's PeerTube instance for
+                             requested domain.
+    check_mastodon_instance: Same as above, but for Mastodon.
+    extractor_retries: Number of retries for known extractor errors. Defaults to 3. Can be "infinite".
+    test_filename:     Use this to filter video file using external executable or regex
+                        (compiled regex or string starting with "re:").
+                       For external executable, return code 0 lets YTDL to start downloading.
+                       For regex, download will begin if re.search matches.
 
     The following parameters are not used by YoutubeDL itself, they are used by
     the downloader (see youtube_dl/downloader/common.py):
@@ -315,12 +365,17 @@ class YoutubeDL(object):
                        to the binary or its containing directory.
     postprocessor_args: A list of additional command-line arguments for the
                         postprocessor.
+    sponskrub:         Enables SponSkrub integration when True. Here's what you're looking for: https://github.com/yt-dlp/SponSkrub
+    sponskrub_cut:     If True, asks SponSkrub to cut out the sponsor sections.
+    sponskrub_force:   If True, runs SponSkrub even if the video was already downloaded.
+    sponskrub_path:    Location of the SponSkrub binary; either the path to the binary or its containing directory.
 
-    The following options are used by the Youtube extractor:
+    The following options are used by the extractors:
     youtube_include_dash_manifest: If True (default), DASH manifests and related
                         data will be downloaded and processed by extractor.
                         You can reduce network I/O by disabling it if you don't
                         care about DASH.
+    extractor_retries: Number of times to retry for known errors
     """
 
     _NUMERIC_FIELDS = set((
@@ -565,7 +620,8 @@ class YoutubeDL(object):
     def __exit__(self, *args):
         self.restore_console_title()
 
-        if self.params.get('cookiefile') is not None:
+        opts_cookiefile = self.params.get('cookiefile')
+        if opts_cookiefile is not None and os.path.exists(opts_cookiefile):
             self.cookiejar.save(ignore_discard=True, ignore_expires=True)
 
     def trouble(self, message=None, tb=None):
@@ -726,6 +782,9 @@ class YoutubeDL(object):
     def _match_entry(self, info_dict, incomplete):
         """ Returns None iff the file should be downloaded """
 
+        if type(info_dict) is not dict:
+            return None
+
         video_title = info_dict.get('title', info_dict.get('id', 'video'))
         if 'title' in info_dict:
             # This can happen when we're just evaluating the playlist
@@ -755,6 +814,8 @@ class YoutubeDL(object):
             return 'Skipping "%s" because it is age restricted' % video_title
         if self.in_download_archive(info_dict):
             return '%s has already been recorded in archive' % video_title
+        if self.has_locked(info_dict):
+            return 'Lock for %s is acquired by some other process' % video_title
 
         if not incomplete:
             match_filter = self.params.get('match_filter')
@@ -833,6 +894,16 @@ class YoutubeDL(object):
 
     @__handle_extraction_exceptions
     def __extract_info(self, url, ie, download, extra_info, process):
+        min_sleep_interval = self.params.get('sleep_before_extract')
+        if min_sleep_interval:
+            max_sleep_interval = self.params.get('max_sleep_before_extract') or min_sleep_interval
+            sleep_interval = random.uniform(min_sleep_interval, max_sleep_interval)
+            self.to_screen(
+                '[extraction] Sleeping %s seconds...' % (
+                    int(sleep_interval) if sleep_interval.is_integer()
+                    else '%.2f' % sleep_interval))
+            time.sleep(sleep_interval)
+
         ie_result = ie.extract(url)
         if ie_result is None:  # Finished already (backwards compatibility; listformats and friends should be moved here)
             return
@@ -866,7 +937,7 @@ class YoutubeDL(object):
         """
         result_type = ie_result.get('_type', 'video')
 
-        if result_type in ('url', 'url_transparent'):
+        if result_type in ('url', 'url_transparent', 'url_transparent_id'):
             ie_result['url'] = sanitize_url(ie_result['url'])
             extract_flat = self.params.get('extract_flat', False)
             if ((extract_flat == 'in_playlist' and 'playlist' in extra_info)
@@ -886,7 +957,7 @@ class YoutubeDL(object):
                                      download,
                                      ie_key=ie_result.get('ie_key'),
                                      extra_info=extra_info)
-        elif result_type == 'url_transparent':
+        elif result_type in ('url_transparent', 'url_transparent_id'):
             # Use the information from the embedding page
             info = self.extract_info(
                 ie_result['url'], ie_key=ie_result.get('ie_key'),
@@ -900,7 +971,11 @@ class YoutubeDL(object):
 
             force_properties = dict(
                 (k, v) for k, v in ie_result.items() if v is not None)
-            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):
+            if result_type == 'url_transparent':
+                rmprops = ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key')
+            else:
+                rmprops = ('_type', 'url', 'extractor', 'extractor_key', 'ie_key')
+            for f in rmprops:
                 if f in force_properties:
                     del force_properties[f]
             new_result = info.copy()
@@ -938,13 +1013,13 @@ class YoutubeDL(object):
         elif result_type == 'compat_list':
             self.report_warning(
                 'Extractor %s returned a compat_list result. '
-                'It needs to be updated.' % ie_result.get('extractor'))
+                'It needs to be updated.' % ie_result.get('extractor', 'generic'))
 
             def _fixup(r):
                 self.add_extra_info(
                     r,
                     {
-                        'extractor': ie_result['extractor'],
+                        'extractor': ie_result.get('extractor', 'generic'),
                         'webpage_url': ie_result['webpage_url'],
                         'webpage_url_basename': url_basename(ie_result['webpage_url']),
                         'extractor_key': ie_result['extractor_key'],
@@ -1350,39 +1425,48 @@ class YoutubeDL(object):
                             yield matches[-1]
             elif selector.type == MERGE:
                 def _merge(formats_info):
-                    format_1, format_2 = [f['format_id'] for f in formats_info]
+                    format_1, format_2 = formats_info[0], formats_info[1]
+                    format_1_id, format_2_id = format_1['format_id'], format_2['format_id']
                     # The first format must contain the video and the
                     # second the audio
-                    if formats_info[0].get('vcodec') == 'none':
+
+                    # If the user swapped the two inputs, try swapping it for
+                    # them
+                    if format_1.get('acodec') != 'none' and format_2.get('vcodec') != 'none':
+                        temp = format_1
+                        format_1 = format_2
+                        format_2 = temp
+
+                    if format_1.get('vcodec') == 'none':
                         self.report_error('The first format must '
                                           'contain the video, try using '
-                                          '"-f %s+%s"' % (format_2, format_1))
-                        return
+                                          '"-f %s+%s"' % (format_1_id, format_2_id))
+
                     # Formats must be opposite (video+audio)
-                    if formats_info[0].get('acodec') == 'none' and formats_info[1].get('acodec') == 'none':
+                    if format_1.get('acodec') == 'none' and format_2.get('acodec') == 'none':
                         self.report_error(
                             'Both formats %s and %s are video-only, you must specify "-f video+audio"'
-                            % (format_1, format_2))
+                            % (format_1_id, format_2_id))
                         return
                     output_ext = (
-                        formats_info[0]['ext']
+                        format_1['ext']
                         if self.params.get('merge_output_format') is None
                         else self.params['merge_output_format'])
                     return {
                         'requested_formats': formats_info,
-                        'format': '%s+%s' % (formats_info[0].get('format'),
-                                             formats_info[1].get('format')),
-                        'format_id': '%s+%s' % (formats_info[0].get('format_id'),
-                                                formats_info[1].get('format_id')),
-                        'width': formats_info[0].get('width'),
-                        'height': formats_info[0].get('height'),
-                        'resolution': formats_info[0].get('resolution'),
-                        'fps': formats_info[0].get('fps'),
-                        'vcodec': formats_info[0].get('vcodec'),
-                        'vbr': formats_info[0].get('vbr'),
-                        'stretched_ratio': formats_info[0].get('stretched_ratio'),
-                        'acodec': formats_info[1].get('acodec'),
-                        'abr': formats_info[1].get('abr'),
+                        'format': '%s+%s' % (format_1.get('format'),
+                                             format_2.get('format')),
+                        'format_id': '%s+%s' % (format_1.get('format_id'),
+                                                format_2.get('format_id')),
+                        'width': format_1.get('width'),
+                        'height': format_1.get('height'),
+                        'resolution': format_1.get('resolution'),
+                        'fps': format_1.get('fps'),
+                        'vcodec': format_1.get('vcodec'),
+                        'vbr': format_1.get('vbr'),
+                        'stretched_ratio': format_1.get('stretched_ratio'),
+                        'acodec': format_2.get('acodec'),
+                        'abr': format_2.get('abr'),
                         'ext': output_ext,
                     }
                 video_selector, audio_selector = map(_build_selector_function, selector.selector)
@@ -1770,8 +1854,10 @@ class YoutubeDL(object):
         if self.params.get('forceduration', False) and info_dict.get('duration') is not None:
             self.to_stdout(formatSeconds(info_dict['duration']))
         print_mandatory('format')
+        if self.params.get('printjsontypes', False):
+            self.to_stdout('\n'.join(dig_object_type(info_dict)))
         if self.params.get('forcejson', False):
-            self.to_stdout(json.dumps(info_dict))
+            self.to_stdout(json.dumps(info_dict, cls=PrintJsonEncoder))
 
     def process_info(self, info_dict):
         """Process a single resolved IE result."""
@@ -1789,6 +1875,10 @@ class YoutubeDL(object):
         if 'format' not in info_dict:
             info_dict['format'] = info_dict['ext']
 
+        if info_dict.get('ext') == 'mp4' and info_dict.get('is_live', False) and self.params.get('live_download_mkv', False):
+            info_dict['format'] = info_dict['ext'] = 'mkv'
+            info_dict['protocol'] = 'ffmpeg'
+
         reason = self._match_entry(info_dict, incomplete=False)
         if reason is not None:
             self.to_screen('[download] ' + reason)
@@ -1832,7 +1922,7 @@ class YoutubeDL(object):
             else:
                 try:
                     self.to_screen('[info] Writing video description to: ' + descfn)
-                    with io.open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:
+                    with self.open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:
                         descfile.write(info_dict['description'])
                 except (OSError, IOError):
                     self.report_error('Cannot write description file ' + descfn)
@@ -1847,7 +1937,7 @@ class YoutubeDL(object):
             else:
                 try:
                     self.to_screen('[info] Writing video annotations to: ' + annofn)
-                    with io.open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:
+                    with self.open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:
                         annofile.write(info_dict['annotations'])
                 except (KeyError, TypeError):
                     self.report_warning('There are no annotations to write.')
@@ -1870,20 +1960,29 @@ class YoutubeDL(object):
                     self.to_screen('[info] Video subtitle %s.%s is already present' % (sub_lang, sub_format))
                 else:
                     self.to_screen('[info] Writing video subtitles to: ' + sub_filename)
-                    if sub_info.get('data') is not None:
+                    if sub_info.get('protocol'):
+                        try:
+                            get_suitable_downloader(sub_info, self.params)(self, self.params).download(sub_filename, sub_info)
+                        except (ExtractorError, IOError, OSError, ValueError, compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
+                            self.report_warning('Unable to download subtitle for "%s": %s' %
+                                                (sub_lang, error_to_compat_str(err)))
+                            continue
+                    elif sub_info.get('data') is not None:
                         try:
                             # Use newline='' to prevent conversion of newline characters
                             # See https://github.com/ytdl-org/youtube-dl/issues/10268
-                            with io.open(encodeFilename(sub_filename), 'w', encoding='utf-8', newline='') as subfile:
+                            with self.open(encodeFilename(sub_filename), 'w', encoding='utf-8', newline='') as subfile:
                                 subfile.write(sub_info['data'])
                         except (OSError, IOError):
                             self.report_error('Cannot write subtitles file ' + sub_filename)
                             return
                     else:
                         try:
+                            headers = info_dict.get('http_headers', {})
                             sub_data = ie._request_webpage(
-                                sub_info['url'], info_dict['id'], note=False).read()
-                            with io.open(encodeFilename(sub_filename), 'wb') as subfile:
+                                sub_info['url'], info_dict['id'],
+                                headers=headers, note=False).read()
+                            with self.open(encodeFilename(sub_filename), 'wb') as subfile:
                                 subfile.write(sub_data)
                         except (ExtractorError, IOError, OSError, ValueError) as err:
                             self.report_warning('Unable to download subtitle for "%s": %s' %
@@ -1906,6 +2005,8 @@ class YoutubeDL(object):
 
         if not self.params.get('skip_download', False):
             try:
+                self.lock_file(info_dict)
+
                 def dl(name, info):
                     fd = get_suitable_downloader(info, self.params)(self, self.params)
                     for ph in self._progress_hooks:
@@ -1926,34 +2027,20 @@ class YoutubeDL(object):
                     else:
                         postprocessors = [merger]
 
-                    def compatible_formats(formats):
-                        video, audio = formats
-                        # Check extension
-                        video_ext, audio_ext = video.get('ext'), audio.get('ext')
-                        if video_ext and audio_ext:
-                            COMPATIBLE_EXTS = (
-                                ('mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma'),
-                                ('webm')
-                            )
-                            for exts in COMPATIBLE_EXTS:
-                                if video_ext in exts and audio_ext in exts:
-                                    return True
-                        # TODO: Check acodec/vcodec
-                        return False
-
                     filename_real_ext = os.path.splitext(filename)[1][1:]
                     filename_wo_ext = (
                         os.path.splitext(filename)[0]
                         if filename_real_ext == info_dict['ext']
                         else filename)
                     requested_formats = info_dict['requested_formats']
-                    if self.params.get('merge_output_format') is None and not compatible_formats(requested_formats):
+                    if self.params.get('merge_output_format') is None:
                         info_dict['ext'] = 'mkv'
-                        self.report_warning(
-                            'Requested formats are incompatible for merge and will be merged into mkv.')
                     # Ensure filename always has a correct extension for successful merge
                     filename = '%s.%s' % (filename_wo_ext, info_dict['ext'])
-                    if os.path.exists(encodeFilename(filename)):
+                    if not self.test_filename_external(filename):
+                        self.to_screen(
+                            '[download] %s has rejected by external test process' % filename)
+                    elif os.path.exists(encodeFilename(filename)):
                         self.to_screen(
                             '[download] %s has already been downloaded and '
                             'merged' % filename)
@@ -1973,15 +2060,28 @@ class YoutubeDL(object):
                         info_dict['__files_to_merge'] = downloaded
                 else:
                     # Just a single file
-                    success = dl(filename, info_dict)
+                    if self.test_filename_external(filename):
+                        success = dl(filename, info_dict)
+                    else:
+                        self.to_screen(
+                            '[download] %s has rejected by external test process' % filename)
+                        success = True
             except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
                 self.report_error('unable to download video data: %s' % error_to_compat_str(err))
+                self.record_failed_archive(info_dict)
                 return
             except (OSError, IOError) as err:
+                self.record_failed_archive(info_dict)
                 raise UnavailableVideoError(err)
             except (ContentTooShortError, ) as err:
                 self.report_error('content too short (expected %s bytes and served %s)' % (err.expected, err.downloaded))
+                self.record_failed_archive(info_dict)
                 return
+            except BaseException:
+                self.record_failed_archive(info_dict)
+                raise
+            finally:
+                self.unlock_file(info_dict)
 
             if success and filename != '-':
                 # Fixup content
@@ -2069,9 +2169,13 @@ class YoutubeDL(object):
                     url, force_generic_extractor=self.params.get('force_generic_extractor', False))
             except UnavailableVideoError:
                 self.report_error('unable to download video')
+                self.record_failed_archive(url)
             except MaxDownloadsReached:
                 self.to_screen('[info] Maximum number of downloaded files reached.')
                 raise
+            except BaseException:
+                self.record_failed_archive(url)
+                raise
             else:
                 if self.params.get('dump_single_json', False):
                     self.to_stdout(json.dumps(res))
@@ -2119,7 +2223,7 @@ class YoutubeDL(object):
                 for old_filename in files_to_delete:
                     self.to_screen('Deleting original file %s (pass -k to keep)' % old_filename)
                     try:
-                        os.remove(encodeFilename(old_filename))
+                        self.remove(encodeFilename(old_filename))
                     except (IOError, OSError):
                         self.report_warning('Unable to remove downloaded original file')
 
@@ -2143,6 +2247,34 @@ class YoutubeDL(object):
                 return
         return extractor.lower() + ' ' + video_id
 
+    def test_filename_external(self, filename):
+        cmd = self.params.get('test_filename')
+        if not cmd or not isinstance(cmd, (compat_str, compiled_regex_type)):
+            return True
+
+        if isinstance(cmd, compiled_regex_type) or cmd.startswith('re:'):
+            # allow Patten object or string begins with 're:' to test against regex
+            if isinstance(cmd, compat_str):
+                cmd = cmd[3:]
+            mobj = re.search(cmd, filename)
+            return bool(mobj)
+
+        if '{}' not in cmd:
+            cmd += ' {}'
+
+        cmd = cmd.replace('{}', compat_shlex_quote(filename))
+
+        if self.params.get('verbose'):
+            self.to_screen('[debug] Testing: %s' % cmd)
+        try:
+            # True when retcode==0
+            retCode = subprocess.call(encodeArgument(cmd), shell=True)
+            return retCode == 0
+        except (IOError, OSError):
+            if self.params.get('verbose'):
+                self.to_screen('[debug] Testing: %s' % cmd)
+            return True
+
     def in_download_archive(self, info_dict):
         fn = self.params.get('download_archive')
         if fn is None:
@@ -2171,6 +2303,60 @@ class YoutubeDL(object):
         with locked_file(fn, 'a', encoding='utf-8') as archive_file:
             archive_file.write(vid_id + '\n')
 
+    def record_failed_archive(self, info_dict_or_url):
+        fn = self.params.get('failed_archive')
+        if fn is None:
+            return
+        if isinstance(info_dict_or_url, compat_str):
+            vid_id = info_dict_or_url
+        else:
+            vid_id = info_dict_or_url.get('webpage_url') or self._make_archive_id(info_dict_or_url)
+        assert vid_id
+        with locked_file(fn, 'a', encoding='utf-8') as archive_file:
+            archive_file.write(vid_id + '\n')
+
+    def lock_file(self, info_dict):
+        if not self.params.get('lock_exclusive', True):
+            return
+        vid_id = self._make_archive_id(info_dict)
+        if not vid_id:
+            return
+        vid_id = re.sub(r'[/\\: ]+', '_', vid_id) + '.lock'
+        if self.params.get('verbose'):
+            self.to_screen('[debug] locking %s' % vid_id)
+        try:
+            with locked_file(vid_id, 'w', encoding='utf-8') as w:
+                w.write('%s\n' % vid_id)
+                url = info_dict.get('url')
+                if url:
+                    w.write('%s\n' % url)
+        except IOError:
+            pass
+
+    def unlock_file(self, info_dict):
+        if not self.params.get('lock_exclusive', True):
+            return
+        vid_id = self._make_archive_id(info_dict)
+        if not vid_id:
+            return
+        vid_id = re.sub(r'[/\\: ]', '_', vid_id) + '.lock'
+        if self.params.get('verbose'):
+            self.to_screen('[debug] unlocking %s' % vid_id)
+        try:
+            os.remove(vid_id)
+        except IOError:
+            pass
+
+    def has_locked(self, info_dict):
+        vid_id = self._make_archive_id(info_dict)
+        if not vid_id:
+            return False
+        vid_id = re.sub(r'[/\\: ]', '_', vid_id) + '.lock'
+        try:
+            return os.path.exists(vid_id)
+        except IOError:
+            return False
+
     @staticmethod
     def format_resolution(format, default='unknown'):
         if format.get('vcodec') == 'none':
@@ -2307,6 +2493,10 @@ class YoutubeDL(object):
         write_string(encoding_str, encoding=None)
 
         self._write_string('[debug] youtube-dl version ' + __version__ + '\n')
+        if git_commit:
+            self._write_string('[debug]        from commit ' + git_commit + '\n')
+        if git_upstream_commit:
+            self._write_string('[debug]           based on ' + git_upstream_commit + '\n')
         if _LAZY_LOADER:
             self._write_string('[debug] Lazy loading extractors enabled' + '\n')
         try:
@@ -2375,8 +2565,11 @@ class YoutubeDL(object):
         else:
             opts_cookiefile = expand_path(opts_cookiefile)
             self.cookiejar = YoutubeDLCookieJar(opts_cookiefile)
-            if os.access(opts_cookiefile, os.R_OK):
-                self.cookiejar.load(ignore_discard=True, ignore_expires=True)
+            if os.access(opts_cookiefile, os.R_OK) or os.path.exists(opts_cookiefile):
+                try:
+                    self.cookiejar.load(ignore_discard=True, ignore_expires=True)
+                except IOError:
+                    pass
 
         cookie_processor = YoutubeDLCookieProcessor(self.cookiejar)
         if opts_proxy is not None:
@@ -2460,10 +2653,86 @@ class YoutubeDL(object):
                                (info_dict['extractor'], info_dict['id'], thumb_display_id))
                 try:
                     uf = self.urlopen(t['url'])
-                    with open(encodeFilename(thumb_filename), 'wb') as thumbf:
+                    with self.open(encodeFilename(thumb_filename), 'wb') as thumbf:
                         shutil.copyfileobj(uf, thumbf)
                     self.to_screen('[%s] %s: Writing thumbnail %sto: %s' %
                                    (info_dict['extractor'], info_dict['id'], thumb_display_id, thumb_filename))
                 except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
                     self.report_warning('Unable to download thumbnail "%s": %s' %
                                         (t['url'], error_to_compat_str(err)))
+
+    def open(self, filename, open_mode, **kwargs):
+        if self.params.get('escape_long_names', False):
+            return escaped_open(filename, open_mode, **kwargs)
+        else:
+            return open(filename, open_mode, **kwargs)
+
+    def sanitize_open(self, filename, open_mode):
+        if self.params.get('escape_long_names', False):
+            return escaped_sanitize_open(filename, open_mode)
+        else:
+            return sanitize_open(filename, open_mode)
+
+    def stat(self, path, *args, **kwargs):
+        if self.params.get('escape_long_names', False):
+            return escaped_stat(path, *args, **kwargs)
+        else:
+            return os.stat(path, *args, **kwargs)
+
+    def unlink(self, path, *args, **kwargs):
+        if self.params.get('escape_long_names', False):
+            escaped_unlink(path, *args, **kwargs)
+        else:
+            os.unlink(path, *args, **kwargs)
+
+    def isfile(self, path):
+        if self.params.get('escape_long_names', False):
+            return escaped_path_isfile(path)
+        else:
+            return os.path.isfile(path)
+
+    def exists(self, path):
+        if self.params.get('escape_long_names', False):
+            return escaped_path_exists(path)
+        else:
+            return os.path.exists(path)
+
+    def getsize(self, filename):
+        if self.params.get('escape_long_names', False):
+            return escaped_path_getsize(filename)
+        else:
+            return os.path.getsize(filename)
+
+    def utime(self, path, *args, **kwargs):
+        if self.params.get('escape_long_names', False):
+            escaped_utime(path, *args, **kwargs)
+        else:
+            os.utime(path, *args, **kwargs)
+
+    def rename(self, src, dst, *args, **kwargs):
+        if self.params.get('escape_long_names', False):
+            escaped_rename(src, dst, *args, **kwargs)
+        else:
+            os.rename(src, dst, *args, **kwargs)
+
+    def remove(self, path, *args, **kwargs):
+        if self.params.get('escape_long_names', False):
+            escaped_remove(path, *args, **kwargs)
+        else:
+            os.remove(path, *args, **kwargs)
+
+    def basename(self, path, *args, **kwargs):
+        if self.params.get('escape_long_names', False):
+            return escaped_basename(path)
+        else:
+            return os.path.basename(path)
+
+    def dirname(self, path, *args, **kwargs):
+        if self.params.get('escape_long_names', False):
+            return escaped_dirname(path)
+        else:
+            return os.path.dirname(path)
+
+    def ensure_directory(self, filename):
+        if self.params.get('escape_long_names', False):
+            ensure_directory(filename)
diff --git a/youtube_dl/__init__.py b/youtube_dl/__init__.py
index e1bd67919..7a2af65e8 100644
--- a/youtube_dl/__init__.py
+++ b/youtube_dl/__init__.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 # coding: utf-8
 
 from __future__ import unicode_literals
@@ -35,6 +35,7 @@ from .utils import (
     std_headers,
     write_string,
     render_table,
+    get_filesystem_encoding,
 )
 from .update import update_self
 from .downloader import (
@@ -42,6 +43,7 @@ from .downloader import (
 )
 from .extractor import gen_extractors, list_extractors
 from .extractor.adobepass import MSO_INFO
+from .longname import DEFAULT_DELIMITER
 from .YoutubeDL import YoutubeDL
 
 
@@ -51,6 +53,8 @@ def _real_main(argv=None):
         # https://github.com/ytdl-org/youtube-dl/issues/820
         codecs.register(lambda name: codecs.lookup('utf-8') if name == 'cp65001' else None)
 
+    codecs.register(lambda name: codecs.lookup('sjis') if name == 'windows-31j' else None)
+
     workaround_optparse_bug9161()
 
     setproctitle('youtube-dl')
@@ -123,6 +127,17 @@ def _real_main(argv=None):
         table = [[mso_id, mso_info['name']] for mso_id, mso_info in MSO_INFO.items()]
         write_string('Supported TV Providers:\n' + render_table(['mso', 'mso name'], table) + '\n', out=sys.stdout)
         sys.exit(0)
+    if opts.rm_longnamedir:
+        for dirpath, _, _ in os.walk(os.getcwd(), topdown=False):
+            if isinstance(dirpath, bytes):  # Python 2
+                dirpath = dirpath.decode(get_filesystem_encoding())
+            if not dirpath.endswith(DEFAULT_DELIMITER):
+                continue
+            if os.listdir(dirpath):
+                continue
+            write_string('Removing %s\n' % dirpath, out=sys.stdout)
+            os.rmdir(dirpath)
+        sys.exit(0)
 
     # Conflicting, missing and erroneous options
     if opts.usenetrc and (opts.username is not None or opts.password is not None):
@@ -175,19 +190,29 @@ def _real_main(argv=None):
     if opts.ap_mso and opts.ap_mso not in MSO_INFO:
         parser.error('Unsupported TV Provider, use --ap-list-mso to get a list of supported TV Providers')
 
-    def parse_retries(retries):
+    def report_conflict(arg1, arg2):
+        write_string('WARNING: %s is ignored since %s was given\n' % (arg2, arg1), out=sys.stderr)
+
+    if opts.sponskrub_cut and opts.split_chapters and opts.sponskrub is not False:
+        report_conflict('--split-chapter', '--sponskrub-cut')
+        opts.sponskrub_cut = False
+
+    def parse_retries(retries, name=''):
         if retries in ('inf', 'infinite'):
             parsed_retries = float('inf')
         else:
             try:
                 parsed_retries = int(retries)
             except (TypeError, ValueError):
-                parser.error('invalid retry count specified')
+                parser.error('invalid %sretry count specified' % name)
+                parsed_retries = None
         return parsed_retries
     if opts.retries is not None:
         opts.retries = parse_retries(opts.retries)
     if opts.fragment_retries is not None:
-        opts.fragment_retries = parse_retries(opts.fragment_retries)
+        opts.fragment_retries = parse_retries(opts.fragment_retries, 'fragment ')
+    if opts.extractor_retries is not None:
+        opts.extractor_retries = parse_retries(opts.extractor_retries, 'extractor ')
     if opts.buffersize is not None:
         numeric_buffersize = FileDownloader.parse_bytes(opts.buffersize)
         if numeric_buffersize is None:
@@ -215,6 +240,9 @@ def _real_main(argv=None):
     if opts.convertsubtitles is not None:
         if opts.convertsubtitles not in ['srt', 'vtt', 'ass', 'lrc']:
             parser.error('invalid subtitle format specified')
+    if opts.convertthumbnails is not None:
+        if opts.convertthumbnails not in ('jpg', ):
+            parser.error('invalid thumbnail format specified')
 
     if opts.date is not None:
         date = DateRange.day(opts.date)
@@ -245,7 +273,8 @@ def _real_main(argv=None):
 
     any_getting = opts.geturl or opts.gettitle or opts.getid or opts.getthumbnail or opts.getdescription or opts.getfilename or opts.getformat or opts.getduration or opts.dumpjson or opts.dump_single_json
     any_printing = opts.print_json
-    download_archive_fn = expand_path(opts.download_archive) if opts.download_archive is not None else opts.download_archive
+    download_archive_fn = expand_path(opts.download_archive) if opts.download_archive is not None else None
+    failed_archive_fn = expand_path(opts.failed_archive) if opts.failed_archive is not None else None
 
     # PostProcessors
     postprocessors = []
@@ -285,6 +314,11 @@ def _real_main(argv=None):
         postprocessors.append({
             'key': 'FFmpegEmbedSubtitle',
         })
+    if opts.convertthumbnails:
+        postprocessors.append({
+            'key': 'FFmpegThumbnailsConvertor',
+            'format': opts.convertthumbnails,
+        })
     if opts.embedthumbnail:
         already_have_thumbnail = opts.writethumbnail or opts.write_all_thumbnails
         postprocessors.append({
@@ -293,6 +327,14 @@ def _real_main(argv=None):
         })
         if not already_have_thumbnail:
             opts.writethumbnail = True
+    if opts.sponskrub is not False:
+        postprocessors.append({
+            'key': 'SponSkrub',
+            'path': opts.sponskrub_path,
+            'cut': opts.sponskrub_cut,
+            'force': opts.sponskrub_force,
+            'ignoreerror': opts.sponskrub is None,
+        })
     # XAttrMetadataPP should be run after post-processors that may change file
     # contents
     if opts.xattrs:
@@ -350,6 +392,7 @@ def _real_main(argv=None):
         'nooverwrites': opts.nooverwrites,
         'retries': opts.retries,
         'fragment_retries': opts.fragment_retries,
+        'extractor_retries': opts.extractor_retries,
         'skip_unavailable_fragments': opts.skip_unavailable_fragments,
         'keep_fragments': opts.keep_fragments,
         'buffersize': opts.buffersize,
@@ -382,6 +425,7 @@ def _real_main(argv=None):
         'rejecttitle': decodeOption(opts.rejecttitle),
         'max_downloads': opts.max_downloads,
         'prefer_free_formats': opts.prefer_free_formats,
+        'prefer_smaller_formats': opts.prefer_smaller_formats,
         'verbose': opts.verbose,
         'dump_intermediate_pages': opts.dump_intermediate_pages,
         'write_pages': opts.write_pages,
@@ -396,6 +440,7 @@ def _real_main(argv=None):
         'youtube_print_sig_code': opts.youtube_print_sig_code,
         'age_limit': opts.age_limit,
         'download_archive': download_archive_fn,
+        'failed_archive': failed_archive_fn,
         'cookiefile': opts.cookiefile,
         'nocheckcertificate': opts.no_check_certificate,
         'prefer_insecure': opts.prefer_insecure,
@@ -417,6 +462,8 @@ def _real_main(argv=None):
         'call_home': opts.call_home,
         'sleep_interval': opts.sleep_interval,
         'max_sleep_interval': opts.max_sleep_interval,
+        'sleep_before_extract': opts.sleep_before_extract,
+        'max_sleep_before_extract': opts.max_sleep_before_extract,
         'external_downloader': opts.external_downloader,
         'list_thumbnails': opts.list_thumbnails,
         'playlist_items': opts.playlist_items,
@@ -434,6 +481,13 @@ def _real_main(argv=None):
         'geo_bypass': opts.geo_bypass,
         'geo_bypass_country': opts.geo_bypass_country,
         'geo_bypass_ip_block': opts.geo_bypass_ip_block,
+        'check_mastodon_instance': opts.check_mastodon_instance,
+        'check_peertube_instance': opts.check_peertube_instance,
+        'escape_long_names': opts.escape_long_names,
+        'test_filename': opts.test_filename,
+        'printjsontypes': opts.printjsontypes,
+        'live_download_mkv': opts.live_download_mkv,
+        'lock_exclusive': opts.lock_exclusive,
         # just for deprecation check
         'autonumber': opts.autonumber if opts.autonumber is True else None,
         'usetitle': opts.usetitle if opts.usetitle is True else None,
diff --git a/youtube_dl/__main__.py b/youtube_dl/__main__.py
index 138f5fbec..06800fd91 100755
--- a/youtube_dl/__main__.py
+++ b/youtube_dl/__main__.py
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 from __future__ import unicode_literals
 
 # Execute with
@@ -13,6 +13,11 @@ if __package__ is None and not hasattr(sys, 'frozen'):
     path = os.path.realpath(os.path.abspath(__file__))
     sys.path.insert(0, os.path.dirname(os.path.dirname(path)))
 
+# assign sys.argv[0] anything better if it's None or ''
+#  or it somehow break Jython
+if not sys.argv[0]:
+    sys.argv[0] = 'youtube-dl'
+
 import youtube_dl
 
 if __name__ == '__main__':
diff --git a/youtube_dl/chrome_versions.py b/youtube_dl/chrome_versions.py
new file mode 100644
index 000000000..9c9e85ec3
--- /dev/null
+++ b/youtube_dl/chrome_versions.py
@@ -0,0 +1,930 @@
+# coding: utf-8
+# AUTOMATICALLY GENERATED FILE. DO NOT EDIT.
+# Generated by ./devscripts/make_chrome_version_list.py
+# This list is created from git tags in https://chromium.googlesource.com/chromium/src
+from __future__ import unicode_literals
+
+versions = [
+    "89.0.4325.0",
+    "89.0.4325.1",
+    "89.0.4326.0",
+    "89.0.4326.1",
+    "89.0.4327.0",
+    "89.0.4327.1",
+    "89.0.4328.0",
+    "89.0.4328.1",
+    "89.0.4328.2",
+    "89.0.4328.3",
+    "89.0.4329.0",
+    "89.0.4329.1",
+    "89.0.4329.2",
+    "89.0.4329.3",
+    "89.0.4330.0",
+    "89.0.4330.1",
+    "89.0.4330.2",
+    "89.0.4330.3",
+    "89.0.4331.0",
+    "89.0.4331.1",
+    "89.0.4331.2",
+    "89.0.4331.3",
+    "89.0.4331.4",
+    "89.0.4332.0",
+    "89.0.4332.1",
+    "89.0.4332.2",
+    "89.0.4333.0",
+    "89.0.4333.1",
+    "89.0.4334.0",
+    "89.0.4334.1",
+    "89.0.4335.0",
+    "89.0.4335.1",
+    "89.0.4335.10",
+    "89.0.4335.11",
+    "89.0.4335.12",
+    "89.0.4335.13",
+    "89.0.4335.2",
+    "89.0.4335.3",
+    "89.0.4335.4",
+    "89.0.4335.5",
+    "89.0.4335.6",
+    "89.0.4335.7",
+    "89.0.4335.8",
+    "89.0.4335.9",
+    "89.0.4336.0",
+    "89.0.4336.1",
+    "89.0.4336.2",
+    "89.0.4337.0",
+    "89.0.4337.1",
+    "89.0.4338.0",
+    "89.0.4338.1",
+    "89.0.4339.0",
+    "89.0.4339.1",
+    "89.0.4340.0",
+    "89.0.4340.1",
+    "89.0.4341.0",
+    "89.0.4341.1",
+    "89.0.4342.0",
+    "89.0.4342.1",
+    "89.0.4342.2",
+    "89.0.4342.3",
+    "89.0.4343.0",
+    "89.0.4343.1",
+    "89.0.4343.2",
+    "89.0.4344.0",
+    "89.0.4345.0",
+    "89.0.4345.1",
+    "89.0.4345.2",
+    "89.0.4346.0",
+    "89.0.4346.1",
+    "89.0.4347.0",
+    "89.0.4347.1",
+    "89.0.4348.0",
+    "89.0.4348.1",
+    "89.0.4348.2",
+    "89.0.4348.3",
+    "89.0.4348.4",
+    "89.0.4349.0",
+    "89.0.4349.1",
+    "89.0.4349.2",
+    "89.0.4349.3",
+    "89.0.4350.0",
+    "89.0.4350.1",
+    "89.0.4350.2",
+    "89.0.4350.3",
+    "89.0.4350.4",
+    "89.0.4350.5",
+    "89.0.4350.6",
+    "89.0.4350.7",
+    "89.0.4351.0",
+    "89.0.4351.1",
+    "89.0.4351.2",
+    "89.0.4352.0",
+    "89.0.4352.1",
+    "89.0.4352.2",
+    "89.0.4352.3",
+    "89.0.4352.4",
+    "89.0.4353.0",
+    "89.0.4353.1",
+    "89.0.4354.0",
+    "89.0.4354.1",
+    "89.0.4355.0",
+    "89.0.4355.1",
+    "89.0.4356.0",
+    "89.0.4356.1",
+    "89.0.4356.2",
+    "89.0.4356.3",
+    "89.0.4356.4",
+    "89.0.4356.5",
+    "89.0.4356.6",
+    "89.0.4357.0",
+    "89.0.4357.1",
+    "89.0.4357.2",
+    "89.0.4357.3",
+    "89.0.4358.0",
+    "89.0.4358.1",
+    "89.0.4358.2",
+    "89.0.4358.3",
+    "89.0.4358.4",
+    "89.0.4358.5",
+    "89.0.4359.0",
+    "89.0.4359.1",
+    "89.0.4359.2",
+    "89.0.4359.3",
+    "89.0.4359.4",
+    "89.0.4363.0",
+    "89.0.4364.0",
+    "89.0.4364.1",
+    "89.0.4364.2",
+    "89.0.4365.0",
+    "89.0.4365.1",
+    "89.0.4366.0",
+    "89.0.4366.1",
+    "89.0.4367.0",
+    "89.0.4367.1",
+    "89.0.4368.0",
+    "89.0.4368.1",
+    "89.0.4369.0",
+    "89.0.4369.1",
+    "89.0.4370.0",
+    "89.0.4370.1",
+    "89.0.4371.0",
+    "89.0.4371.1",
+    "89.0.4371.2",
+    "89.0.4373.0",
+    "89.0.4373.1",
+    "89.0.4374.0",
+    "89.0.4374.1",
+    "89.0.4375.0",
+    "89.0.4375.1",
+    "89.0.4376.0",
+    "89.0.4376.1",
+    "89.0.4377.0",
+    "89.0.4377.1",
+    "89.0.4378.0",
+    "89.0.4378.1",
+    "89.0.4379.0",
+    "89.0.4379.1",
+    "89.0.4379.2",
+    "89.0.4379.3",
+    "89.0.4380.0",
+    "89.0.4380.1",
+    "89.0.4380.2",
+    "89.0.4380.3",
+    "89.0.4380.4",
+    "89.0.4380.5",
+    "89.0.4380.6",
+    "89.0.4381.0",
+    "89.0.4381.1",
+    "89.0.4381.2",
+    "89.0.4381.3",
+    "89.0.4381.4",
+    "89.0.4381.5",
+    "89.0.4381.6",
+    "89.0.4381.7",
+    "89.0.4381.8",
+    "89.0.4382.0",
+    "89.0.4382.1",
+    "89.0.4382.2",
+    "89.0.4383.0",
+    "89.0.4383.1",
+    "89.0.4383.2",
+    "89.0.4384.0",
+    "89.0.4384.1",
+    "89.0.4385.0",
+    "89.0.4385.1",
+    "89.0.4385.2",
+    "89.0.4385.3",
+    "89.0.4386.0",
+    "89.0.4386.1",
+    "89.0.4386.2",
+    "89.0.4386.3",
+    "89.0.4386.4",
+    "89.0.4387.0",
+    "89.0.4387.1",
+    "89.0.4387.2",
+    "89.0.4387.3",
+    "89.0.4388.0",
+    "89.0.4388.1",
+    "89.0.4388.2",
+    "89.0.4388.3",
+    "89.0.4388.4",
+    "89.0.4389.0",
+    "89.0.4389.1",
+    "89.0.4389.10",
+    "89.0.4389.100",
+    "89.0.4389.101",
+    "89.0.4389.102",
+    "89.0.4389.104",
+    "89.0.4389.105",
+    "89.0.4389.108",
+    "89.0.4389.109",
+    "89.0.4389.11",
+    "89.0.4389.110",
+    "89.0.4389.111",
+    "89.0.4389.112",
+    "89.0.4389.113",
+    "89.0.4389.114",
+    "89.0.4389.115",
+    "89.0.4389.116",
+    "89.0.4389.117",
+    "89.0.4389.118",
+    "89.0.4389.119",
+    "89.0.4389.12",
+    "89.0.4389.120",
+    "89.0.4389.121",
+    "89.0.4389.122",
+    "89.0.4389.123",
+    "89.0.4389.124",
+    "89.0.4389.125",
+    "89.0.4389.126",
+    "89.0.4389.127",
+    "89.0.4389.128",
+    "89.0.4389.129",
+    "89.0.4389.13",
+    "89.0.4389.130",
+    "89.0.4389.14",
+    "89.0.4389.15",
+    "89.0.4389.16",
+    "89.0.4389.18",
+    "89.0.4389.2",
+    "89.0.4389.20",
+    "89.0.4389.23",
+    "89.0.4389.24",
+    "89.0.4389.25",
+    "89.0.4389.26",
+    "89.0.4389.27",
+    "89.0.4389.28",
+    "89.0.4389.29",
+    "89.0.4389.3",
+    "89.0.4389.30",
+    "89.0.4389.31",
+    "89.0.4389.32",
+    "89.0.4389.33",
+    "89.0.4389.35",
+    "89.0.4389.39",
+    "89.0.4389.4",
+    "89.0.4389.40",
+    "89.0.4389.41",
+    "89.0.4389.42",
+    "89.0.4389.43",
+    "89.0.4389.44",
+    "89.0.4389.45",
+    "89.0.4389.46",
+    "89.0.4389.47",
+    "89.0.4389.48",
+    "89.0.4389.49",
+    "89.0.4389.5",
+    "89.0.4389.50",
+    "89.0.4389.51",
+    "89.0.4389.52",
+    "89.0.4389.53",
+    "89.0.4389.54",
+    "89.0.4389.55",
+    "89.0.4389.56",
+    "89.0.4389.57",
+    "89.0.4389.58",
+    "89.0.4389.59",
+    "89.0.4389.6",
+    "89.0.4389.60",
+    "89.0.4389.61",
+    "89.0.4389.62",
+    "89.0.4389.63",
+    "89.0.4389.64",
+    "89.0.4389.65",
+    "89.0.4389.66",
+    "89.0.4389.67",
+    "89.0.4389.68",
+    "89.0.4389.69",
+    "89.0.4389.7",
+    "89.0.4389.70",
+    "89.0.4389.71",
+    "89.0.4389.72",
+    "89.0.4389.73",
+    "89.0.4389.74",
+    "89.0.4389.75",
+    "89.0.4389.76",
+    "89.0.4389.77",
+    "89.0.4389.78",
+    "89.0.4389.79",
+    "89.0.4389.8",
+    "89.0.4389.80",
+    "89.0.4389.81",
+    "89.0.4389.82",
+    "89.0.4389.83",
+    "89.0.4389.84",
+    "89.0.4389.85",
+    "89.0.4389.86",
+    "89.0.4389.87",
+    "89.0.4389.88",
+    "89.0.4389.89",
+    "89.0.4389.9",
+    "89.0.4389.90",
+    "89.0.4389.91",
+    "89.0.4389.92",
+    "89.0.4389.93",
+    "89.0.4389.94",
+    "89.0.4389.95",
+    "89.0.4389.96",
+    "89.0.4389.97",
+    "89.0.4389.98",
+    "89.0.4389.99",
+    "90.0.4390.0",
+    "90.0.4390.1",
+    "90.0.4391.0",
+    "90.0.4391.1",
+    "90.0.4392.0",
+    "90.0.4392.1",
+    "90.0.4393.0",
+    "90.0.4393.1",
+    "90.0.4394.0",
+    "90.0.4394.1",
+    "90.0.4394.2",
+    "90.0.4394.3",
+    "90.0.4394.4",
+    "90.0.4395.0",
+    "90.0.4395.1",
+    "90.0.4395.2",
+    "90.0.4395.3",
+    "90.0.4396.0",
+    "90.0.4396.1",
+    "90.0.4396.2",
+    "90.0.4397.0",
+    "90.0.4397.1",
+    "90.0.4398.0",
+    "90.0.4398.1",
+    "90.0.4399.0",
+    "90.0.4399.1",
+    "90.0.4400.0",
+    "90.0.4400.1",
+    "90.0.4400.10",
+    "90.0.4400.2",
+    "90.0.4400.3",
+    "90.0.4400.4",
+    "90.0.4400.5",
+    "90.0.4400.7",
+    "90.0.4400.8",
+    "90.0.4402.0",
+    "90.0.4402.2",
+    "90.0.4403.0",
+    "90.0.4403.1",
+    "90.0.4403.2",
+    "90.0.4403.3",
+    "90.0.4403.4",
+    "90.0.4403.5",
+    "90.0.4403.6",
+    "90.0.4403.7",
+    "90.0.4403.8",
+    "90.0.4403.9",
+    "90.0.4404.0",
+    "90.0.4404.1",
+    "90.0.4404.2",
+    "90.0.4405.0",
+    "90.0.4405.1",
+    "90.0.4405.2",
+    "90.0.4405.3",
+    "90.0.4405.4",
+    "90.0.4405.5",
+    "90.0.4405.6",
+    "90.0.4406.0",
+    "90.0.4406.1",
+    "90.0.4406.2",
+    "90.0.4406.3",
+    "90.0.4406.4",
+    "90.0.4407.0",
+    "90.0.4407.1",
+    "90.0.4407.2",
+    "90.0.4407.3",
+    "90.0.4408.0",
+    "90.0.4408.1",
+    "90.0.4408.2",
+    "90.0.4408.3",
+    "90.0.4408.4",
+    "90.0.4408.5",
+    "90.0.4409.0",
+    "90.0.4409.1",
+    "90.0.4410.0",
+    "90.0.4410.1",
+    "90.0.4410.2",
+    "90.0.4411.0",
+    "90.0.4411.1",
+    "90.0.4412.0",
+    "90.0.4412.1",
+    "90.0.4412.2",
+    "90.0.4412.3",
+    "90.0.4412.4",
+    "90.0.4412.5",
+    "90.0.4413.0",
+    "90.0.4413.1",
+    "90.0.4413.2",
+    "90.0.4413.3",
+    "90.0.4413.4",
+    "90.0.4413.5",
+    "90.0.4414.0",
+    "90.0.4414.1",
+    "90.0.4415.0",
+    "90.0.4415.1",
+    "90.0.4415.2",
+    "90.0.4415.3",
+    "90.0.4415.4",
+    "90.0.4416.0",
+    "90.0.4416.1",
+    "90.0.4417.0",
+    "90.0.4417.1",
+    "90.0.4418.0",
+    "90.0.4418.1",
+    "90.0.4419.0",
+    "90.0.4419.1",
+    "90.0.4420.0",
+    "90.0.4420.1",
+    "90.0.4420.2",
+    "90.0.4420.3",
+    "90.0.4420.4",
+    "90.0.4421.0",
+    "90.0.4421.1",
+    "90.0.4421.2",
+    "90.0.4421.3",
+    "90.0.4421.4",
+    "90.0.4421.5",
+    "90.0.4422.0",
+    "90.0.4422.1",
+    "90.0.4422.2",
+    "90.0.4422.3",
+    "90.0.4422.4",
+    "90.0.4423.0",
+    "90.0.4423.1",
+    "90.0.4424.0",
+    "90.0.4424.1",
+    "90.0.4424.2",
+    "90.0.4424.3",
+    "90.0.4425.0",
+    "90.0.4425.1",
+    "90.0.4425.2",
+    "90.0.4426.0",
+    "90.0.4426.1",
+    "90.0.4426.2",
+    "90.0.4426.3",
+    "90.0.4427.0",
+    "90.0.4427.1",
+    "90.0.4427.2",
+    "90.0.4427.3",
+    "90.0.4427.4",
+    "90.0.4427.5",
+    "90.0.4427.6",
+    "90.0.4427.7",
+    "90.0.4429.0",
+    "90.0.4429.1",
+    "90.0.4430.0",
+    "90.0.4430.1",
+    "90.0.4430.10",
+    "90.0.4430.100",
+    "90.0.4430.101",
+    "90.0.4430.103",
+    "90.0.4430.11",
+    "90.0.4430.12",
+    "90.0.4430.13",
+    "90.0.4430.14",
+    "90.0.4430.15",
+    "90.0.4430.16",
+    "90.0.4430.17",
+    "90.0.4430.18",
+    "90.0.4430.19",
+    "90.0.4430.2",
+    "90.0.4430.20",
+    "90.0.4430.202",
+    "90.0.4430.203",
+    "90.0.4430.204",
+    "90.0.4430.205",
+    "90.0.4430.206",
+    "90.0.4430.207",
+    "90.0.4430.208",
+    "90.0.4430.209",
+    "90.0.4430.21",
+    "90.0.4430.210",
+    "90.0.4430.211",
+    "90.0.4430.212",
+    "90.0.4430.213",
+    "90.0.4430.214",
+    "90.0.4430.215",
+    "90.0.4430.216",
+    "90.0.4430.217",
+    "90.0.4430.218",
+    "90.0.4430.219",
+    "90.0.4430.22",
+    "90.0.4430.220",
+    "90.0.4430.221",
+    "90.0.4430.222",
+    "90.0.4430.223",
+    "90.0.4430.23",
+    "90.0.4430.24",
+    "90.0.4430.25",
+    "90.0.4430.26",
+    "90.0.4430.27",
+    "90.0.4430.28",
+    "90.0.4430.29",
+    "90.0.4430.3",
+    "90.0.4430.30",
+    "90.0.4430.31",
+    "90.0.4430.32",
+    "90.0.4430.33",
+    "90.0.4430.34",
+    "90.0.4430.35",
+    "90.0.4430.36",
+    "90.0.4430.37",
+    "90.0.4430.38",
+    "90.0.4430.39",
+    "90.0.4430.4",
+    "90.0.4430.40",
+    "90.0.4430.41",
+    "90.0.4430.42",
+    "90.0.4430.43",
+    "90.0.4430.44",
+    "90.0.4430.45",
+    "90.0.4430.46",
+    "90.0.4430.47",
+    "90.0.4430.48",
+    "90.0.4430.49",
+    "90.0.4430.5",
+    "90.0.4430.50",
+    "90.0.4430.51",
+    "90.0.4430.52",
+    "90.0.4430.53",
+    "90.0.4430.54",
+    "90.0.4430.55",
+    "90.0.4430.56",
+    "90.0.4430.57",
+    "90.0.4430.58",
+    "90.0.4430.59",
+    "90.0.4430.6",
+    "90.0.4430.60",
+    "90.0.4430.61",
+    "90.0.4430.62",
+    "90.0.4430.63",
+    "90.0.4430.64",
+    "90.0.4430.65",
+    "90.0.4430.66",
+    "90.0.4430.67",
+    "90.0.4430.68",
+    "90.0.4430.69",
+    "90.0.4430.7",
+    "90.0.4430.70",
+    "90.0.4430.71",
+    "90.0.4430.72",
+    "90.0.4430.73",
+    "90.0.4430.74",
+    "90.0.4430.75",
+    "90.0.4430.76",
+    "90.0.4430.77",
+    "90.0.4430.78",
+    "90.0.4430.79",
+    "90.0.4430.8",
+    "90.0.4430.80",
+    "90.0.4430.81",
+    "90.0.4430.82",
+    "90.0.4430.83",
+    "90.0.4430.84",
+    "90.0.4430.85",
+    "90.0.4430.86",
+    "90.0.4430.87",
+    "90.0.4430.88",
+    "90.0.4430.89",
+    "90.0.4430.9",
+    "90.0.4430.90",
+    "90.0.4430.91",
+    "90.0.4430.92",
+    "90.0.4430.93",
+    "90.0.4430.94",
+    "90.0.4430.95",
+    "90.0.4430.96",
+    "91.0.4431.0",
+    "91.0.4431.1",
+    "91.0.4431.2",
+    "91.0.4431.3",
+    "91.0.4431.4",
+    "91.0.4432.0",
+    "91.0.4432.1",
+    "91.0.4432.2",
+    "91.0.4433.0",
+    "91.0.4433.1",
+    "91.0.4434.0",
+    "91.0.4434.1",
+    "91.0.4435.0",
+    "91.0.4435.1",
+    "91.0.4435.2",
+    "91.0.4435.3",
+    "91.0.4435.4",
+    "91.0.4435.5",
+    "91.0.4437.0",
+    "91.0.4437.1",
+    "91.0.4437.2",
+    "91.0.4437.3",
+    "91.0.4438.0",
+    "91.0.4438.1",
+    "91.0.4438.2",
+    "91.0.4438.3",
+    "91.0.4438.4",
+    "91.0.4439.0",
+    "91.0.4439.1",
+    "91.0.4440.0",
+    "91.0.4440.1",
+    "91.0.4441.0",
+    "91.0.4441.1",
+    "91.0.4441.2",
+    "91.0.4441.3",
+    "91.0.4442.0",
+    "91.0.4442.1",
+    "91.0.4442.2",
+    "91.0.4442.3",
+    "91.0.4442.4",
+    "91.0.4442.5",
+    "91.0.4443.0",
+    "91.0.4443.1",
+    "91.0.4444.0",
+    "91.0.4444.1",
+    "91.0.4444.2",
+    "91.0.4444.3",
+    "91.0.4445.0",
+    "91.0.4445.1",
+    "91.0.4446.0",
+    "91.0.4446.1",
+    "91.0.4446.2",
+    "91.0.4446.3",
+    "91.0.4448.0",
+    "91.0.4448.1",
+    "91.0.4448.2",
+    "91.0.4449.0",
+    "91.0.4449.1",
+    "91.0.4449.2",
+    "91.0.4449.3",
+    "91.0.4449.4",
+    "91.0.4449.5",
+    "91.0.4449.6",
+    "91.0.4449.7",
+    "91.0.4449.8",
+    "91.0.4449.9",
+    "91.0.4450.0",
+    "91.0.4450.1",
+    "91.0.4450.2",
+    "91.0.4450.3",
+    "91.0.4450.4",
+    "91.0.4452.0",
+    "91.0.4453.0",
+    "91.0.4453.1",
+    "91.0.4453.2",
+    "91.0.4453.3",
+    "91.0.4453.4",
+    "91.0.4454.0",
+    "91.0.4454.1",
+    "91.0.4454.2",
+    "91.0.4455.0",
+    "91.0.4455.1",
+    "91.0.4455.2",
+    "91.0.4455.3",
+    "91.0.4456.0",
+    "91.0.4456.1",
+    "91.0.4456.2",
+    "91.0.4456.3",
+    "91.0.4457.0",
+    "91.0.4457.1",
+    "91.0.4457.2",
+    "91.0.4458.0",
+    "91.0.4458.1",
+    "91.0.4458.2",
+    "91.0.4459.0",
+    "91.0.4459.1",
+    "91.0.4459.2",
+    "91.0.4460.0",
+    "91.0.4460.1",
+    "91.0.4461.0",
+    "91.0.4461.1",
+    "91.0.4462.0",
+    "91.0.4462.1",
+    "91.0.4462.2",
+    "91.0.4462.3",
+    "91.0.4462.4",
+    "91.0.4463.0",
+    "91.0.4463.1",
+    "91.0.4463.2",
+    "91.0.4463.3",
+    "91.0.4464.0",
+    "91.0.4464.1",
+    "91.0.4464.2",
+    "91.0.4464.3",
+    "91.0.4464.4",
+    "91.0.4464.5",
+    "91.0.4465.0",
+    "91.0.4465.1",
+    "91.0.4465.2",
+    "91.0.4466.0",
+    "91.0.4466.1",
+    "91.0.4466.2",
+    "91.0.4466.3",
+    "91.0.4466.4",
+    "91.0.4466.5",
+    "91.0.4467.0",
+    "91.0.4468.0",
+    "91.0.4468.1",
+    "91.0.4468.2",
+    "91.0.4468.3",
+    "91.0.4468.4",
+    "91.0.4468.5",
+    "91.0.4469.0",
+    "91.0.4469.1",
+    "91.0.4469.2",
+    "91.0.4469.3",
+    "91.0.4469.4",
+    "91.0.4469.5",
+    "91.0.4469.6",
+    "91.0.4471.0",
+    "91.0.4471.1",
+    "91.0.4471.2",
+    "91.0.4471.3",
+    "91.0.4471.4",
+    "91.0.4472.0",
+    "91.0.4472.1",
+    "91.0.4472.10",
+    "91.0.4472.11",
+    "91.0.4472.12",
+    "91.0.4472.13",
+    "91.0.4472.14",
+    "91.0.4472.15",
+    "91.0.4472.16",
+    "91.0.4472.17",
+    "91.0.4472.18",
+    "91.0.4472.19",
+    "91.0.4472.2",
+    "91.0.4472.20",
+    "91.0.4472.21",
+    "91.0.4472.22",
+    "91.0.4472.23",
+    "91.0.4472.24",
+    "91.0.4472.25",
+    "91.0.4472.26",
+    "91.0.4472.27",
+    "91.0.4472.28",
+    "91.0.4472.29",
+    "91.0.4472.3",
+    "91.0.4472.30",
+    "91.0.4472.31",
+    "91.0.4472.32",
+    "91.0.4472.33",
+    "91.0.4472.34",
+    "91.0.4472.35",
+    "91.0.4472.37",
+    "91.0.4472.38",
+    "91.0.4472.4",
+    "91.0.4472.46",
+    "91.0.4472.47",
+    "91.0.4472.48",
+    "91.0.4472.49",
+    "91.0.4472.5",
+    "91.0.4472.50",
+    "91.0.4472.51",
+    "91.0.4472.52",
+    "91.0.4472.53",
+    "91.0.4472.54",
+    "91.0.4472.56",
+    "91.0.4472.57",
+    "91.0.4472.6",
+    "91.0.4472.60",
+    "91.0.4472.61",
+    "91.0.4472.62",
+    "91.0.4472.63",
+    "91.0.4472.64",
+    "91.0.4472.65",
+    "91.0.4472.66",
+    "91.0.4472.67",
+    "91.0.4472.68",
+    "91.0.4472.69",
+    "91.0.4472.7",
+    "91.0.4472.8",
+    "91.0.4472.9",
+    "92.0.4473.0",
+    "92.0.4473.1",
+    "92.0.4474.0",
+    "92.0.4474.1",
+    "92.0.4475.0",
+    "92.0.4475.1",
+    "92.0.4475.2",
+    "92.0.4476.0",
+    "92.0.4476.1",
+    "92.0.4477.0",
+    "92.0.4477.1",
+    "92.0.4477.2",
+    "92.0.4478.0",
+    "92.0.4478.1",
+    "92.0.4478.2",
+    "92.0.4478.3",
+    "92.0.4478.4",
+    "92.0.4479.0",
+    "92.0.4479.1",
+    "92.0.4479.2",
+    "92.0.4479.3",
+    "92.0.4479.4",
+    "92.0.4479.5",
+    "92.0.4480.0",
+    "92.0.4480.1",
+    "92.0.4480.2",
+    "92.0.4481.0",
+    "92.0.4481.1",
+    "92.0.4481.2",
+    "92.0.4482.0",
+    "92.0.4482.1",
+    "92.0.4482.2",
+    "92.0.4482.3",
+    "92.0.4483.0",
+    "92.0.4483.1",
+    "92.0.4484.0",
+    "92.0.4484.1",
+    "92.0.4484.2",
+    "92.0.4484.3",
+    "92.0.4484.4",
+    "92.0.4484.5",
+    "92.0.4484.6",
+    "92.0.4484.7",
+    "92.0.4484.8",
+    "92.0.4485.0",
+    "92.0.4485.1",
+    "92.0.4486.0",
+    "92.0.4486.1",
+    "92.0.4486.2",
+    "92.0.4487.0",
+    "92.0.4487.1",
+    "92.0.4488.0",
+    "92.0.4488.1",
+    "92.0.4489.0",
+    "92.0.4489.1",
+    "92.0.4489.2",
+    "92.0.4489.3",
+    "92.0.4489.4",
+    "92.0.4490.0",
+    "92.0.4490.1",
+    "92.0.4490.2",
+    "92.0.4490.3",
+    "92.0.4491.0",
+    "92.0.4491.1",
+    "92.0.4491.2",
+    "92.0.4491.3",
+    "92.0.4491.4",
+    "92.0.4491.5",
+    "92.0.4491.6",
+    "92.0.4491.7",
+    "92.0.4492.0",
+    "92.0.4492.1",
+    "92.0.4492.2",
+    "92.0.4493.0",
+    "92.0.4493.1",
+    "92.0.4493.2",
+    "92.0.4493.3",
+    "92.0.4494.0",
+    "92.0.4494.1",
+    "92.0.4495.0",
+    "92.0.4495.1",
+    "92.0.4496.0",
+    "92.0.4496.1",
+    "92.0.4496.2",
+    "92.0.4496.3",
+    "92.0.4497.0",
+    "92.0.4497.1",
+    "92.0.4497.2",
+    "92.0.4498.0",
+    "92.0.4498.1",
+    "92.0.4498.2",
+    "92.0.4498.3",
+    "92.0.4499.0",
+    "92.0.4499.1",
+    "92.0.4500.0",
+    "92.0.4500.1",
+    "92.0.4500.2",
+    "92.0.4501.0",
+    "92.0.4501.1",
+    "92.0.4502.0",
+    "92.0.4502.1",
+    "92.0.4503.0",
+    "92.0.4503.1",
+    "92.0.4503.3",
+    "92.0.4503.4",
+    "92.0.4503.5",
+    "92.0.4504.0",
+    "92.0.4504.1",
+    "92.0.4504.2",
+    "92.0.4505.0",
+    "92.0.4505.1",
+    "92.0.4506.0",
+    "92.0.4506.1",
+    "92.0.4506.2",
+    "92.0.4506.3",
+    "92.0.4507.0",
+    "92.0.4508.0",
+    "92.0.4508.1",
+    "92.0.4509.0",
+    "92.0.4509.1",
+    "92.0.4510.0",
+    "92.0.4510.1",
+    "92.0.4510.2",
+    "92.0.4511.0",
+    "92.0.4511.1",
+    "92.0.4511.2",
+    "92.0.4511.3",
+    "92.0.4511.4",
+]
+
+__all__ = ['versions']
diff --git a/youtube_dl/compat.py b/youtube_dl/compat.py
index 9e45c454b..5356c02c3 100644
--- a/youtube_dl/compat.py
+++ b/youtube_dl/compat.py
@@ -2997,7 +2997,6 @@ else:
     def compat_ctypes_WINFUNCTYPE(*args, **kwargs):
         return ctypes.WINFUNCTYPE(*args, **kwargs)
 
-
 __all__ = [
     'compat_HTMLParseError',
     'compat_HTMLParser',
diff --git a/youtube_dl/downloader/__init__.py b/youtube_dl/downloader/__init__.py
index 2e485df9d..2ca389d7a 100644
--- a/youtube_dl/downloader/__init__.py
+++ b/youtube_dl/downloader/__init__.py
@@ -8,6 +8,8 @@ from .rtmp import RtmpFD
 from .dash import DashSegmentsFD
 from .rtsp import RtspFD
 from .ism import IsmFD
+from .youtube_live_chat import YoutubeLiveChatReplayFD
+from .niconico import NiconicoLiveFD, NiconicoDmcFD
 from .external import (
     get_external_downloader,
     FFmpegFD,
@@ -26,6 +28,10 @@ PROTOCOL_MAP = {
     'f4m': F4mFD,
     'http_dash_segments': DashSegmentsFD,
     'ism': IsmFD,
+    'youtube_live_chat_replay': YoutubeLiveChatReplayFD,
+    'ffmpeg': FFmpegFD,
+    'niconico_dmc': NiconicoDmcFD,
+    'niconico_live': NiconicoLiveFD,
 }
 
 
@@ -37,7 +43,7 @@ def get_suitable_downloader(info_dict, params={}):
     # if (info_dict.get('start_time') or info_dict.get('end_time')) and not info_dict.get('requested_formats') and FFmpegFD.can_download(info_dict):
     #     return FFmpegFD
 
-    external_downloader = params.get('external_downloader')
+    external_downloader = params.get('external_downloader') or info_dict.get('external_downloader')
     if external_downloader is not None:
         ed = get_external_downloader(external_downloader)
         if ed.can_download(info_dict):
diff --git a/youtube_dl/downloader/common.py b/youtube_dl/downloader/common.py
index 1cdba89cd..2241d00fc 100644
--- a/youtube_dl/downloader/common.py
+++ b/youtube_dl/downloader/common.py
@@ -1,12 +1,15 @@
 from __future__ import division, unicode_literals
 
-import os
 import re
 import sys
 import time
 import random
+import threading
 
-from ..compat import compat_os_name
+from ..compat import (
+    compat_os_name,
+    compat_urllib_request,
+)
 from ..utils import (
     decodeArgument,
     encodeFilename,
@@ -183,7 +186,7 @@ class FileDownloader(object):
     def temp_name(self, filename):
         """Returns a temporary filename for the given filename."""
         if self.params.get('nopart', False) or filename == '-' or \
-                (os.path.exists(encodeFilename(filename)) and not os.path.isfile(encodeFilename(filename))):
+                (self.ydl.exists(encodeFilename(filename)) and not self.ydl.isfile(encodeFilename(filename))):
             return filename
         return filename + '.part'
 
@@ -199,7 +202,7 @@ class FileDownloader(object):
         try:
             if old_filename == new_filename:
                 return
-            os.rename(encodeFilename(old_filename), encodeFilename(new_filename))
+            self.ydl.rename(encodeFilename(old_filename), encodeFilename(new_filename))
         except (IOError, OSError) as err:
             self.report_error('unable to rename file: %s' % error_to_compat_str(err))
 
@@ -207,7 +210,7 @@ class FileDownloader(object):
         """Try to set the last-modified time of the given file."""
         if last_modified_hdr is None:
             return
-        if not os.path.isfile(encodeFilename(filename)):
+        if not self.ydl.isfile(encodeFilename(filename)):
             return
         timestr = last_modified_hdr
         if timestr is None:
@@ -219,7 +222,7 @@ class FileDownloader(object):
         if filetime == 0:
             return
         try:
-            os.utime(filename, (time.time(), filetime))
+            self.ydl.utime(filename, (time.time(), filetime))
         except Exception:
             pass
         return filetime
@@ -257,6 +260,8 @@ class FileDownloader(object):
                 if s.get('elapsed') is not None:
                     s['_elapsed_str'] = self.format_seconds(s['elapsed'])
                     msg_template += ' in %(_elapsed_str)s'
+                if s.get('fragment_count') is not None:
+                    msg_template += ' (%(fragment_count)s fragments)'
                 self._report_progress_status(
                     msg_template % s, is_last_line=True)
 
@@ -302,6 +307,8 @@ class FileDownloader(object):
                     msg_template = '%(_downloaded_bytes_str)s at %(_speed_str)s'
             else:
                 msg_template = '%(_percent_str)s % at %(_speed_str)s ETA %(_eta_str)s'
+        if s.get('fragment_count') is not None and s.get('fragment_index') is not None:
+            msg_template += ' (%(fragment_index)d fragments of %(fragment_count)d)'
 
         self._report_progress_status(msg_template % s)
 
@@ -333,13 +340,13 @@ class FileDownloader(object):
 
         nooverwrites_and_exists = (
             self.params.get('nooverwrites', False)
-            and os.path.exists(encodeFilename(filename))
+            and self.ydl.exists(encodeFilename(filename))
         )
 
         if not hasattr(filename, 'write'):
             continuedl_and_exists = (
                 self.params.get('continuedl', True)
-                and os.path.isfile(encodeFilename(filename))
+                and self.ydl.isfile(encodeFilename(filename))
                 and not self.params.get('nopart', False)
             )
 
@@ -349,7 +356,7 @@ class FileDownloader(object):
                 self._hook_progress({
                     'filename': filename,
                     'status': 'finished',
-                    'total_bytes': os.path.getsize(encodeFilename(filename)),
+                    'total_bytes': self.ydl.getsize(encodeFilename(filename)),
                 })
                 return True
 
@@ -363,7 +370,37 @@ class FileDownloader(object):
                     else '%.2f' % sleep_interval))
             time.sleep(sleep_interval)
 
-        return self.real_download(filename, info_dict)
+        timer = [None]
+        heartbeat_lock = None
+        download_complete = False
+        if 'heartbeat_url' in info_dict:
+            heartbeat_lock = threading.Lock()
+
+            heartbeat_url = info_dict['heartbeat_url']
+            heartbeat_data = info_dict['heartbeat_data']
+            heartbeat_interval = info_dict.get('heartbeat_interval', 30)
+            self.to_screen('[download] Heartbeat with %s second interval...' % heartbeat_interval)
+
+            def heartbeat():
+                try:
+                    compat_urllib_request.urlopen(url=heartbeat_url, data=heartbeat_data)
+                except Exception:
+                    self.to_screen("[download] Heartbeat failed")
+
+                with heartbeat_lock:
+                    if not download_complete:
+                        timer[0] = threading.Timer(heartbeat_interval, heartbeat)
+                        timer[0].start()
+
+            heartbeat()
+
+        try:
+            return self.real_download(filename, info_dict)
+        finally:
+            if heartbeat_lock:
+                with heartbeat_lock:
+                    timer[0].cancel()
+                    download_complete = True
 
     def real_download(self, filename, info_dict):
         """Real download process. Redefine in subclasses."""
@@ -385,7 +422,7 @@ class FileDownloader(object):
         str_args = [decodeArgument(a) for a in args]
 
         if exe is None:
-            exe = os.path.basename(str_args[0])
+            exe = self.ydl.basename(str_args[0])
 
         self.to_screen('[debug] %s command line: %s' % (
             exe, shell_quote(str_args)))
diff --git a/youtube_dl/downloader/external.py b/youtube_dl/downloader/external.py
index c31f8910a..d1ad64a68 100644
--- a/youtube_dl/downloader/external.py
+++ b/youtube_dl/downloader/external.py
@@ -23,6 +23,7 @@ from ..utils import (
     check_executable,
     is_outdated_version,
 )
+from ..longname import split_longname
 
 
 class ExternalFD(FileDownloader):
@@ -49,7 +50,7 @@ class ExternalFD(FileDownloader):
                 'elapsed': time.time() - started,
             }
             if filename != '-':
-                fsize = os.path.getsize(encodeFilename(tmpfilename))
+                fsize = self.ydl.getsize(encodeFilename(tmpfilename))
                 self.to_screen('\r[%s] Downloaded %s bytes' % (self.get_basename(), fsize))
                 self.try_rename(tmpfilename, filename)
                 status.update({
@@ -185,10 +186,10 @@ class Aria2cFD(ExternalFD):
         cmd = [self.exe, '-c']
         cmd += self._configuration_args([
             '--min-split-size', '1M', '--max-connection-per-server', '4'])
-        dn = os.path.dirname(tmpfilename)
+        dn = self.ydl.dirname(tmpfilename)
         if dn:
             cmd += ['--dir', dn]
-        cmd += ['--out', os.path.basename(tmpfilename)]
+        cmd += ['--out', self.ydl.basename(tmpfilename)]
         for key, val in info_dict['http_headers'].items():
             cmd += ['--header', '%s: %s' % (key, val)]
         cmd += self._option('--interface', 'source_address')
@@ -228,12 +229,17 @@ class FFmpegFD(ExternalFD):
             return False
         ffpp.check_version()
 
+        if self.ydl.params.get('escape_long_names', False):
+            tmpfilename = split_longname(tmpfilename)
+
         args = [ffpp.executable, '-y']
 
         for log_level in ('quiet', 'verbose'):
             if self.params.get(log_level, False):
                 args += ['-loglevel', log_level]
                 break
+        else:
+            args += ['-hide_banner']
 
         seekable = info_dict.get('_seekable')
         if seekable is not None:
@@ -272,10 +278,7 @@ class FFmpegFD(ExternalFD):
                     '%s does not support SOCKS proxies. Downloading is likely to fail. '
                     'Consider adding --hls-prefer-native to your command.' % self.get_basename())
 
-            # Since December 2015 ffmpeg supports -http_proxy option (see
-            # http://git.videolan.org/?p=ffmpeg.git;a=commit;h=b4eb1f29ebddd60c41a2eb39f5af701e38e0d3fd)
-            # We could switch to the following code if we are able to detect version properly
-            # args += ['-http_proxy', proxy]
+            args += ['-http_proxy', proxy]
             env = os.environ.copy()
             compat_setenv('HTTP_PROXY', proxy, env=env)
             compat_setenv('http_proxy', proxy, env=env)
@@ -311,22 +314,40 @@ class FFmpegFD(ExternalFD):
             elif isinstance(conn, compat_str):
                 args += ['-rtmp_conn', conn]
 
-        args += ['-i', url, '-c', 'copy']
+        input_params = info_dict.get('input_params')
+        if isinstance(input_params, (list, tuple)):
+            args += list(input_params)
+
+        args += ['-i', url]
+        extra_url = info_dict.get('extra_url')
+        if isinstance(extra_url, (list, tuple)):
+            for u in extra_url:
+                args += ['-i', u]
+        elif isinstance(extra_url, compat_str):
+            args += ['-i', extra_url]
+        args += ['-c', 'copy']
+
+        output_params = info_dict.get('output_params')
+        if isinstance(output_params, (list, tuple)):
+            args += list(output_params)
 
         if self.params.get('test', False):
             args += ['-fs', compat_str(self._TEST_FILE_SIZE)]
 
+        file_ext = info_dict['ext']
         if protocol in ('m3u8', 'm3u8_native'):
             if self.params.get('hls_use_mpegts', False) or tmpfilename == '-':
                 args += ['-f', 'mpegts']
-            else:
+            elif file_ext == 'mp4':
                 args += ['-f', 'mp4']
                 if (ffpp.basename == 'ffmpeg' and is_outdated_version(ffpp._versions['ffmpeg'], '3.2', False)) and (not info_dict.get('acodec') or info_dict['acodec'].split('.')[0] in ('aac', 'mp4a')):
                     args += ['-bsf:a', 'aac_adtstoasc']
+            else:
+                args += ['-f', EXT_TO_OUT_FORMATS.get(file_ext, file_ext)]
         elif protocol == 'rtmp':
             args += ['-f', 'flv']
         else:
-            args += ['-f', EXT_TO_OUT_FORMATS.get(info_dict['ext'], info_dict['ext'])]
+            args += ['-f', EXT_TO_OUT_FORMATS.get(file_ext, file_ext)]
 
         args = [encodeArgument(opt) for opt in args]
         args.append(encodeFilename(ffpp._ffmpeg_filename_argument(tmpfilename), True))
diff --git a/youtube_dl/downloader/fragment.py b/youtube_dl/downloader/fragment.py
index 35c76feba..96f14b96d 100644
--- a/youtube_dl/downloader/fragment.py
+++ b/youtube_dl/downloader/fragment.py
@@ -1,6 +1,5 @@
 from __future__ import division, unicode_literals
 
-import os
 import time
 import json
 
@@ -9,7 +8,6 @@ from .http import HttpFD
 from ..utils import (
     error_to_compat_str,
     encodeFilename,
-    sanitize_open,
     sanitized_Request,
 )
 
@@ -75,7 +73,7 @@ class FragmentFD(FileDownloader):
 
     def _read_ytdl_file(self, ctx):
         assert 'ytdl_corrupt' not in ctx
-        stream, _ = sanitize_open(self.ytdl_filename(ctx['filename']), 'r')
+        stream, _ = self.ydl.sanitize_open(self.ytdl_filename(ctx['filename']), 'r')
         try:
             ctx['fragment_index'] = json.loads(stream.read())['downloader']['current_fragment']['index']
         except Exception:
@@ -84,7 +82,7 @@ class FragmentFD(FileDownloader):
             stream.close()
 
     def _write_ytdl_file(self, ctx):
-        frag_index_stream, _ = sanitize_open(self.ytdl_filename(ctx['filename']), 'w')
+        frag_index_stream, _ = self.ydl.sanitize_open(self.ytdl_filename(ctx['filename']), 'w')
         downloader = {
             'current_fragment': {
                 'index': ctx['fragment_index'],
@@ -95,18 +93,19 @@ class FragmentFD(FileDownloader):
         frag_index_stream.write(json.dumps({'downloader': downloader}))
         frag_index_stream.close()
 
-    def _download_fragment(self, ctx, frag_url, info_dict, headers=None):
+    def _download_fragment(self, ctx, frag_url, info_dict, headers=None, request_data=None):
         fragment_filename = '%s-Frag%d' % (ctx['tmpfilename'], ctx['fragment_index'])
         fragment_info_dict = {
             'url': frag_url,
             'http_headers': headers or info_dict.get('http_headers'),
+            'request_data': request_data,
         }
         success = ctx['dl'].download(fragment_filename, fragment_info_dict)
         if not success:
             return False, None
         if fragment_info_dict.get('filetime'):
             ctx['fragment_filetime'] = fragment_info_dict.get('filetime')
-        down, frag_sanitized = sanitize_open(fragment_filename, 'rb')
+        down, frag_sanitized = self.ydl.sanitize_open(fragment_filename, 'rb')
         ctx['fragment_filename_sanitized'] = frag_sanitized
         frag_content = down.read()
         down.close()
@@ -120,7 +119,7 @@ class FragmentFD(FileDownloader):
             if self.__do_ytdl_file(ctx):
                 self._write_ytdl_file(ctx)
             if not self.params.get('keep_fragments', False):
-                os.remove(encodeFilename(ctx['fragment_filename_sanitized']))
+                self.ydl.remove(encodeFilename(ctx['fragment_filename_sanitized']))
             del ctx['fragment_filename_sanitized']
 
     def _prepare_frag_download(self, ctx):
@@ -153,9 +152,9 @@ class FragmentFD(FileDownloader):
         resume_len = 0
 
         # Establish possible resume length
-        if os.path.isfile(encodeFilename(tmpfilename)):
+        if self.ydl.isfile(encodeFilename(tmpfilename)):
             open_mode = 'ab'
-            resume_len = os.path.getsize(encodeFilename(tmpfilename))
+            resume_len = self.ydl.getsize(encodeFilename(tmpfilename))
 
         # Should be initialized before ytdl file check
         ctx.update({
@@ -164,7 +163,7 @@ class FragmentFD(FileDownloader):
         })
 
         if self.__do_ytdl_file(ctx):
-            if os.path.isfile(encodeFilename(self.ytdl_filename(ctx['filename']))):
+            if self.ydl.isfile(encodeFilename(self.ytdl_filename(ctx['filename']))):
                 self._read_ytdl_file(ctx)
                 is_corrupt = ctx.get('ytdl_corrupt') is True
                 is_inconsistent = ctx['fragment_index'] > 0 and resume_len == 0
@@ -182,7 +181,7 @@ class FragmentFD(FileDownloader):
                 self._write_ytdl_file(ctx)
                 assert ctx['fragment_index'] == 0
 
-        dest_stream, tmpfilename = sanitize_open(tmpfilename, open_mode)
+        dest_stream, tmpfilename = self.ydl.sanitize_open(tmpfilename, open_mode)
 
         ctx.update({
             'dl': dl,
@@ -253,8 +252,8 @@ class FragmentFD(FileDownloader):
         ctx['dest_stream'].close()
         if self.__do_ytdl_file(ctx):
             ytdl_filename = encodeFilename(self.ytdl_filename(ctx['filename']))
-            if os.path.isfile(ytdl_filename):
-                os.remove(ytdl_filename)
+            if self.ydl.isfile(ytdl_filename):
+                self.ydl.remove(ytdl_filename)
         elapsed = time.time() - ctx['started']
 
         if ctx['tmpfilename'] == '-':
@@ -265,10 +264,10 @@ class FragmentFD(FileDownloader):
                 filetime = ctx.get('fragment_filetime')
                 if filetime:
                     try:
-                        os.utime(ctx['filename'], (time.time(), filetime))
+                        self.ydl.utime(ctx['filename'], (time.time(), filetime))
                     except Exception:
                         pass
-            downloaded_bytes = os.path.getsize(encodeFilename(ctx['filename']))
+            downloaded_bytes = self.ydl.getsize(encodeFilename(ctx['filename']))
 
         self._hook_progress({
             'downloaded_bytes': downloaded_bytes,
@@ -276,4 +275,5 @@ class FragmentFD(FileDownloader):
             'filename': ctx['filename'],
             'status': 'finished',
             'elapsed': elapsed,
+            'fragment_count': ctx['total_frags'],
         })
diff --git a/youtube_dl/downloader/http.py b/youtube_dl/downloader/http.py
index d8ac41dcc..7e9973ca2 100644
--- a/youtube_dl/downloader/http.py
+++ b/youtube_dl/downloader/http.py
@@ -1,7 +1,6 @@
 from __future__ import unicode_literals
 
 import errno
-import os
 import socket
 import time
 import random
@@ -16,7 +15,6 @@ from ..utils import (
     ContentTooShortError,
     encodeFilename,
     int_or_none,
-    sanitize_open,
     sanitized_Request,
     write_xattr,
     XAttrMetadataError,
@@ -27,6 +25,7 @@ from ..utils import (
 class HttpFD(FileDownloader):
     def real_download(self, filename, info_dict):
         url = info_dict['url']
+        request_data = info_dict.get('request_data', None)
 
         class DownloadContext(dict):
             __getattr__ = dict.get
@@ -58,8 +57,8 @@ class HttpFD(FileDownloader):
 
         if self.params.get('continuedl', True):
             # Establish possible resume length
-            if os.path.isfile(encodeFilename(ctx.tmpfilename)):
-                ctx.resume_len = os.path.getsize(
+            if self.ydl.isfile(encodeFilename(ctx.tmpfilename)):
+                ctx.resume_len = self.ydl.getsize(
                     encodeFilename(ctx.tmpfilename))
 
         ctx.is_resume = ctx.resume_len > 0
@@ -101,7 +100,7 @@ class HttpFD(FileDownloader):
                 range_end = ctx.data_len - 1
             has_range = range_start is not None
             ctx.has_range = has_range
-            request = sanitized_Request(url, None, headers)
+            request = sanitized_Request(url, request_data, headers)
             if has_range:
                 set_range(request, range_start, range_end)
             # Establish connection
@@ -152,7 +151,7 @@ class HttpFD(FileDownloader):
                     try:
                         # Open the connection again without the range header
                         ctx.data = self.ydl.urlopen(
-                            sanitized_Request(url, None, headers))
+                            sanitized_Request(url, request_data, headers))
                         content_length = ctx.data.info()['Content-Length']
                     except (compat_urllib_error.HTTPError, ) as err:
                         if err.code < 500 or err.code >= 600:
@@ -229,7 +228,7 @@ class HttpFD(FileDownloader):
                     if not to_stdout:
                         ctx.stream.close()
                     ctx.stream = None
-                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))
+                ctx.resume_len = byte_counter if to_stdout else self.ydl.getsize(encodeFilename(ctx.tmpfilename))
                 raise RetryDownload(e)
 
             while True:
@@ -256,7 +255,7 @@ class HttpFD(FileDownloader):
                 # Open destination file just in time
                 if ctx.stream is None:
                     try:
-                        ctx.stream, ctx.tmpfilename = sanitize_open(
+                        ctx.stream, ctx.tmpfilename = self.ydl.sanitize_open(
                             ctx.tmpfilename, ctx.open_mode)
                         assert ctx.stream is not None
                         ctx.filename = self.undo_temp_name(ctx.tmpfilename)
diff --git a/youtube_dl/downloader/niconico.py b/youtube_dl/downloader/niconico.py
new file mode 100644
index 000000000..e398a9f06
--- /dev/null
+++ b/youtube_dl/downloader/niconico.py
@@ -0,0 +1,175 @@
+from __future__ import division, unicode_literals
+
+import json
+import threading
+import time
+
+from ..extractor.niconico import NiconicoIE
+from .external import FFmpegFD
+from .common import FileDownloader
+from ..utils import (
+    str_or_none,
+    std_headers,
+    to_str,
+    DownloadError,
+    try_get,
+)
+from ..compat import compat_str
+from ..websocket import (
+    WebSocket,
+    HAVE_WEBSOCKET,
+)
+
+
+class NiconicoDmcFD(FileDownloader):
+    """
+    Performs niconico DMC request and download it
+    Note that it very differs from yt-dlp one
+    """
+
+    def real_download(self, filename, info_dict):
+        from ..downloader import get_suitable_downloader
+
+        nie = self.ydl.get_info_extractor(NiconicoIE.ie_key())
+
+        format_id = info_dict['format_id']
+        video_id = info_dict['video_id']
+        request_url = info_dict['url']
+        dmc_data = info_dict['dmc_data']
+        session_api_data = info_dict['session_api_data']
+        extract_m3u8 = info_dict['extract_m3u8']
+
+        session_response = nie._download_json(
+            request_url, video_id,
+            query={'_format': 'json'},
+            headers={'Content-Type': 'application/json'},
+            note='Downloading JSON metadata for %s' % format_id,
+            data=json.dumps(dmc_data).encode())
+
+        # get heartbeat info
+        heartbeat_url = session_api_data['urls'][0]['url'] + '/' + session_response['data']['session']['id'] + '?_format=json&_method=PUT'
+        heartbeat_data = json.dumps(session_response['data']).encode()
+        # interval, convert milliseconds to seconds, then halve to make a buffer.
+        heartbeat_interval = session_api_data['heartbeatLifetime'] / 8000
+
+        info_dict.update({
+            'url': session_response['data']['session']['content_uri'],
+            'protocol': info_dict['expected_protocol'],
+            'heartbeat_url': heartbeat_url,
+            'heartbeat_data': heartbeat_data,
+            'heartbeat_interval': heartbeat_interval,
+        })
+
+        if extract_m3u8:
+            try:
+                m3u8_format = nie._extract_m3u8_formats(
+                    info_dict['url'], video_id, ext='mp4', entry_protocol='m3u8_native', note=False)[0]
+            except BaseException:
+                info_dict['protocol'] = 'm3u8'
+            else:
+                del m3u8_format['format_id'], m3u8_format['protocol']
+                info_dict.update(m3u8_format)
+
+        return get_suitable_downloader(info_dict)(self.ydl, self.params or {}).download(filename, info_dict)
+
+
+class NiconicoLiveFD(FileDownloader):
+    """ Downloads niconico live without being stopped """
+
+    def real_download(self, filename, info_dict):
+        if not HAVE_WEBSOCKET:  # this is unreachable because this is checked at Extractor
+            raise DownloadError('Install websockets or websocket_client package via pip, or install websockat program')
+
+        video_id = info_dict['video_id']
+        ws_url = info_dict['url']
+        cookies = info_dict.get('cookies')
+        live_quality = info_dict.get('live_quality', 'high')
+        dl = FFmpegFD(self.ydl, self.params or {})
+        self.to_screen('[%s] %s: Fetching HLS playlist info via WebSocket' % ('niconico:live', video_id))
+
+        new_info_dict = {}
+        new_info_dict.update(info_dict)
+        new_info_dict.update({
+            'url': None,
+            'protocol': 'ffmpeg',
+        })
+        lock = threading.Lock()
+        lock.acquire()
+
+        def communicate_ws(reconnect):
+            with WebSocket(ws_url, {
+                'Cookie': str_or_none(cookies) or '',
+                'Origin': 'https://live2.nicovideo.jp',
+                'Accept': '*/*',
+                'User-Agent': std_headers['User-Agent'],
+            }) as ws:
+                if self.ydl.params.get('verbose', False):
+                    self.to_screen('[debug] Sending HLS server request')
+                ws.send(json.dumps({
+                    "type": "startWatching",
+                    "data": {
+                        "stream": {
+                            "quality": live_quality,
+                            "protocol": "hls",
+                            "latency": "high",
+                            "chasePlay": False
+                        },
+                        "room": {
+                            "protocol": "webSocket",
+                            "commentable": True
+                        },
+                        "reconnect": reconnect
+                    }
+                }))
+
+                while True:
+                    recv = to_str(ws.recv()).strip()
+                    if not recv:
+                        continue
+                    data = json.loads(recv)
+                    if not data or not isinstance(data, dict):
+                        continue
+                    if data.get('type') == 'stream' and not new_info_dict.get('url'):
+                        new_info_dict['url'] = data['data']['uri']
+                        lock.release()
+                    elif data.get('type') == 'ping':
+                        # pong back
+                        ws.send(r'{"type":"pong"}')
+                        ws.send(r'{"type":"keepSeat"}')
+                    elif data.get('type') == 'disconnect':
+                        print(data)
+                        return True
+                    elif data.get('type') == 'error':
+                        message = try_get(data, lambda x: x["body"]["code"], compat_str) or recv
+                        return DownloadError(message)
+                    elif self.ydl.params.get('verbose', False):
+                        if len(recv) > 100:
+                            recv = recv[:100] + '...'
+                        self.to_screen('[debug] Server said: %s' % recv)
+
+        def ws_main():
+            reconnect = False
+            while True:
+                try:
+                    ret = communicate_ws(reconnect)
+                    if ret is True:
+                        return
+                    if isinstance(ret, BaseException):
+                        new_info_dict['error'] = ret
+                        lock.release()
+                        return
+                except BaseException as e:
+                    self.to_screen('[%s] %s: Connection error occured, reconnecting after 10 seconds: %s' % ('niconico:live', video_id, str_or_none(e)))
+                    time.sleep(10)
+                    continue
+                finally:
+                    reconnect = True
+
+        thread = threading.Thread(target=ws_main, daemon=True)
+        thread.start()
+
+        lock.acquire(True)
+        err = new_info_dict.get('error')
+        if isinstance(err, BaseException):
+            raise err
+        return dl.download(filename, new_info_dict)
diff --git a/youtube_dl/downloader/rtmp.py b/youtube_dl/downloader/rtmp.py
index fbb7f51b0..5312b1e33 100644
--- a/youtube_dl/downloader/rtmp.py
+++ b/youtube_dl/downloader/rtmp.py
@@ -1,6 +1,5 @@
 from __future__ import unicode_literals
 
-import os
 import re
 import subprocess
 import time
@@ -180,7 +179,7 @@ class RtmpFD(FileDownloader):
             return False
 
         while retval in (RD_INCOMPLETE, RD_FAILED) and not test and not live:
-            prevsize = os.path.getsize(encodeFilename(tmpfilename))
+            prevsize = self.ydl.getsize(encodeFilename(tmpfilename))
             self.to_screen('[rtmpdump] Downloaded %s bytes' % prevsize)
             time.sleep(5.0)  # This seems to be needed
             args = basic_args + ['--resume']
@@ -188,7 +187,7 @@ class RtmpFD(FileDownloader):
                 args += ['--skip', '1']
             args = [encodeArgument(a) for a in args]
             retval = run_rtmpdump(args)
-            cursize = os.path.getsize(encodeFilename(tmpfilename))
+            cursize = self.ydl.getsize(encodeFilename(tmpfilename))
             if prevsize == cursize and retval == RD_FAILED:
                 break
             # Some rtmp streams seem abort after ~ 99.8%. Don't complain for those
@@ -197,7 +196,7 @@ class RtmpFD(FileDownloader):
                 retval = RD_SUCCESS
                 break
         if retval == RD_SUCCESS or (test and retval == RD_INCOMPLETE):
-            fsize = os.path.getsize(encodeFilename(tmpfilename))
+            fsize = self.ydl.getsize(encodeFilename(tmpfilename))
             self.to_screen('[rtmpdump] Downloaded %s bytes' % fsize)
             self.try_rename(tmpfilename, filename)
             self._hook_progress({
diff --git a/youtube_dl/downloader/rtsp.py b/youtube_dl/downloader/rtsp.py
index 939358b2a..d1ff013c3 100644
--- a/youtube_dl/downloader/rtsp.py
+++ b/youtube_dl/downloader/rtsp.py
@@ -1,6 +1,5 @@
 from __future__ import unicode_literals
 
-import os
 import subprocess
 
 from .common import FileDownloader
@@ -31,7 +30,7 @@ class RtspFD(FileDownloader):
 
         retval = subprocess.call(args)
         if retval == 0:
-            fsize = os.path.getsize(encodeFilename(tmpfilename))
+            fsize = self.ydl.getsize(encodeFilename(tmpfilename))
             self.to_screen('\r[%s] %s bytes' % (args[0], fsize))
             self.try_rename(tmpfilename, filename)
             self._hook_progress({
diff --git a/youtube_dl/downloader/youtube_live_chat.py b/youtube_dl/downloader/youtube_live_chat.py
new file mode 100644
index 000000000..2bfc18381
--- /dev/null
+++ b/youtube_dl/downloader/youtube_live_chat.py
@@ -0,0 +1,122 @@
+from __future__ import division, unicode_literals
+
+import json
+
+from .fragment import FragmentFD
+from ..compat import compat_urllib_error
+from ..utils import (
+    try_get,
+    RegexNotFoundError,
+)
+from ..extractor.youtube import YoutubeBaseInfoExtractor as YT_BaseIE
+
+
+class YoutubeLiveChatReplayFD(FragmentFD):
+    """ Downloads YouTube live chat replays fragment by fragment """
+
+    FD_NAME = 'youtube_live_chat_replay'
+
+    def real_download(self, filename, info_dict):
+        video_id = info_dict['video_id']
+        self.to_screen('[%s] Downloading live chat' % self.FD_NAME)
+
+        fragment_retries = self.params.get('fragment_retries', 0)
+        test = self.params.get('test', False)
+
+        ctx = {
+            'filename': filename,
+            'live': True,
+            'total_frags': None,
+        }
+
+        ie = YT_BaseIE(self.ydl)
+
+        def dl_fragment(url, data=None, headers=None):
+            http_headers = info_dict.get('http_headers', {})
+            if headers:
+                http_headers = http_headers.copy()
+                http_headers.update(headers)
+            return self._download_fragment(ctx, url, info_dict, headers=http_headers, request_data=data)
+
+        def download_and_parse_fragment(url, frag_index, request_data):
+            count = 0
+            while count <= fragment_retries:
+                try:
+                    success, raw_fragment = dl_fragment(url, request_data, {'content-type': 'application/json'})
+                    if not success:
+                        return False, None, None
+                    try:
+                        data = ie._extract_yt_initial_data(video_id, raw_fragment.decode('utf-8', 'replace'))
+                    except RegexNotFoundError:
+                        data = None
+                    if not data:
+                        data = json.loads(raw_fragment)
+                    live_chat_continuation = try_get(
+                        data,
+                        lambda x: x['continuationContents']['liveChatContinuation'], dict) or {}
+                    offset = continuation_id = None
+                    processed_fragment = bytearray()
+                    for action in live_chat_continuation.get('actions', []):
+                        if 'replayChatItemAction' in action:
+                            replay_chat_item_action = action['replayChatItemAction']
+                            offset = int(replay_chat_item_action['videoOffsetTimeMsec'])
+                        processed_fragment.extend(
+                            json.dumps(action, ensure_ascii=False).encode('utf-8') + b'\n')
+                    if offset is not None:
+                        continuation_id = try_get(
+                            live_chat_continuation,
+                            lambda x: x['continuations'][0]['liveChatReplayContinuationData']['continuation'])
+                    self._append_fragment(ctx, processed_fragment)
+
+                    return True, continuation_id, offset
+                except compat_urllib_error.HTTPError as err:
+                    count += 1
+                    if count <= fragment_retries:
+                        self.report_retry_fragment(err, frag_index, count, fragment_retries)
+            if count > fragment_retries:
+                self.report_error('giving up after %s fragment retries' % fragment_retries)
+                return False, None, None
+
+        self._prepare_and_start_frag_download(ctx)
+
+        success, raw_fragment = dl_fragment(info_dict['url'])
+        if not success:
+            return False
+        try:
+            data = ie._extract_yt_initial_data(video_id, raw_fragment.decode('utf-8', 'replace'))
+        except RegexNotFoundError:
+            return False
+        continuation_id = try_get(
+            data,
+            lambda x: x['contents']['twoColumnWatchNextResults']['conversationBar']['liveChatRenderer']['continuations'][0]['reloadContinuationData']['continuation'])
+        # no data yet but required to call _append_fragment
+        self._append_fragment(ctx, b'')
+
+        ytcfg = ie._extract_ytcfg(video_id, raw_fragment.decode('utf-8', 'replace'))
+
+        if not ytcfg:
+            return False
+        api_key = try_get(ytcfg, lambda x: x['INNERTUBE_API_KEY'])
+        innertube_context = try_get(ytcfg, lambda x: x['INNERTUBE_CONTEXT'])
+        if not api_key or not innertube_context:
+            return False
+        url = 'https://www.youtube.com/youtubei/v1/live_chat/get_live_chat_replay?key=' + api_key
+
+        frag_index = offset = 0
+        while continuation_id is not None:
+            frag_index += 1
+            request_data = {
+                'context': innertube_context,
+                'continuation': continuation_id,
+            }
+            if frag_index > 1:
+                request_data['currentPlayerState'] = {'playerOffsetMs': str(max(offset - 5000, 0))}
+            success, continuation_id, offset = download_and_parse_fragment(
+                url, frag_index, json.dumps(request_data, ensure_ascii=False).encode('utf-8') + b'\n')
+            if not success:
+                return False
+            if test:
+                break
+
+        self._finish_frag_download(ctx)
+        return True
diff --git a/youtube_dl/extractor/askmona.py b/youtube_dl/extractor/askmona.py
new file mode 100644
index 000000000..18056f01a
--- /dev/null
+++ b/youtube_dl/extractor/askmona.py
@@ -0,0 +1,32 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import (
+    ExtractorError,
+)
+
+
+class AskMonaIE(InfoExtractor):
+    IE_NAME = 'askmona'
+    _VALID_URL = r'https?://(?:www\.)?askmona.org/(?P<id>\d+)/?'
+    ALL_POSTS_URL = 'https://askmona.org/%s?n=1000'
+    YOUTUBE_RE = r'<a\s+class="youtube"\s*name="([0-9A-Za-z_-]{11})"\s*href="[^"]*"\s*>'
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage(self.ALL_POSTS_URL % video_id, video_id)
+
+        youtube_results = [self.url_result('https://youtu.be/%s' % frg.group(1))
+                           for frg in re.finditer(self.YOUTUBE_RE, webpage)]
+        if not youtube_results:
+            raise ExtractorError('No videos found', expected=True)
+        return self.playlist_result(youtube_results, video_id)
+
+
+class AskMona3IE(AskMonaIE):
+    IE_NAME = 'askmona3'
+    _VALID_URL = r'https?://web3\.askmona.org/(?P<id>\d+)/?'
+    ALL_POSTS_URL = 'https://web3.askmona.org/%s?n=1000'
diff --git a/youtube_dl/extractor/bilibili.py b/youtube_dl/extractor/bilibili.py
index 589fdc1ce..0746db4f4 100644
--- a/youtube_dl/extractor/bilibili.py
+++ b/youtube_dl/extractor/bilibili.py
@@ -3,9 +3,12 @@ from __future__ import unicode_literals
 
 import hashlib
 import re
+import itertools
 
+from math import ceil
 from .common import InfoExtractor
 from ..compat import (
+    compat_str,
     compat_parse_qs,
     compat_urlparse,
 )
@@ -17,6 +20,7 @@ from ..utils import (
     smuggle_url,
     str_or_none,
     strip_jsonp,
+    try_get,
     unified_timestamp,
     unsmuggle_url,
     urlencode_postdata,
@@ -325,6 +329,63 @@ class BiliBiliBangumiIE(InfoExtractor):
             season_info.get('bangumi_title'), season_info.get('evaluate'))
 
 
+class BilibiliChannelIE(InfoExtractor):
+    _VALID_URL = r'https?://space.bilibili\.com/(?P<id>\d+)'
+    _API_URL = "https://api.bilibili.com/x/space/arc/search?mid=%s&pn=%d&jsonp=jsonp"
+    _USER_INFO_API_URL = "https://api.bilibili.com/x/space/acc/info?mid=%s&jsonp=jsonp"
+    _TESTS = [{
+        'url': 'https://space.bilibili.com/3985676/video',
+        'info_dict': {
+            'id': '3985676',
+            'title': 'Êâ∂Ê°ëÂ§ßÁ∫¢Ëä±‰∏∂',
+        },
+        'playlist_count': 111,
+    }, {
+        'url': 'https://space.bilibili.com/477631979',
+        'info_dict': {
+            'id': '477631979',
+            'title': 'vansamaofficial',
+        },
+        'playlist_count': 98,
+    }]
+
+    def _real_extract(self, url):
+        list_id = self._match_id(url)
+
+        try:
+            user_info = self._download_json(
+                self._USER_INFO_API_URL % list_id, list_id,
+                fatal=False)['data']
+        except BaseException:
+            user_info = None
+        title = try_get(user_info, lambda x: x['name'], compat_str) or list_id
+
+        entries = []
+        for page_num in itertools.count(1):
+            data = self._download_json(
+                self._API_URL % (list_id, page_num), list_id,
+                note='Downloading page %d' % page_num)['data']
+            vlist = data['list']['vlist']
+            count = data['page']['count']
+
+            entries.extend(
+                self.url_result(
+                    'https://www.bilibili.com/video/%s' % entry['bvid'],
+                    BiliBiliIE.ie_key(),
+                    entry['bvid']) for entry in vlist)
+
+            page_maximum = ceil(count / data['page']['ps'])
+            if page_maximum <= page_num or not vlist or len(entries) == count:
+                break
+
+        return {
+            '_type': 'playlist',
+            'id': list_id,
+            'title': title,
+            'entries': entries
+        }
+
+
 class BilibiliAudioBaseIE(InfoExtractor):
     def _call_api(self, path, sid, query=None):
         if not query:
diff --git a/youtube_dl/extractor/cloudflarestream.py b/youtube_dl/extractor/cloudflarestream.py
index 2fdcfbb3a..e73590390 100644
--- a/youtube_dl/extractor/cloudflarestream.py
+++ b/youtube_dl/extractor/cloudflarestream.py
@@ -6,6 +6,10 @@ import re
 
 from .common import InfoExtractor
 
+from ..compat import (
+    compat_urllib_parse_urlparse,
+)
+
 
 class CloudflareStreamIE(InfoExtractor):
     _DOMAIN_RE = r'(?:cloudflarestream\.com|(?:videodelivery|bytehighway)\.net)'
@@ -50,7 +54,8 @@ class CloudflareStreamIE(InfoExtractor):
 
     def _real_extract(self, url):
         video_id = self._match_id(url)
-        domain = 'bytehighway.net' if 'bytehighway.net/' in url else 'videodelivery.net'
+        parsed_url = compat_urllib_parse_urlparse(url)
+        domain = 'bytehighway.net' if parsed_url.hostname.endswith('bytehighway.net') else 'videodelivery.net'
         base_url = 'https://%s/%s/' % (domain, video_id)
         if '.' in video_id:
             video_id = self._parse_json(base64.urlsafe_b64decode(
diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py
index 797c35fd5..b2fcefcd1 100644
--- a/youtube_dl/extractor/common.py
+++ b/youtube_dl/extractor/common.py
@@ -82,6 +82,7 @@ from ..utils import (
     xpath_text,
     xpath_with_ns,
 )
+from ..websocket import HAVE_WEBSOCKET
 
 
 class InfoExtractor(object):
@@ -396,6 +397,15 @@ class InfoExtractor(object):
     _GEO_COUNTRIES = None
     _GEO_IP_BLOCKS = None
     _WORKING = True
+    """
+    Feature dependency declaration. Case sensitive.
+
+    Following features are known and recognized:
+
+    - websocket - Requires WebSocket-related package/command
+    - yaml - pyyaml package
+    """
+    _FEATURE_DEPENDENCY = tuple()
 
     def __init__(self, downloader=None):
         """Constructor. Receives an optional downloader."""
@@ -406,21 +416,22 @@ class InfoExtractor(object):
     @classmethod
     def suitable(cls, url):
         """Receives a URL and returns True if suitable for this IE."""
+        return cls._valid_url_re().match(url) is not None
 
+    @classmethod
+    def _match_id(cls, url):
+        m = cls._valid_url_re().match(url)
+        assert m
+        return compat_str(m.group('id'))
+
+    @classmethod
+    def _valid_url_re(cls):
         # This does not use has/getattr intentionally - we want to know whether
         # we have cached the regexp for *this* class, whereas getattr would also
         # match the superclass
         if '_VALID_URL_RE' not in cls.__dict__:
             cls._VALID_URL_RE = re.compile(cls._VALID_URL)
-        return cls._VALID_URL_RE.match(url) is not None
-
-    @classmethod
-    def _match_id(cls, url):
-        if '_VALID_URL_RE' not in cls.__dict__:
-            cls._VALID_URL_RE = re.compile(cls._VALID_URL)
-        m = cls._VALID_URL_RE.match(url)
-        assert m
-        return compat_str(m.group('id'))
+        return cls._VALID_URL_RE
 
     @classmethod
     def working(cls):
@@ -527,6 +538,14 @@ class InfoExtractor(object):
 
     def extract(self, url):
         """Extracts URL information and returns it in list of dicts."""
+
+        if 'websocket' in self._FEATURE_DEPENDENCY and not HAVE_WEBSOCKET:
+            raise ExtractorError('Please install websockets or websocket_client package via pip, or websockat command', expected=True)
+        try:
+            if 'yaml' in self._FEATURE_DEPENDENCY:
+                __import__('yaml')
+        except ImportError:
+            raise ExtractorError('Please install pyyaml package via pip.', expected=True)
         try:
             for _ in range(2):
                 try:
@@ -627,6 +646,12 @@ class InfoExtractor(object):
                 url_or_request = update_url_query(url_or_request, query)
             if data is not None or headers:
                 url_or_request = sanitized_Request(url_or_request, data, headers)
+        if self._downloader.params.get('verbose', False):
+            if isinstance(url_or_request, compat_urllib_request.Request):
+                url = url_or_request.get_full_url()
+            else:
+                url = url_or_request
+            self.to_screen('[debug] Fetching webpage from %s' % url)
         exceptions = [compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error]
         if hasattr(ssl, 'CertificateError'):
             exceptions.append(ssl.CertificateError)
@@ -741,7 +766,7 @@ class InfoExtractor(object):
                 absfilepath = os.path.abspath(filename)
                 if len(absfilepath) > 259:
                     filename = '\\\\?\\' + absfilepath
-            with open(filename, 'wb') as outf:
+            with self._downloader.open(filename, 'wb') as outf:
                 outf.write(webpage_bytes)
 
         try:
@@ -793,6 +818,7 @@ class InfoExtractor(object):
 
         success = False
         try_count = 0
+        res = False
         while success is False:
             try:
                 res = self._download_webpage_handle(
@@ -970,6 +996,9 @@ class InfoExtractor(object):
     @staticmethod
     def playlist_result(entries, playlist_id=None, playlist_title=None, playlist_description=None):
         """Returns a playlist"""
+        if isinstance(entries, list) and len(entries) == 1:
+            return entries[0]
+
         video_info = {'_type': 'playlist',
                       'entries': entries}
         if playlist_id:
@@ -988,12 +1017,16 @@ class InfoExtractor(object):
         RegexNotFoundError, depending on fatal, specifying the field name.
         """
         if isinstance(pattern, (str, compat_str, compiled_regex_type)):
-            mobj = re.search(pattern, string, flags)
-        else:
-            for p in pattern:
-                mobj = re.search(p, string, flags)
-                if mobj:
-                    break
+            pattern = [pattern]
+        mobj = None
+        for p in pattern:
+            if self._downloader.params.get('verbose', False):
+                self.to_screen('trying regex %s' % p)
+            mobj = re.search(p, string, flags)
+            if mobj:
+                if self._downloader.params.get('verbose', False):
+                    self.to_screen('%s found' % p)
+                break
 
         if not self._downloader.params.get('no_color') and compat_os_name != 'nt' and sys.stderr.isatty():
             _name = '\033[0;34m%s\033[0m' % name
@@ -1101,6 +1134,16 @@ class InfoExtractor(object):
                     (?=[^>]+(?:itemprop|name|property|id|http-equiv)=(["\']?)%s\1)
                     [^>]+?content=(["\'])(?P<content>.*?)\2''' % re.escape(prop)
 
+    @staticmethod
+    def _meta_regexes(prop):
+        property_re = r'(?:itemprop|name|property|id|http-equiv)=(["\']?)%s' % re.escape(prop)
+        content_re = r'content=(["\'])(?P<content>.*?)'
+        template = r'(?isx)<meta(?=[^>]+%s)\1[^>]+?%s\2'
+        return [
+            template % (property_re, content_re),
+            template % (content_re, property_re),
+        ]
+
     def _og_search_property(self, prop, html, name=None, **kargs):
         if not isinstance(prop, (list, tuple)):
             prop = [prop]
@@ -1132,14 +1175,22 @@ class InfoExtractor(object):
     def _og_search_url(self, html, **kargs):
         return self._og_search_property('url', html, **kargs)
 
-    def _html_search_meta(self, name, html, display_name=None, fatal=False, **kwargs):
+    def _html_extract_title(self, html, name, **kwargs):
+        return self._html_search_regex(
+            r'(?s)<title>(.*?)</title>', html, name, **kwargs)
+
+    def _html_search_meta(self, name, html, display_name=None, fatal=False, **kargs):
         if not isinstance(name, (list, tuple)):
             name = [name]
         if display_name is None:
-            display_name = name[0]
-        return self._html_search_regex(
-            [self._meta_regex(n) for n in name],
-            html, display_name, fatal=fatal, group='content', **kwargs)
+            display_name = ' '.join(name)
+        meta_regexes = []
+        for p in name:
+            meta_regexes.append(self._meta_regex(p))
+        escaped = self._search_regex(meta_regexes, html, display_name, flags=re.DOTALL, fatal=fatal, group='content', **kargs)
+        if escaped is None:
+            return None
+        return unescapeHTML(escaped)
 
     def _dc_search_uploader(self, html):
         return self._html_search_meta('dc.creator', html, 'uploader')
@@ -1369,7 +1420,15 @@ class InfoExtractor(object):
             html, '%s form' % form_id, group='form')
         return self._hidden_inputs(form)
 
-    def _sort_formats(self, formats, field_preference=None):
+    PROTOCOL_PREFERENCE = {
+        'm3u8': 1,
+        'm3u8_native': 1,
+        'http': 0,
+        'https': 0,
+        'rtsp': -0.5,
+    }
+
+    def _sort_formats(self, formats, field_preference=None, id_preference_dict={}):
         if not formats:
             raise ExtractorError('No video formats found')
 
@@ -1398,8 +1457,18 @@ class InfoExtractor(object):
                 if f.get('ext') in ['f4f', 'f4m']:  # Not yet supported
                     preference -= 0.5
 
-            protocol = f.get('protocol') or determine_protocol(f)
-            proto_preference = 0 if protocol in ['http', 'https'] else (-0.5 if protocol == 'rtsp' else -0.1)
+            # expected_protocol key is only used by NiconicoDmcFD
+            protocol = f.get('expected_protocol') or f.get('protocol') or determine_protocol(f)
+            proto_preference = self.PROTOCOL_PREFERENCE.get(protocol, -0.1)
+
+            filesize = f.get('filesize') if f.get('filesize') is not None else -1
+            filesize_approx = f.get('filesize_approx') if f.get('filesize_approx') is not None else -1
+            tbr = f.get('tbr') if f.get('tbr') is not None else -1
+            vbr = f.get('vbr') if f.get('vbr') is not None else -1
+            abr = f.get('abr') if f.get('abr') is not None else -1
+            if self._downloader.params.get('prefer_smaller_formats'):
+                filesize *= -1
+                filesize_approx *= -1
 
             if f.get('vcodec') == 'none':  # audio only
                 preference -= 50
@@ -1415,6 +1484,9 @@ class InfoExtractor(object):
             else:
                 if f.get('acodec') == 'none':  # video only
                     preference -= 40
+                    if self._downloader.params.get('prefer_smaller_formats'):
+                        tbr *= -1
+                        vbr *= -1
                 if self._downloader.params.get('prefer_free_formats'):
                     ORDER = ['flv', 'mp4', 'webm']
                 else:
@@ -1425,23 +1497,31 @@ class InfoExtractor(object):
                     ext_preference = -1
                 audio_ext_preference = 0
 
+            fmt_id = f.get('format_id') or 'none'
+
+            id_preference = 0
+            for k, v in id_preference_dict.items():
+                if k in fmt_id.lower():
+                    id_preference += v
+
             return (
                 preference,
                 f.get('language_preference') if f.get('language_preference') is not None else -1,
                 f.get('quality') if f.get('quality') is not None else -1,
-                f.get('tbr') if f.get('tbr') is not None else -1,
-                f.get('filesize') if f.get('filesize') is not None else -1,
-                f.get('vbr') if f.get('vbr') is not None else -1,
-                f.get('height') if f.get('height') is not None else -1,
-                f.get('width') if f.get('width') is not None else -1,
+                f.get('height') if f.get('height') is not None else -1,  # moved
+                f.get('width') if f.get('width') is not None else -1,  # moved
+                tbr,
+                filesize,
+                vbr,
                 proto_preference,
                 ext_preference,
-                f.get('abr') if f.get('abr') is not None else -1,
+                abr,
                 audio_ext_preference,
                 f.get('fps') if f.get('fps') is not None else -1,
-                f.get('filesize_approx') if f.get('filesize_approx') is not None else -1,
+                filesize_approx,
                 f.get('source_preference') if f.get('source_preference') is not None else -1,
-                f.get('format_id') if f.get('format_id') is not None else '',
+                id_preference,
+                fmt_id,
             )
         formats.sort(key=_formats_key)
 
@@ -1634,12 +1714,13 @@ class InfoExtractor(object):
     def _extract_m3u8_formats(self, m3u8_url, video_id, ext=None,
                               entry_protocol='m3u8', preference=None,
                               m3u8_id=None, note=None, errnote=None,
+                              quality=None,
                               fatal=True, live=False, data=None, headers={},
                               query={}):
         res = self._download_webpage_handle(
             m3u8_url, video_id,
-            note=note or 'Downloading m3u8 information',
-            errnote=errnote or 'Failed to download m3u8 information',
+            note=note if note is not None else 'Downloading m3u8 information',
+            errnote=errnote if errnote is not None else 'Failed to download m3u8 information',
             fatal=fatal, data=data, headers=headers, query=query)
 
         if res is False:
@@ -1650,11 +1731,11 @@ class InfoExtractor(object):
 
         return self._parse_m3u8_formats(
             m3u8_doc, m3u8_url, ext=ext, entry_protocol=entry_protocol,
-            preference=preference, m3u8_id=m3u8_id, live=live)
+            preference=preference, m3u8_id=m3u8_id, live=live, quality=quality)
 
     def _parse_m3u8_formats(self, m3u8_doc, m3u8_url, ext=None,
                             entry_protocol='m3u8', preference=None,
-                            m3u8_id=None, live=False):
+                            m3u8_id=None, live=False, quality=None):
         if '#EXT-X-FAXS-CM:' in m3u8_doc:  # Adobe Flash Access
             return []
 
@@ -1719,6 +1800,7 @@ class InfoExtractor(object):
                     'ext': ext,
                     'protocol': entry_protocol,
                     'preference': preference,
+                    'quality': quality,
                 }
                 if media_type == 'AUDIO':
                     f['vcodec'] = 'none'
@@ -1778,6 +1860,7 @@ class InfoExtractor(object):
                     'fps': float_or_none(last_stream_inf.get('FRAME-RATE')),
                     'protocol': entry_protocol,
                     'preference': preference,
+                    'quality': quality,
                 }
                 resolution = last_stream_inf.get('RESOLUTION')
                 if resolution:
diff --git a/youtube_dl/extractor/crunchyroll.py b/youtube_dl/extractor/crunchyroll.py
index bc2d1fa8b..bdb4c5dd4 100644
--- a/youtube_dl/extractor/crunchyroll.py
+++ b/youtube_dl/extractor/crunchyroll.py
@@ -550,7 +550,8 @@ Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
                         'width': int_or_none(xpath_text(metadata, './width')),
                     }
 
-                    if '.fplive.net/' in video_url:
+                    parsed_url = compat_urlparse.urlparse(video_url)
+                    if parsed_url.hostname.endswith('.fplive.net/'):
                         video_url = re.sub(r'^rtmpe?://', 'http://', video_url.strip())
                         parsed_video_url = compat_urlparse.urlparse(video_url)
                         direct_video_url = compat_urlparse.urlunparse(parsed_video_url._replace(
diff --git a/youtube_dl/extractor/damtomo.py b/youtube_dl/extractor/damtomo.py
new file mode 100644
index 000000000..59bab0d2b
--- /dev/null
+++ b/youtube_dl/extractor/damtomo.py
@@ -0,0 +1,80 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import ExtractorError, clean_html, int_or_none, try_get
+from ..compat import compat_etree_fromstring, compat_str
+
+
+class DamtomoIE(InfoExtractor):
+    IE_NAME = 'clubdam:damtomo'
+    _VALID_URL = r'https?://(www\.)?clubdam\.com/app/damtomo/(?:SP/)?karaokeMovie/StreamingDkm\.do\?karaokeMovieId=(?P<id>\d+)'
+    _TEST = {
+        'url': 'https://www.clubdam.com/app/damtomo/karaokeMovie/StreamingDkm.do?karaokeMovieId=2414316',
+        'info_dict': {
+            'id': '2414316',
+            'uploader': 'Ôº´„Éâ„É≠„É≥',
+            'uploader_id': 'ODk5NTQwMzQ',
+            'song_title': 'Get Wild',
+            'song_artist': 'TM NETWORK(TMN)',
+            'upload_date': '20201226',
+        }
+    }
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage, handle = self._download_webpage_handle(
+            'https://www.clubdam.com/app/damtomo/karaokeMovie/StreamingDkm.do?karaokeMovieId=%s' % video_id, video_id,
+            encoding='sjis')
+
+        if handle.url == 'https://www.clubdam.com/sorry/':
+            raise ExtractorError('You are rate-limited. Try again later.', expected=True)
+        if '<h2>‰∫àÊúü„Åõ„Å¨„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇ</h2>' in webpage:
+            raise ExtractorError('There is a error in server-side. Try again later.', expected=True)
+
+        # NOTE: there is excessive amount of spaces and line breaks, so ignore spaces around these part
+        description = self._search_regex(r'(?m)<div id="public_comment">\s*<p>\s*([^<]*?)\s*</p>', webpage, 'description', default=None)
+        uploader_id = self._search_regex(r'<a href="https://www\.clubdam\.com/app/damtomo/member/info/Profile\.do\?damtomoId=([^"]+)"', webpage, 'uploader_id', default=None)
+
+        # cleaner way to extract information in HTML
+        # example content: https://gist.github.com/nao20010128nao/1d419cc9ca3177be134094addf28ab51
+        data_dict = {g.group(2): clean_html(g.group(3), True) for g in re.finditer(r'(?s)<(p|div)\s+class="([^" ]+?)">(.+?)</\1>', webpage)}
+        data_dict = {k: re.sub(r'\s+', ' ', v) for k, v in data_dict.items() if v}
+        # print(json.dumps(data_dict))
+
+        # since videos do not have title, name the video like '%(song_title)s-%(song_artist)s-%(uploader)s' for convenience
+        data_dict['user_name'] = re.sub(r'\s*„Åï„Çì', '', data_dict['user_name'])
+        title = '%(song_title)s-%(song_artist)s-%(user_name)s' % data_dict
+
+        stream_xml = self._download_webpage(
+            'https://www.clubdam.com/app/damtomo/karaokeMovie/GetStreamingDkmUrlXML.do?movieSelectFlg=2&karaokeMovieId=%s' % video_id, video_id,
+            note='Requesting stream information', encoding='sjis')
+        try:
+            stream_tree = compat_etree_fromstring(stream_xml)
+            m3u8_url = try_get(stream_tree, lambda x: x.find(
+                './/d:streamingUrl',
+                {'d': 'https://www.clubdam.com/app/damtomo/karaokeMovie/GetStreamingDkmUrlXML'}).text.strip(), compat_str)
+            if not m3u8_url or not isinstance(m3u8_url, compat_str):
+                raise ExtractorError('There is no streaming URL')
+        except ValueError:  # Python <= 2, ValueError: multi-byte encodings are not supported
+            m3u8_url = self._search_regex(r'<streamingUrl>\s*(.+?)\s*</streamingUrl>', stream_xml, 'm3u8 url')
+        formats = self._extract_m3u8_formats(
+            m3u8_url, video_id,
+            ext='mp4', entry_protocol='m3u8_native', m3u8_id='hls')
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'uploader_id': uploader_id,
+            'description': description,
+            'formats': formats,
+            'uploader': data_dict['user_name'],
+            'upload_date': try_get(data_dict, lambda x: self._search_regex(r'(\d\d\d\d/\d\d/\d\d)', x['date'], 'upload_date', default=None).replace('/', ''), compat_str),
+            'view_count': int_or_none(self._search_regex(r'(\d+)', data_dict['audience'], 'view_count', default=None)),
+            'like_count': int_or_none(self._search_regex(r'(\d+)', data_dict['nice'], 'like_count', default=None)),
+            'song_title': data_dict['song_title'],
+            'song_artist': data_dict['song_artist'],
+        }
diff --git a/youtube_dl/extractor/disneychris.py b/youtube_dl/extractor/disneychris.py
new file mode 100644
index 000000000..62050f1b2
--- /dev/null
+++ b/youtube_dl/extractor/disneychris.py
@@ -0,0 +1,61 @@
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import (
+    clean_html,
+    determine_ext,
+)
+
+
+class DisneyChrisBaseIE(InfoExtractor):
+    def extract_article_sections(self, webpage, base_id):
+        sections = []
+        for idx, x in enumerate(re.finditer(r'(?s)<td style="text-align: center;">(.+?)</td>', webpage)):
+            content = x.group(1)
+
+            title = self._search_regex(r'class="ch\d{2}Ttl[AB]">(.+?)\s*&nbsp;<span', content, 'title', group=1, default=False)
+            title = clean_html(title)
+
+            description = self._search_regex(r'class="ch\d{2}TrkC">(.+?)</div>', content, 'description', group=1, default=None)
+            description = clean_html(description)
+
+            audio_url = self._search_regex(r'class="amazingaudioplayer-source" data-src="(.+\.mp3)"', content, 'audio url', group=1, default=False)
+            if not title or not audio_url:
+                continue
+
+            title = re.sub(r'^\d+\s*-\s*', '', title)
+
+            sections.append({
+                'id': '%s-%d' % (base_id, idx),
+                'title': title,
+                'description': description,
+                'url': audio_url,
+                'ext': determine_ext(audio_url, 'mp3'),
+                'vcodec': 'none',
+                'acodec': 'mp3',
+            })
+
+        return sections
+
+
+class DisneyChrisIE(DisneyChrisBaseIE):
+    IE_NAME = 'disneychris'
+    _VALID_URL = r'https?://(?:www\.)?disneychris\.com/(?P<id>a-day-at-disneyland|15-disneyland-soundtracks/.+?)\.html'
+
+    def _real_extract(self, url):
+        long_id = self._match_id(url)
+        video_id = long_id.split('/')[-1]
+        webpage = self._download_webpage(url, video_id)
+        sections = self.extract_article_sections(webpage, video_id)
+
+        title = None
+        for section in re.finditer(r'href="/(.+?)\.html">(.+?)</a>', webpage):
+            if section.group(1) == long_id:
+                title = section.group(2)
+                break
+        else:
+            title = self._search_regex(r'<title>(.+?)</title>', webpage, 'playlist title', group=1)
+
+        return self.playlist_result(sections, video_id, title)
diff --git a/youtube_dl/extractor/dnatube.py b/youtube_dl/extractor/dnatube.py
new file mode 100644
index 000000000..058db7954
--- /dev/null
+++ b/youtube_dl/extractor/dnatube.py
@@ -0,0 +1,33 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+
+
+class DnaTubeBaseIE(InfoExtractor):
+    pass
+
+
+class DnaTubePlaylistBaseIE(DnaTubeBaseIE):
+    pass
+
+
+class DnaTubeIE(DnaTubeBaseIE):
+    IE_NAME = 'dnatube'
+    _VALID_URL = r'https?://(www\.)?dnatube\.com/video/(?P<id>\d+)/(?:[^/]+)/?'
+    _TEST = {}
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage(url, video_id)
+
+        title = self._og_search_title(webpage, default=None)
+
+        entries = self._parse_html5_media_entries(url, webpage, video_id, m3u8_id='hls')
+        entry = entries[0]
+        self._sort_formats(entry['formats'])
+        entry.update({
+            'id': video_id,
+            'title': title,
+        })
+        return entry
diff --git a/youtube_dl/extractor/evoload.py b/youtube_dl/extractor/evoload.py
new file mode 100644
index 000000000..7133294cc
--- /dev/null
+++ b/youtube_dl/extractor/evoload.py
@@ -0,0 +1,46 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+from ..utils import (
+    try_get,
+    ExtractorError,
+)
+
+
+class EvoLoadBaseIE(InfoExtractor):
+    IE_DESC = False  # Do not list
+
+
+class EvoLoadIE(EvoLoadBaseIE):
+    IE_NAME = 'evoload'
+    # https://evoload.io/v/58AtgpYn5tAKEp
+    _VALID_URL = r'https?://evoload\.io/[ev]/(?P<id>[a-zA-Z0-9]+)/?'
+    _TEST = {}
+    TITLE_RE = (
+        r'<h3\s+class="kt-subheader__title">\s*(.+)\s*</h3>',
+        r'<title>(?:Evoload|EvoLOAD.io) - Play\s(.+)\s*</title>'
+    )
+    VIDEO_URL = 'https://evoload.io/v/%s/'
+    EMBED_URL = 'https://evoload.io/e/%s/'
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage_video = self._download_webpage(self.VIDEO_URL % video_id, video_id, note='Downloading video page')
+        webpage_embed = self._download_webpage(self.EMBED_URL % video_id, video_id, note='Downloading embed page')
+
+        title = try_get(self.TITLE_RE, (
+            lambda x: self._search_regex(x, webpage_video, 'video title'),
+            lambda x: self._search_regex(x, webpage_embed, 'video title'),
+        ), None)
+        if not title:
+            raise ExtractorError('Failed to extract video title')
+
+        entry = self._parse_html5_media_entries(url, webpage_embed, video_id, m3u8_id='hls')[0]
+        self._sort_formats(entry['formats'])
+        entry.update({
+            'id': video_id,
+            'title': title,
+            'age_limit': 18,
+        })
+        return entry
diff --git a/youtube_dl/extractor/extractors.py b/youtube_dl/extractor/extractors.py
index 402e542ae..8dacc41d2 100644
--- a/youtube_dl/extractor/extractors.py
+++ b/youtube_dl/extractor/extractors.py
@@ -77,6 +77,10 @@ from .asiancrush import (
     AsianCrushIE,
     AsianCrushPlaylistIE,
 )
+from .askmona import (
+    AskMonaIE,
+    AskMona3IE,
+)
 from .atresplayer import AtresPlayerIE
 from .atttechchannel import ATTTechChannelIE
 from .atvat import ATVAtIE
@@ -118,6 +122,7 @@ from .bild import BildIE
 from .bilibili import (
     BiliBiliIE,
     BiliBiliBangumiIE,
+    BilibiliChannelIE,
     BilibiliAudioIE,
     BilibiliAudioAlbumIE,
     BiliBiliPlayerIE,
@@ -276,6 +281,7 @@ from .dailymotion import (
     DailymotionPlaylistIE,
     DailymotionUserIE,
 )
+from .damtomo import DamtomoIE
 from .daum import (
     DaumIE,
     DaumClipIE,
@@ -289,6 +295,7 @@ from .democracynow import DemocracynowIE
 from .dfb import DFBIE
 from .dhm import DHMIE
 from .digg import DiggIE
+from .dnatube import DnaTubeIE
 from .dotsub import DotsubIE
 from .douyutv import (
     DouyuShowIE,
@@ -318,6 +325,7 @@ from .discoverygo import (
 from .discoverynetworks import DiscoveryNetworksDeIE
 from .discoveryvr import DiscoveryVRIE
 from .disney import DisneyIE
+from .disneychris import DisneyChrisIE
 from .dispeak import DigitallySpeakingIE
 from .dropbox import DropboxIE
 from .dw import (
@@ -353,10 +361,12 @@ from .espn import (
 )
 from .esri import EsriVideoIE
 from .europa import EuropaIE
+from .evoload import EvoLoadIE
 from .expotv import ExpoTVIE
 from .expressen import ExpressenIE
 from .extremetube import ExtremeTubeIE
 from .eyedotv import EyedoTVIE
+from .eyny import EynyIE
 from .facebook import (
     FacebookIE,
     FacebookPluginsVideoIE,
@@ -365,6 +375,8 @@ from .faz import FazIE
 from .fc2 import (
     FC2IE,
     FC2EmbedIE,
+    FC2UserIE,
+    FC2LiveIE,
 )
 from .fczenit import FczenitIE
 from .filmon import (
@@ -416,7 +428,6 @@ from .frontendmasters import (
     FrontendMastersLessonIE,
     FrontendMastersCourseIE
 )
-from .fujitv import FujiTVFODPlus7IE
 from .funimation import FunimationIE
 from .funk import FunkIE
 from .fusion import FusionIE
@@ -452,6 +463,7 @@ from .groupon import GrouponIE
 from .hbo import HBOIE
 from .hearthisat import HearThisAtIE
 from .heise import HeiseIE
+from .hellonewdream import HelloNewDreamIE
 from .hellporno import HellPornoIE
 from .helsinki import HelsinkiIE
 from .hentaistigma import HentaiStigmaIE
@@ -480,6 +492,10 @@ from .hungama import (
     HungamaSongIE,
 )
 from .hypem import HypemIE
+from .ichinanalive import (
+    IchinanaLiveIE,
+    IchinanaLiveClipIE,
+)
 from .ign import (
     IGNIE,
     IGNVideoIE,
@@ -521,8 +537,13 @@ from .ivi import (
     IviCompilationIE
 )
 from .ivideon import IvideonIE
-from .iwara import IwaraIE
+from .iwara import (
+    IwaraIE,
+    IwaraUserIE,
+    IwaraUser2IE,
+)
 from .izlesene import IzleseneIE
+from .javhub import JavhubIE
 from .jamendo import (
     JamendoIE,
     JamendoAlbumIE,
@@ -645,6 +666,11 @@ from .markiza import (
     MarkizaPageIE,
 )
 from .massengeschmacktv import MassengeschmackTVIE
+from .mastodon import (
+    MastodonIE,
+    MastodonUserIE,
+    MastodonUserNumericIE,
+)
 from .matchtv import MatchTVIE
 from .mdr import MDRIE
 from .medaltv import MedalTVIE
@@ -668,6 +694,11 @@ from .microsoftvirtualacademy import (
     MicrosoftVirtualAcademyIE,
     MicrosoftVirtualAcademyCourseIE,
 )
+from .mildom import (
+    MildomIE,
+    MildomVodIE,
+    MildomUserVodIE,
+)
 from .minds import (
     MindsIE,
     MindsChannelIE,
@@ -676,6 +707,10 @@ from .minds import (
 from .ministrygrid import MinistryGridIE
 from .minoto import MinotoIE
 from .miomio import MioMioIE
+from .mirrativ import (
+    MirrativIE,
+    MirrativUserIE,
+)
 from .mit import TechTVMITIE, OCWMITIE
 from .mitele import MiTeleIE
 from .mixcloud import (
@@ -793,7 +828,14 @@ from .nick import (
     NickNightIE,
     NickRuIE,
 )
-from .niconico import NiconicoIE, NiconicoPlaylistIE
+from .niconico import (
+    NiconicoIE,
+    NiconicoPlaylistIE,
+    NiconicoUserIE,
+    NiconicoSeriesIE,
+    NiconicoLiveIE,
+)
+from .niconico_smile import NiconicoSmileIE
 from .ninecninemedia import NineCNineMediaIE
 from .ninegag import NineGagIE
 from .ninenow import NineNowIE
@@ -864,6 +906,10 @@ from .ooyala import (
     OoyalaIE,
     OoyalaExternalIE,
 )
+from .openrec import (
+    OpenRecIE,
+    OpenRecCaptureIE,
+)
 from .ora import OraTVIE
 from .orf import (
     ORFTVthekIE,
@@ -913,6 +959,10 @@ from .picarto import (
 )
 from .piksel import PikselIE
 from .pinkbike import PinkbikeIE
+from .pixivsketch import (
+    PixivSketchIE,
+    PixivSketchUserIE,
+)
 from .pinterest import (
     PinterestIE,
     PinterestCollectionIE,
@@ -949,6 +999,7 @@ from .pornhub import (
     PornHubUserIE,
     PornHubPagedVideoListIE,
     PornHubUserVideosUploadIE,
+    PornHubUserLiveIE,
 )
 from .pornotube import PornotubeIE
 from .pornovoisines import PornoVoisinesIE
@@ -961,6 +1012,12 @@ from .presstv import PressTVIE
 from .prosiebensat1 import ProSiebenSat1IE
 from .puls4 import Puls4IE
 from .pyvideo import PyvideoIE
+from .qawebsites import (
+    AskfmIE,
+    MarshmallowQAIE,
+    MottohometeIE,
+    PeingIE,
+)
 from .qqmusic import (
     QQMusicIE,
     QQMusicSingerIE,
@@ -972,6 +1029,7 @@ from .r7 import (
     R7IE,
     R7ArticleIE,
 )
+from .radiko import RadikoIE
 from .radiocanada import (
     RadioCanadaIE,
     RadioCanadaAudioVideoIE,
@@ -1082,6 +1140,7 @@ from .shared import (
     SharedIE,
     VivoIE,
 )
+from .sharevideos import ShareVideosIE
 from .showroomlive import ShowRoomLiveIE
 from .simplecast import (
     SimplecastIE,
@@ -1255,6 +1314,7 @@ from .tiktok import (
     TikTokUserIE,
 )
 from .tinypic import TinyPicIE
+from .tktube import TktubeIE
 from .tmz import (
     TMZIE,
     TMZArticleIE,
@@ -1269,6 +1329,13 @@ from .toggle import (
     ToggleIE,
     MeWatchIE,
 )
+from .tokyomotion import (
+    TokyoMotionIE,
+    TokyoMotionUserFavsIE,
+    TokyoMotionUserIE,
+    TokyoMotionSearchesIE,
+    TokyoMotionScannerIE,
+)
 from .tonline import TOnlineIE
 from .toongoggles import ToonGogglesIE
 from .toutv import TouTvIE
@@ -1350,7 +1417,10 @@ from .tweakers import TweakersIE
 from .twentyfourvideo import TwentyFourVideoIE
 from .twentymin import TwentyMinutenIE
 from .twentythreevideo import TwentyThreeVideoIE
-from .twitcasting import TwitCastingIE
+from .twitcasting import (
+    TwitCastingIE,
+    TwitCastingUserIE,
+)
 from .twitch import (
     TwitchVodIE,
     TwitchCollectionIE,
@@ -1421,6 +1491,7 @@ from .vice import (
 from .vidbit import VidbitIE
 from .viddler import ViddlerIE
 from .videa import VideaIE
+from .videobin import VideobinIE
 from .videodetective import VideoDetectiveIE
 from .videofyme import VideofyMeIE
 from .videomore import (
@@ -1483,6 +1554,10 @@ from .vodlocker import VodlockerIE
 from .vodpl import VODPlIE
 from .vodplatform import VODPlatformIE
 from .voicerepublic import VoiceRepublicIE
+from .voicy import (
+    VoicyIE,
+    VoicyChannelIE,
+)
 from .voot import VootIE
 from .voxmedia import (
     VoxMediaVolumeIE,
@@ -1533,6 +1608,7 @@ from .weibo import (
     WeiboMobileIE
 )
 from .weiqitv import WeiqiTVIE
+from .whowatch import WhoWatchIE
 from .wistia import (
     WistiaIE,
     WistiaPlaylistIE,
diff --git a/youtube_dl/extractor/eyny.py b/youtube_dl/extractor/eyny.py
new file mode 100644
index 000000000..d8a7263df
--- /dev/null
+++ b/youtube_dl/extractor/eyny.py
@@ -0,0 +1,38 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import (
+    ExtractorError,
+)
+
+
+class EynyBaseIE(InfoExtractor):
+    IE_DESC = False  # Do not list
+
+
+class EynyIE(EynyBaseIE):
+    IE_NAME = 'eyny'
+    _VALID_URL = r'https?:\/\/(?:www\.)?eyny\.com\/\d+\/watch\?v=(?P<id>[^#?&]+)(?:&[^&]+)*'
+    _TEST = {}
+    _TITLE_REGEX = re.compile(r'(?ms)<title>(.+) -  Free Videos \& Sex Movies - XXX Tube - EYNY<\/title>')
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage(url, video_id)
+
+        title = self._TITLE_REGEX.search(webpage).group(1)
+
+        entries = self._parse_html5_media_entries(url, webpage, video_id, m3u8_id='hls')
+        if not entries:
+            raise ExtractorError('Private video', expected=True)
+        entry = entries[0]
+        self._sort_formats(entry['formats'])
+        entry.update({
+            'id': video_id,
+            'title': title,
+            'age_limit': 18,
+        })
+        return entry
diff --git a/youtube_dl/extractor/fc2.py b/youtube_dl/extractor/fc2.py
index 435561147..c0179d8b7 100644
--- a/youtube_dl/extractor/fc2.py
+++ b/youtube_dl/extractor/fc2.py
@@ -1,49 +1,29 @@
 # coding: utf-8
-from __future__ import unicode_literals
+from __future__ import unicode_literals, with_statement
 
 import hashlib
 import re
+import itertools
 
 from .common import InfoExtractor
 from ..compat import (
     compat_parse_qs,
     compat_urllib_request,
     compat_urlparse,
+    compat_str,
 )
 from ..utils import (
     ExtractorError,
-    sanitized_Request,
+    sanitized_Request, try_get,
     urlencode_postdata,
+    to_str,
+    std_headers,
 )
+from ..websocket import WebSocket
 
 
-class FC2IE(InfoExtractor):
-    _VALID_URL = r'^(?:https?://video\.fc2\.com/(?:[^/]+/)*content/|fc2:)(?P<id>[^/]+)'
-    IE_NAME = 'fc2'
+class FC2BaseIE(InfoExtractor):
     _NETRC_MACHINE = 'fc2'
-    _TESTS = [{
-        'url': 'http://video.fc2.com/en/content/20121103kUan1KHs',
-        'md5': 'a6ebe8ebe0396518689d963774a54eb7',
-        'info_dict': {
-            'id': '20121103kUan1KHs',
-            'ext': 'flv',
-            'title': 'Boxing again with Puff',
-        },
-    }, {
-        'url': 'http://video.fc2.com/en/content/20150125cEva0hDn/',
-        'info_dict': {
-            'id': '20150125cEva0hDn',
-            'ext': 'mp4',
-        },
-        'params': {
-            'username': 'ytdl@yt-dl.org',
-            'password': '(snip)',
-        },
-        'skip': 'requires actual password',
-    }, {
-        'url': 'http://video.fc2.com/en/a/content/20130926eZpARwsF',
-        'only_matching': True,
-    }]
 
     def _login(self):
         username, password = self._get_login_info()
@@ -63,17 +43,44 @@ class FC2IE(InfoExtractor):
             'https://secure.id.fc2.com/index.php?mode=login&switch_language=en', login_data)
 
         login_results = self._download_webpage(request, None, note='Logging in', errnote='Unable to log in')
-        if 'mode=redirect&login=done' not in login_results:
+        if 'login=done' not in login_results:
             self.report_warning('unable to log in: bad username or password')
             return False
 
         # this is also needed
-        login_redir = sanitized_Request('http://id.fc2.com/?mode=redirect&login=done')
+        login_redir = sanitized_Request('http://secure.id.fc2.com/?login=done')
         self._download_webpage(
             login_redir, None, note='Login redirect', errnote='Login redirect failed')
 
         return True
 
+
+class FC2IE(FC2BaseIE):
+    _VALID_URL = r'^(?:https?://video\.fc2\.com/(?:[^/]+/)*content/|fc2:)(?P<id>[^/]+)'
+    IE_NAME = 'fc2'
+    _TESTS = [{
+        'url': 'http://video.fc2.com/en/content/20121103kUan1KHs',
+        'info_dict': {
+            'id': '20121103kUan1KHs',
+            'ext': 'flv',
+            'title': 'Boxing again with Puff',
+        },
+    }, {
+        'url': 'http://video.fc2.com/en/content/20150125cEva0hDn/',
+        'info_dict': {
+            'id': '20150125cEva0hDn',
+            'ext': 'mp4',
+        },
+        'params': {
+            'username': 'ytdl@yt-dl.org',
+            'password': '(snip)',
+        },
+        'skip': 'requires actual password',
+    }, {
+        'url': 'http://video.fc2.com/en/a/content/20130926eZpARwsF',
+        'only_matching': True,
+    }]
+
     def _real_extract(self, url):
         video_id = self._match_id(url)
         self._login()
@@ -83,41 +90,68 @@ class FC2IE(InfoExtractor):
             self._downloader.cookiejar.clear_session_cookies()  # must clear
             self._login()
 
-        title = 'FC2 video %s' % video_id
+        title = None
         thumbnail = None
         if webpage is not None:
-            title = self._og_search_title(webpage)
+            title = self._search_regex(
+                r'<h2\s+(?:[a-zA-Z_-]+="[^"]+"\s+)*class="videoCnt_title"(?:[a-zA-Z_-]+="[^"]+"\s+)*>([^<]+)</h2>', webpage, 'Extracting title', video_id)
             thumbnail = self._og_search_thumbnail(webpage)
         refer = url.replace('/content/', '/a/content/') if '/a/content/' not in url else url
 
         mimi = hashlib.md5((video_id + '_gGddgPfeaf_gzyr').encode('utf-8')).hexdigest()
 
+        formats = []
+
         info_url = (
             'http://video.fc2.com/ginfo.php?mimi={1:s}&href={2:s}&v={0:s}&fversion=WIN%2011%2C6%2C602%2C180&from=2&otag=0&upid={0:s}&tk=null&'.
             format(video_id, mimi, compat_urllib_request.quote(refer, safe=b'').replace('.', '%2E')))
 
         info_webpage = self._download_webpage(
-            info_url, video_id, note='Downloading info page')
+            info_url, video_id, note='Downloading flv info page')
         info = compat_urlparse.parse_qs(info_webpage)
 
-        if 'err_code' in info:
-            # most of the time we can still download wideo even if err_code is 403 or 602
-            self.report_warning(
-                'Error code was: %s... but still trying' % info['err_code'][0])
+        mid = try_get(info, (lambda x: x['mid'][0], lambda x: x['amp;mid'][0]), compat_str)
+        if 'err_code' not in info and 'filepath' in info and mid:
+            # flv download is not available if err_code is present
+            video_url = info['filepath'][0] + '?mid=' + mid
+            formats.append({
+                'format_id': 'flv',
+                'url': video_url,
+                'ext': 'flv',
+                'protocol': 'http',
+            })
 
-        if 'filepath' not in info:
-            raise ExtractorError('Cannot download file. Are you logged in?')
-
-        video_url = info['filepath'][0] + '?mid=' + info['mid'][0]
         title_info = info.get('title')
         if title_info:
             title = title_info[0]
 
+        info_data = self._download_json(
+            'https://video.fc2.com/api/v3/videoplaylist/%s?sh=1&fs=0' % video_id, video_id,
+            note='Downloading m3u8 playlist info')
+        playlists = info_data.get('playlist') or {}
+        for (name, m3u8_url) in playlists.items():
+            # m3u8_url may be either HLS playlist or direct MP4 download.
+            # since ffmpeg accepts both,
+            #  we use m3u8 rather than http or m3u8_native
+            formats.append({
+                'format_id': 'hls-%s' % name,
+                'url': 'https://video.fc2.com%s' % m3u8_url,
+                'ext': 'mp4',
+                'protocol': 'ffmpeg',
+            })
+
+        if not formats:
+            raise ExtractorError('Cannot download file. Are you logged in?')
+
+        if not title:
+            title = 'FC2 video %s' % video_id
+
+        self._sort_formats(formats)
+
         return {
             'id': video_id,
             'title': title,
-            'url': video_url,
-            'ext': 'flv',
+            'formats': formats,
             'thumbnail': thumbnail,
         }
 
@@ -128,7 +162,6 @@ class FC2EmbedIE(InfoExtractor):
 
     _TEST = {
         'url': 'http://video.fc2.com/flv2.swf?t=201404182936758512407645&i=20130316kwishtfitaknmcgd76kjd864hso93htfjcnaogz629mcgfs6rbfk0hsycma7shkf85937cbchfygd74&i=201403223kCqB3Ez&d=2625&sj=11&lang=ja&rel=1&from=11&cmt=1&tk=TlRBM09EQTNNekU9&tl=„Éó„É™„Ç∫„É≥ÔΩ•„Éñ„É¨„Ç§„ÇØ%20S1-01%20„Éû„Ç§„Ç±„É´%20„ÄêÂêπÊõø„Äë',
-        'md5': 'b8aae5334cb691bdb1193a88a6ab5d5a',
         'info_dict': {
             'id': '201403223kCqB3Ez',
             'ext': 'flv',
@@ -158,3 +191,146 @@ class FC2EmbedIE(InfoExtractor):
             'title': title,
             'thumbnail': thumbnail,
         }
+
+
+class FC2UserIE(FC2BaseIE):
+    _VALID_URL = r'^https?://video\.fc2\.com/(?P<extra>(?:[^/]+/)*)account/(?P<id>\d+)'
+    IE_NAME = 'fc2:user'
+
+    def _real_extract(self, url):
+        mobj = re.match(self._VALID_URL, url)
+        extra = mobj.group('extra') or ''
+        user_id = mobj.group('id')
+        self._login()
+
+        results = []
+        uploader_name = None
+        for page in itertools.count(1):
+            webpage = self._download_webpage(
+                'https://video.fc2.com/%saccount/%s/content?page=%d' % (extra, user_id, page), user_id,
+                note='Downloading page %d' % page)
+            uploader_name = uploader_name or self._search_regex(r'<span\s+class="memberName">(.+?)</span>', webpage, 'uploader name', fatal=False, group=1)
+            videos = [self.url_result(x.group(1)) for x in re.finditer(r'<a\s+href="(https://video\.fc2\.com/(?:[^/]+/)*content/\d{8}[a-zA-Z0-9]+)"\s*class="c-boxList-111_video_ttl"', webpage)]
+            if not videos:
+                break
+            results.extend(videos)
+
+        return self.playlist_result(results, user_id, uploader_name)
+
+
+class FC2LiveIE(InfoExtractor):
+    _VALID_URL = r'^https?://live\.fc2\.com/(?P<id>\d+)'
+    IE_NAME = 'fc2:live'
+    _WORKING = False
+    _FEATURE_DEPENDENCY = ('websocket', )
+
+    # TODO: split this extractor into separate file
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage('https://live.fc2.com/%s/' % video_id, video_id)
+
+        # self._download_json(
+        #     'https://live.fc2.com/api/userInfo.php', video_id, note='Downloading userInfo.php',
+        #     data=b'', headers={'X-Requested-With': 'XMLHttpRequest'})
+        # self._download_json(
+        #     'https://live.fc2.com/contents/userlist.php', video_id, note='Downloading userlist.php',
+        #     data=('page=0&channel=%s' % video_id).encode('utf-8'), headers={'X-Requested-With': 'XMLHttpRequest'})
+        # self._download_json(
+        #     'https://live.fc2.com/api/memberApi.php', video_id, note='Downloading memberApi.php',
+        #     data=('channel=1&profile=1&user=1&streamid=%s' % video_id).encode('utf-8'), headers={'X-Requested-With': 'XMLHttpRequest'})
+        # self._download_json(
+        #     'https://live.fc2.com/api/favoriteManager.php', video_id, note='Downloading favoriteManager.php',
+        #     data=('id=%s&mode=check&token=' % video_id).encode('utf-8'), headers={'X-Requested-With': 'XMLHttpRequest'})
+        # self._download_json(
+        #     'https://live.fc2.com/api/profileInfo.php', video_id, note='Downloading profileInfo.php',
+        #     data=('mode=get&userid=%s' % video_id).encode('utf-8'), headers={'X-Requested-With': 'XMLHttpRequest'})
+        # self._download_json(
+        #     'https://live.fc2.com/api/videoRecList.php', video_id, note='Downloading videoRecList.php',
+        #     data=('userid=%s&isadult=0&publish=0&per_page=10&page=1' % video_id).encode('utf-8'), headers={'X-Requested-With': 'XMLHttpRequest'})
+
+        post_dict = {
+            'channel_id': video_id,
+            'mode': 'play',
+            'orz': '',
+            'channel_version': '33fe385c-5eed-4e15-bb04-a6b5ae438d8e',
+            'client_version': '2.0.0\n [1]',
+            'client_type': 'pc',
+            'client_app': 'browser_hls',
+            'ipv6': '',
+        }
+        self._set_cookie('live.fc2.com', 'js-player_size', '1')
+
+        # https://live.fc2.com/js/playerVersion/version.txt?0.0674203108942784
+
+        control_server = self._download_json(
+            'https://live.fc2.com/api/getControlServer.php', video_id, note='Downloading ControlServer data',
+            data=urlencode_postdata(post_dict), headers={'X-Requested-With': 'XMLHttpRequest'})
+        post_dict['orz'] = control_server['orz']
+        self._set_cookie('live.fc2.com', 'l_ortkn', control_server['orz_raw'])
+
+        ws_url = control_server['url'] + '?control_token=' + control_server['control_token']
+        playlist_data = None
+
+        self.to_screen('%s: Fetching HLS playlist info via WebSocket' % video_id)
+        with WebSocket(ws_url, {
+            'Cookie': str(self._get_cookies('https://live.fc2.com/'))[12:],
+            'Origin': 'https://live.fc2.com',
+            'Accept': '*/*',
+            'User-Agent': std_headers['User-Agent'],
+        }) as ws:
+            if self._downloader.params.get('verbose', False):
+                self.to_screen('[debug] Sending HLS server request')
+            ws.send(r'{"name":"get_hls_information","arguments":{},"id":1}')
+
+            while True:
+                recv = to_str(ws.recv()).strip()
+                if not recv:
+                    continue
+                data = self._parse_json(recv, video_id, fatal=False)
+                if not data or not isinstance(data, dict):
+                    continue
+                if data.get('name') == '_response_' and data.get('id') == 1:
+                    if self._downloader.params.get('verbose', False):
+                        self.to_screen('[debug] Goodbye.')
+                    playlist_data = data
+                    break
+                elif self._downloader.params.get('verbose', False):
+                    if len(recv) > 100:
+                        recv = recv[:100] + '...'
+                    self.to_screen('[debug] Server said: %s' % recv)
+
+        if not playlist_data:
+            raise ExtractorError('Unable to fetch HLS playlist info via WebSocket')
+        arguments = playlist_data['arguments']
+        playlists = arguments['playlists']
+        hls_url = None
+
+        for pl in playlists:
+            if pl.get('status') == 0 and 'master_playlist' in pl.get('url'):
+                hls_url = pl.get('url')
+                break
+
+        if not hls_url:
+            raise ExtractorError('Unable to find HLS playlist')
+
+        title = try_get(
+            webpage,
+            (lambda x: self._html_search_meta(('og:title', 'twitter:title'), x, 'live title', fatal=False).strip('\u3000 ')[1:-1],
+             lambda x: self._search_regex((r'<title>[\u3000:space:]*\[(.+?)\]\s*-\s*[^-]+?</title>', r'<p class="c-ctbName">(.+?)</p>'), x, 'live title'),
+             ), compat_str)
+        description = self._html_search_meta(
+            ('description', 'og:description', 'twitter:description'),
+            webpage, 'live description', fatal=False)
+        formats = self._extract_m3u8_formats(
+            hls_url, video_id, ext='mp4', m3u8_id='hls', live=True,
+            headers={
+                'Origin': 'https://live.fc2.com',
+                'Referer': url,
+            })
+
+        return {
+            'id': video_id,
+            'title': title,
+            'description': description,
+            'formats': formats,
+        }
diff --git a/youtube_dl/extractor/fujitv.py b/youtube_dl/extractor/fujitv.py
deleted file mode 100644
index a02a94374..000000000
--- a/youtube_dl/extractor/fujitv.py
+++ /dev/null
@@ -1,35 +0,0 @@
-# coding: utf-8
-from __future__ import unicode_literals
-
-from .common import InfoExtractor
-
-
-class FujiTVFODPlus7IE(InfoExtractor):
-    _VALID_URL = r'https?://i\.fod\.fujitv\.co\.jp/plus7/web/[0-9a-z]{4}/(?P<id>[0-9a-z]+)'
-    _BASE_URL = 'http://i.fod.fujitv.co.jp/'
-    _BITRATE_MAP = {
-        300: (320, 180),
-        800: (640, 360),
-        1200: (1280, 720),
-        2000: (1280, 720),
-    }
-
-    def _real_extract(self, url):
-        video_id = self._match_id(url)
-        formats = self._extract_m3u8_formats(
-            self._BASE_URL + 'abr/pc_html5/%s.m3u8' % video_id, video_id, 'mp4')
-        for f in formats:
-            wh = self._BITRATE_MAP.get(f.get('tbr'))
-            if wh:
-                f.update({
-                    'width': wh[0],
-                    'height': wh[1],
-                })
-        self._sort_formats(formats)
-
-        return {
-            'id': video_id,
-            'title': video_id,
-            'formats': formats,
-            'thumbnail': self._BASE_URL + 'pc/image/wbtn/wbtn_%s.jpg' % video_id,
-        }
diff --git a/youtube_dl/extractor/generic.py b/youtube_dl/extractor/generic.py
index 87594534f..15755b12d 100644
--- a/youtube_dl/extractor/generic.py
+++ b/youtube_dl/extractor/generic.py
@@ -14,6 +14,7 @@ from ..compat import (
     compat_urllib_parse_unquote,
     compat_urlparse,
     compat_xml_parse_error,
+    compat_parse_qs,
 )
 from ..utils import (
     determine_ext,
@@ -30,6 +31,7 @@ from ..utils import (
     parse_duration,
     sanitized_Request,
     smuggle_url,
+    try_get,
     unescapeHTML,
     unified_timestamp,
     unsmuggle_url,
@@ -130,6 +132,11 @@ from .vk import VKIE
 from .kinja import KinjaEmbedIE
 from .arcpublishing import ArcPublishingIE
 from .medialaan import MedialaanIE
+from .mastodon import (
+    MastodonIE,
+    MastodonUserIE,
+    MastodonUserNumericIE,
+)
 from .simplecast import SimplecastIE
 
 
@@ -2100,16 +2107,6 @@ class GenericIE(InfoExtractor):
                 'skip_download': True,
             },
         },
-        {
-            'url': 'http://share-videos.se/auto/video/83645793?uid=13',
-            'md5': 'b68d276de422ab07ee1d49388103f457',
-            'info_dict': {
-                'id': '83645793',
-                'title': 'Lock up and get excited',
-                'ext': 'mp4'
-            },
-            'skip': 'TODO: fix nested playlists processing in tests',
-        },
         {
             # Viqeo embeds
             'url': 'https://viqeo.tv/',
@@ -2240,6 +2237,48 @@ class GenericIE(InfoExtractor):
                 'duration': 159,
             },
         },
+        {
+            # Firebase Dynamic Link
+            'url': 'https://mirrativ.page.link/?link=https%3A%2F%2Fwww.mirrativ.com%2Flive%2FD8y_lZXb7tb5gdOARr7Zpw&apn=com.dena.mirrativ&ibi=com.dena.mirrativ&isi=1028944599&ius=mirrativ&st=%E3%81%9D%E3%82%89%E3%82%8A%E3%81%99&sd=%E9%9F%B3%E3%82%B2%E3%83%BC&si=https%3A%2F%2Fcdn.mirrativ.com%2Fmirrorman-prod%2Fimage%2Fuser_profile%2F31f1a71beb12cd22f515897993e549ccaa9f8c33084ce28beff58f5fad222555_share.jpeg',
+            'info_dict': {},
+            'add_ie': ['Mirrativ'],
+        },
+        {
+            # 5ch.net redirect
+            'url': 'https://jump.5ch.net?https://www.youtube.com/watch?v=XCmzSSlZQ0w',
+            'info_dict': {},
+            'add_ie': ['Youtube'],
+        },
+        {
+            # URL prefixed by view-source:
+            'url': 'view-source:https://www.youtube.com/watch?v=XCmzSSlZQ0w',
+            'info_dict': {},
+            'add_ie': ['Youtube'],
+        },
+        {
+            # URL starting with invalid (but fixable) scheme
+            'url': 'tps://www.youtube.com/watch?v=MVpMUgKtds4',
+            'info_dict': {},
+            'add_ie': ['Youtube'],
+        },
+        {
+            # another URL starting with invalid (but fixable) scheme
+            'url': 'ttp://www.youtube.com/watch?v=MVpMUgKtds4',
+            'info_dict': {},
+            'add_ie': ['Youtube'],
+        },
+        {
+            # Pixiv jump URL (1)
+            'url': 'https://www.pixiv.net/jump.php?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DMVpMUgKtds4',
+            'info_dict': {},
+            'add_ie': ['Youtube'],
+        },
+        {
+            # Pixiv jump URL (2)
+            'url': 'https://www.pixiv.net/jump.php?https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DMVpMUgKtds4',
+            'info_dict': {},
+            'add_ie': ['Youtube'],
+        },
         {
             # Simplecast player embed
             'url': 'https://www.bio.org/podcast',
@@ -2256,6 +2295,16 @@ class GenericIE(InfoExtractor):
         },
     ]
 
+    _CORRUPTED_SCHEME_CONVERSION_TABLE = {
+        # HTTP protocol
+        'htp': 'http',
+        'htps': 'https',
+        'ttp': 'http',
+        'ttps': 'https',
+        'tp': 'http',
+        'tps': 'https',
+    }
+
     def report_following_redirect(self, new_url):
         """Report information extraction."""
         self._downloader.to_screen('[redirect] Following redirect to %s' % new_url)
@@ -2361,6 +2410,10 @@ class GenericIE(InfoExtractor):
     def _real_extract(self, url):
         if url.startswith('//'):
             return self.url_result(self.http_scheme() + url)
+        mobj = re.match(r'^view-source:(.+)$', url)
+        if mobj:
+            self._downloader.report_warning('URL is pasted with "view-source:" appended')
+            return self.url_result(mobj.group(1))
 
         parsed_url = compat_urlparse.urlparse(url)
         if not parsed_url.scheme:
@@ -2392,6 +2445,41 @@ class GenericIE(InfoExtractor):
                 if ':' not in default_search:
                     default_search += ':'
                 return self.url_result(default_search + url)
+        elif parsed_url.scheme in self._CORRUPTED_SCHEME_CONVERSION_TABLE:
+            new_scheme = self._CORRUPTED_SCHEME_CONVERSION_TABLE[parsed_url.scheme]
+            self.report_warning('scheme seems corrupted, correcting to %s' % new_scheme)
+            fixed_urlp = parsed_url._replace(scheme=new_scheme)
+            fixed_url = compat_urlparse.urlunparse(fixed_urlp)
+            return self.url_result(fixed_url)
+
+        host = parsed_url.netloc
+        path = parsed_url.path
+        if ':' in host:
+            host = host[:host.index(':')]
+        if host[:4] == 'www.':
+            host = host[4:]
+        # japan BBS redirect
+        if host in ('pinktower.com', 'jump.5ch.net', 'jump.megabbs.info'):
+            return self.url_result(parsed_url.query)
+        # Pixiv redirect (usually requires referer to jump)
+        if host in ('pixiv.net', 'www.pixiv.net') and path == '/jump.php':
+            # Following URLs are valid and acceptable:
+            # 1. https://www.pixiv.net/jump.php?url=URL_HERE (found in profile page)
+            # 2. https://www.pixiv.net/jump.php?URL_HERE (found in description in artwork page)
+            # I think I previously saw Pixiv jump URLs with mutation, but I can't find it anymore
+            link = try_get(
+                parsed_url.query,
+                (lambda x: compat_parse_qs(x)['url'][0],
+                 lambda x: compat_urllib_parse_unquote(x)),
+                compat_str)
+            if link:
+                return self.url_result(link)
+        # Firebase Dynamic Link
+        # https://firebase.google.com/docs/dynamic-links/create-manually
+        if host.endswith('.page.link'):
+            link = try_get(compat_parse_qs(parsed_url.query), lambda qs: qs['link'][0], compat_str)
+            if link:
+                return self.url_result(link)
 
         url, smuggled_data = unsmuggle_url(url)
         force_videoid = None
@@ -2402,6 +2490,24 @@ class GenericIE(InfoExtractor):
         else:
             video_id = self._generic_id(url)
 
+        # Test if the URL is for Mastodon instance
+
+        if self._downloader.params.get('check_mastodon_instance', False):
+            prefix = self._search_regex(
+                (MastodonIE._VALID_URL,
+                 MastodonUserIE._VALID_URL,
+                 MastodonUserNumericIE._VALID_URL),
+                url, 'mastdon test', group='prefix', default=None)
+            if MastodonIE._test_mastodon_instance(self, parsed_url.hostname, False, prefix):
+                return self.url_result(url)
+
+        if self._downloader.params.get('check_peertube_instance', False):
+            prefix = self._search_regex(
+                PeerTubeIE._VALID_URL,
+                url, 'peertube test', group='prefix', default=None)
+            if PeerTubeIE._test_peertube_instance(self, parsed_url.hostname, False, prefix):
+                return self.url_result(url)
+
         self.to_screen('%s: Requesting header' % video_id)
 
         head_req = HEADRequest(url)
@@ -2556,9 +2662,8 @@ class GenericIE(InfoExtractor):
         #   Video Title - Tagline | Site Name
         # and so on and so forth; it's just not practical
         video_title = self._og_search_title(
-            webpage, default=None) or self._html_search_regex(
-            r'(?s)<title>(.*?)</title>', webpage, 'video title',
-            default='video')
+            webpage, default=None) or self._html_extract_title(
+            webpage, 'video title', default='video')
 
         # Try to detect age limit automatically
         age_limit = self._rta_search(webpage)
@@ -2584,6 +2689,8 @@ class GenericIE(InfoExtractor):
             'age_limit': age_limit,
         })
 
+        waitlist = []
+
         # Look for Brightcove Legacy Studio embeds
         bc_urls = BrightcoveLegacyIE._extract_brightcove_urls(webpage)
         if bc_urls:
@@ -2593,35 +2700,35 @@ class GenericIE(InfoExtractor):
                 'ie_key': 'BrightcoveLegacy'
             } for bc_url in bc_urls]
 
-            return {
+            waitlist.append({
                 '_type': 'playlist',
                 'title': video_title,
                 'id': video_id,
                 'entries': entries,
-            }
+            })
 
         # Look for Brightcove New Studio embeds
         bc_urls = BrightcoveNewIE._extract_urls(self, webpage)
         if bc_urls:
-            return self.playlist_from_matches(
+            waitlist.append(self.playlist_from_matches(
                 bc_urls, video_id, video_title,
                 getter=lambda x: smuggle_url(x, {'referrer': url}),
-                ie='BrightcoveNew')
+                ie='BrightcoveNew'))
 
         # Look for Nexx embeds
         nexx_urls = NexxIE._extract_urls(webpage)
         if nexx_urls:
-            return self.playlist_from_matches(nexx_urls, video_id, video_title, ie=NexxIE.ie_key())
+            waitlist.append(self.playlist_from_matches(nexx_urls, video_id, video_title, ie=NexxIE.ie_key()))
 
         # Look for Nexx iFrame embeds
         nexx_embed_urls = NexxEmbedIE._extract_urls(webpage)
         if nexx_embed_urls:
-            return self.playlist_from_matches(nexx_embed_urls, video_id, video_title, ie=NexxEmbedIE.ie_key())
+            waitlist.append(self.playlist_from_matches(nexx_embed_urls, video_id, video_title, ie=NexxEmbedIE.ie_key()))
 
         # Look for ThePlatform embeds
         tp_urls = ThePlatformIE._extract_urls(webpage)
         if tp_urls:
-            return self.playlist_from_matches(tp_urls, video_id, video_title, ie='ThePlatform')
+            waitlist.append(self.playlist_from_matches(tp_urls, video_id, video_title, ie='ThePlatform'))
 
         arc_urls = ArcPublishingIE._extract_urls(webpage)
         if arc_urls:
@@ -2637,11 +2744,11 @@ class GenericIE(InfoExtractor):
             r'<iframe[^>]+?src="((?:https?:)?//(?:(?:www|static)\.)?rtl\.nl/(?:system/videoplayer/[^"]+(?:video_)?)?embed[^"]+)"',
             webpage)
         if matches:
-            return self.playlist_from_matches(matches, video_id, video_title, ie='RtlNl')
+            waitlist.append(self.playlist_from_matches(matches, video_id, video_title, ie='RtlNl'))
 
         vimeo_urls = VimeoIE._extract_urls(url, webpage)
         if vimeo_urls:
-            return self.playlist_from_matches(vimeo_urls, video_id, video_title, ie=VimeoIE.ie_key())
+            waitlist.append(self.playlist_from_matches(vimeo_urls, video_id, video_title, ie=VimeoIE.ie_key()))
 
         vhx_url = VHXEmbedIE._extract_url(webpage)
         if vhx_url:
@@ -2651,17 +2758,17 @@ class GenericIE(InfoExtractor):
             r'src=[\'"](https?://vid\.me/[^\'"]+)[\'"]',
             webpage, 'vid.me embed', default=None)
         if vid_me_embed_url is not None:
-            return self.url_result(vid_me_embed_url, 'Vidme')
+            waitlist.append(self.url_result(vid_me_embed_url, 'Vidme'))
 
         # Look for YouTube embeds
         youtube_urls = YoutubeIE._extract_urls(webpage)
         if youtube_urls:
-            return self.playlist_from_matches(
-                youtube_urls, video_id, video_title, ie=YoutubeIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                youtube_urls, video_id, video_title, ie=YoutubeIE.ie_key()))
 
         matches = DailymotionIE._extract_urls(webpage)
         if matches:
-            return self.playlist_from_matches(matches, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(matches, video_id, video_title))
 
         # Look for embedded Dailymotion playlist player (#3822)
         m = re.search(
@@ -2670,19 +2777,19 @@ class GenericIE(InfoExtractor):
             playlists = re.findall(
                 r'list\[\]=/playlist/([^/]+)/', unescapeHTML(m.group('url')))
             if playlists:
-                return self.playlist_from_matches(
-                    playlists, video_id, video_title, lambda p: '//dailymotion.com/playlist/%s' % p)
+                waitlist.append(self.playlist_from_matches(
+                    playlists, video_id, video_title, lambda p: '//dailymotion.com/playlist/%s' % p))
 
         # Look for DailyMail embeds
         dailymail_urls = DailyMailIE._extract_urls(webpage)
         if dailymail_urls:
-            return self.playlist_from_matches(
-                dailymail_urls, video_id, video_title, ie=DailyMailIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                dailymail_urls, video_id, video_title, ie=DailyMailIE.ie_key()))
 
         # Look for Teachable embeds, must be before Wistia
         teachable_url = TeachableIE._extract_url(webpage, url)
         if teachable_url:
-            return self.url_result(teachable_url)
+            waitlist.append(self.url_result(teachable_url))
 
         # Look for embedded Wistia player
         wistia_urls = WistiaIE._extract_urls(webpage)
@@ -2693,45 +2800,45 @@ class GenericIE(InfoExtractor):
                     '_type': 'url_transparent',
                     'uploader': video_uploader,
                 })
-            return playlist
+            waitlist.append(playlist)
 
         # Look for SVT player
         svt_url = SVTIE._extract_url(webpage)
         if svt_url:
-            return self.url_result(svt_url, 'SVT')
+            waitlist.append(self.url_result(svt_url, 'SVT'))
 
         # Look for Bandcamp pages with custom domain
         mobj = re.search(r'<meta property="og:url"[^>]*?content="(.*?bandcamp\.com.*?)"', webpage)
         if mobj is not None:
             burl = unescapeHTML(mobj.group(1))
             # Don't set the extractor because it can be a track url or an album
-            return self.url_result(burl)
+            waitlist.append(self.url_result(burl))
 
         # Look for embedded Vevo player
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>(?:https?:)?//(?:cache\.)?vevo\.com/.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
 
         # Look for embedded Viddler player
         mobj = re.search(
             r'<(?:iframe[^>]+?src|param[^>]+?value)=(["\'])(?P<url>(?:https?:)?//(?:www\.)?viddler\.com/(?:embed|player)/.+?)\1',
             webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
 
         # Look for NYTimes player
         mobj = re.search(
             r'<iframe[^>]+src=(["\'])(?P<url>(?:https?:)?//graphics8\.nytimes\.com/bcvideo/[^/]+/iframe/embed\.html.+?)\1>',
             webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
 
         # Look for Libsyn player
         mobj = re.search(
             r'<iframe[^>]+src=(["\'])(?P<url>(?:https?:)?//html5-player\.libsyn\.com/embed/.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
 
         # Look for Ooyala videos
         mobj = (re.search(r'player\.ooyala\.com/[^"?]+[?#][^"]*?(?:embedCode|ec)=(?P<ec>[^"&]+)', webpage)
@@ -2743,75 +2850,75 @@ class GenericIE(InfoExtractor):
             embed_token = self._search_regex(
                 r'embedToken[\'"]?\s*:\s*[\'"]([^\'"]+)',
                 webpage, 'ooyala embed token', default=None)
-            return OoyalaIE._build_url_result(smuggle_url(
+            waitlist.append(OoyalaIE._build_url_result(smuggle_url(
                 mobj.group('ec'), {
                     'domain': url,
                     'embed_token': embed_token,
-                }))
+                })))
 
         # Look for multiple Ooyala embeds on SBN network websites
         mobj = re.search(r'SBN\.VideoLinkset\.entryGroup\((\[.*?\])', webpage)
         if mobj is not None:
             embeds = self._parse_json(mobj.group(1), video_id, fatal=False)
             if embeds:
-                return self.playlist_from_matches(
+                waitlist.append(self.playlist_from_matches(
                     embeds, video_id, video_title,
-                    getter=lambda v: OoyalaIE._url_for_embed_code(smuggle_url(v['provider_video_id'], {'domain': url})), ie='Ooyala')
+                    getter=lambda v: OoyalaIE._url_for_embed_code(smuggle_url(v['provider_video_id'], {'domain': url}))), ie='Ooyala')
 
         # Look for Aparat videos
         mobj = re.search(r'<iframe .*?src="(http://www\.aparat\.com/video/[^"]+)"', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group(1), 'Aparat')
+            waitlist.append(self.url_result(mobj.group(1), 'Aparat'))
 
         # Look for MPORA videos
         mobj = re.search(r'<iframe .*?src="(http://mpora\.(?:com|de)/videos/[^"]+)"', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group(1), 'Mpora')
+            waitlist.append(self.url_result(mobj.group(1), 'Mpora'))
 
         # Look for embedded Facebook player
         facebook_urls = FacebookIE._extract_urls(webpage)
         if facebook_urls:
-            return self.playlist_from_matches(facebook_urls, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(facebook_urls, video_id, video_title))
 
         # Look for embedded VK player
         mobj = re.search(r'<iframe[^>]+?src=(["\'])(?P<url>https?://vk\.com/video_ext\.php.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'VK')
+            waitlist.append(self.url_result(mobj.group('url'), 'VK'))
 
         # Look for embedded Odnoklassniki player
         odnoklassniki_url = OdnoklassnikiIE._extract_url(webpage)
         if odnoklassniki_url:
-            return self.url_result(odnoklassniki_url, OdnoklassnikiIE.ie_key())
+            waitlist.append(self.url_result(odnoklassniki_url, OdnoklassnikiIE.ie_key()))
 
         # Look for sibnet embedded player
         sibnet_urls = VKIE._extract_sibnet_urls(webpage)
         if sibnet_urls:
-            return self.playlist_from_matches(sibnet_urls, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(sibnet_urls, video_id, video_title))
 
         # Look for embedded ivi player
         mobj = re.search(r'<embed[^>]+?src=(["\'])(?P<url>https?://(?:www\.)?ivi\.ru/video/player.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'Ivi')
+            waitlist.append(self.url_result(mobj.group('url'), 'Ivi'))
 
         # Look for embedded Huffington Post player
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>https?://embed\.live\.huffingtonpost\.com/.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'HuffPost')
+            waitlist.append(self.url_result(mobj.group('url'), 'HuffPost'))
 
         # Look for embed.ly
         mobj = re.search(r'class=["\']embedly-card["\'][^>]href=["\'](?P<url>[^"\']+)', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
         mobj = re.search(r'class=["\']embedly-embed["\'][^>]src=["\'][^"\']*url=(?P<url>[^&]+)', webpage)
         if mobj is not None:
-            return self.url_result(compat_urllib_parse_unquote(mobj.group('url')))
+            waitlist.append(self.url_result(compat_urllib_parse_unquote(mobj.group('url'))))
 
         # Look for funnyordie embed
         matches = re.findall(r'<iframe[^>]+?src="(https?://(?:www\.)?funnyordie\.com/embed/[^"]+)"', webpage)
         if matches:
-            return self.playlist_from_matches(
-                matches, video_id, video_title, getter=unescapeHTML, ie='FunnyOrDie')
+            waitlist.append(self.playlist_from_matches(
+                matches, video_id, video_title, getter=unescapeHTML, ie='FunnyOrDie'))
 
         # Look for Simplecast embeds
         simplecast_urls = SimplecastIE._extract_urls(webpage)
@@ -2822,123 +2929,123 @@ class GenericIE(InfoExtractor):
         # Look for BBC iPlayer embed
         matches = re.findall(r'setPlaylist\("(https?://www\.bbc\.co\.uk/iplayer/[^/]+/[\da-z]{8})"\)', webpage)
         if matches:
-            return self.playlist_from_matches(matches, video_id, video_title, ie='BBCCoUk')
+            waitlist.append(self.playlist_from_matches(matches, video_id, video_title, ie='BBCCoUk'))
 
         # Look for embedded RUTV player
         rutv_url = RUTVIE._extract_url(webpage)
         if rutv_url:
-            return self.url_result(rutv_url, 'RUTV')
+            waitlist.append(self.url_result(rutv_url, 'RUTV'))
 
         # Look for embedded TVC player
         tvc_url = TVCIE._extract_url(webpage)
         if tvc_url:
-            return self.url_result(tvc_url, 'TVC')
+            waitlist.append(self.url_result(tvc_url, 'TVC'))
 
         # Look for embedded SportBox player
         sportbox_urls = SportBoxIE._extract_urls(webpage)
         if sportbox_urls:
-            return self.playlist_from_matches(sportbox_urls, video_id, video_title, ie=SportBoxIE.ie_key())
+            waitlist.append(self.playlist_from_matches(sportbox_urls, video_id, video_title, ie=SportBoxIE.ie_key()))
 
         # Look for embedded XHamster player
         xhamster_urls = XHamsterEmbedIE._extract_urls(webpage)
         if xhamster_urls:
-            return self.playlist_from_matches(xhamster_urls, video_id, video_title, ie='XHamsterEmbed')
+            waitlist.append(self.playlist_from_matches(xhamster_urls, video_id, video_title, ie='XHamsterEmbed'))
 
         # Look for embedded TNAFlixNetwork player
         tnaflix_urls = TNAFlixNetworkEmbedIE._extract_urls(webpage)
         if tnaflix_urls:
-            return self.playlist_from_matches(tnaflix_urls, video_id, video_title, ie=TNAFlixNetworkEmbedIE.ie_key())
+            waitlist.append(self.playlist_from_matches(tnaflix_urls, video_id, video_title, ie=TNAFlixNetworkEmbedIE.ie_key()))
 
         # Look for embedded PornHub player
         pornhub_urls = PornHubIE._extract_urls(webpage)
         if pornhub_urls:
-            return self.playlist_from_matches(pornhub_urls, video_id, video_title, ie=PornHubIE.ie_key())
+            waitlist.append(self.playlist_from_matches(pornhub_urls, video_id, video_title, ie=PornHubIE.ie_key()))
 
         # Look for embedded DrTuber player
         drtuber_urls = DrTuberIE._extract_urls(webpage)
         if drtuber_urls:
-            return self.playlist_from_matches(drtuber_urls, video_id, video_title, ie=DrTuberIE.ie_key())
+            waitlist.append(self.playlist_from_matches(drtuber_urls, video_id, video_title, ie=DrTuberIE.ie_key()))
 
         # Look for embedded RedTube player
         redtube_urls = RedTubeIE._extract_urls(webpage)
         if redtube_urls:
-            return self.playlist_from_matches(redtube_urls, video_id, video_title, ie=RedTubeIE.ie_key())
+            waitlist.append(self.playlist_from_matches(redtube_urls, video_id, video_title, ie=RedTubeIE.ie_key()))
 
         # Look for embedded Tube8 player
         tube8_urls = Tube8IE._extract_urls(webpage)
         if tube8_urls:
-            return self.playlist_from_matches(tube8_urls, video_id, video_title, ie=Tube8IE.ie_key())
+            waitlist.append(self.playlist_from_matches(tube8_urls, video_id, video_title, ie=Tube8IE.ie_key()))
 
         # Look for embedded Mofosex player
         mofosex_urls = MofosexEmbedIE._extract_urls(webpage)
         if mofosex_urls:
-            return self.playlist_from_matches(mofosex_urls, video_id, video_title, ie=MofosexEmbedIE.ie_key())
+            waitlist.append(self.playlist_from_matches(mofosex_urls, video_id, video_title, ie=MofosexEmbedIE.ie_key()))
 
         # Look for embedded Spankwire player
         spankwire_urls = SpankwireIE._extract_urls(webpage)
         if spankwire_urls:
-            return self.playlist_from_matches(spankwire_urls, video_id, video_title, ie=SpankwireIE.ie_key())
+            waitlist.append(self.playlist_from_matches(spankwire_urls, video_id, video_title, ie=SpankwireIE.ie_key()))
 
         # Look for embedded YouPorn player
         youporn_urls = YouPornIE._extract_urls(webpage)
         if youporn_urls:
-            return self.playlist_from_matches(youporn_urls, video_id, video_title, ie=YouPornIE.ie_key())
+            waitlist.append(self.playlist_from_matches(youporn_urls, video_id, video_title, ie=YouPornIE.ie_key()))
 
         # Look for embedded Tvigle player
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>(?:https?:)?//cloud\.tvigle\.ru/video/.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'Tvigle')
+            waitlist.append(self.url_result(mobj.group('url'), 'Tvigle'))
 
         # Look for embedded TED player
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>https?://embed(?:-ssl)?\.ted\.com/.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'TED')
+            waitlist.append(self.url_result(mobj.group('url'), 'TED'))
 
         # Look for embedded Ustream videos
         ustream_url = UstreamIE._extract_url(webpage)
         if ustream_url:
-            return self.url_result(ustream_url, UstreamIE.ie_key())
+            waitlist.append(self.url_result(ustream_url, UstreamIE.ie_key()))
 
         # Look for embedded arte.tv player
         arte_urls = ArteTVEmbedIE._extract_urls(webpage)
         if arte_urls:
-            return self.playlist_from_matches(arte_urls, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(arte_urls, video_id, video_title))
 
         # Look for embedded francetv player
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>(?:https?://)?embed\.francetv\.fr/\?ue=.+?)\1',
             webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
 
         # Look for embedded Myvi.ru player
         myvi_url = MyviIE._extract_url(webpage)
         if myvi_url:
-            return self.url_result(myvi_url)
+            waitlist.append(self.url_result(myvi_url))
 
         # Look for embedded soundcloud player
         soundcloud_urls = SoundcloudEmbedIE._extract_urls(webpage)
         if soundcloud_urls:
-            return self.playlist_from_matches(soundcloud_urls, video_id, video_title, getter=unescapeHTML)
+            waitlist.append(self.playlist_from_matches(soundcloud_urls, video_id, video_title, getter=unescapeHTML))
 
         # Look for tunein player
         tunein_urls = TuneInBaseIE._extract_urls(webpage)
         if tunein_urls:
-            return self.playlist_from_matches(tunein_urls, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(tunein_urls, video_id, video_title))
 
         # Look for embedded mtvservices player
         mtvservices_url = MTVServicesEmbeddedIE._extract_url(webpage)
         if mtvservices_url:
-            return self.url_result(mtvservices_url, ie='MTVServicesEmbedded')
+            waitlist.append(self.url_result(mtvservices_url, ie='MTVServicesEmbedded'))
 
         # Look for embedded yahoo player
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>https?://(?:screen|movies)\.yahoo\.com/.+?\.html\?format=embed)\1',
             webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'Yahoo')
+            waitlist.append(self.url_result(mobj.group('url'), 'Yahoo'))
 
         # Look for embedded sbs.com.au player
         mobj = re.search(
@@ -2950,14 +3057,14 @@ class GenericIE(InfoExtractor):
             (["\'])(?P<url>https?://(?:www\.)?sbs\.com\.au/ondemand/video/.+?)\1''',
             webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'SBS')
+            waitlist.append(self.url_result(mobj.group('url'), 'SBS'))
 
         # Look for embedded Cinchcast player
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>https?://player\.cinchcast\.com/.+?)\1',
             webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'Cinchcast')
+            waitlist.append(self.url_result(mobj.group('url'), 'Cinchcast'))
 
         mobj = re.search(
             r'<iframe[^>]+?src=(["\'])(?P<url>https?://m(?:lb)?\.mlb\.com/shared/video/embed/embed\.html\?.+?)\1',
@@ -2967,178 +3074,178 @@ class GenericIE(InfoExtractor):
                 r'data-video-link=["\'](?P<url>http://m\.mlb\.com/video/[^"\']+)',
                 webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'MLB')
+            waitlist.append(self.url_result(mobj.group('url'), 'MLB'))
 
         mobj = re.search(
             r'<(?:iframe|script)[^>]+?src=(["\'])(?P<url>%s)\1' % CondeNastIE.EMBED_URL,
             webpage)
         if mobj is not None:
-            return self.url_result(self._proto_relative_url(mobj.group('url'), scheme='http:'), 'CondeNast')
+            waitlist.append(self.url_result(self._proto_relative_url(mobj.group('url'), scheme='http:'), 'CondeNast'))
 
         mobj = re.search(
             r'<iframe[^>]+src="(?P<url>https?://(?:new\.)?livestream\.com/[^"]+/player[^"]+)"',
             webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'Livestream')
+            waitlist.append(self.url_result(mobj.group('url'), 'Livestream'))
 
         # Look for Zapiks embed
         mobj = re.search(
             r'<iframe[^>]+src="(?P<url>https?://(?:www\.)?zapiks\.fr/index\.php\?.+?)"', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'), 'Zapiks')
+            waitlist.append(self.url_result(mobj.group('url'), 'Zapiks'))
 
         # Look for Kaltura embeds
         kaltura_urls = KalturaIE._extract_urls(webpage)
         if kaltura_urls:
-            return self.playlist_from_matches(
+            waitlist.append(self.playlist_from_matches(
                 kaltura_urls, video_id, video_title,
                 getter=lambda x: smuggle_url(x, {'source_url': url}),
-                ie=KalturaIE.ie_key())
+                ie=KalturaIE.ie_key()))
 
         # Look for EaglePlatform embeds
         eagleplatform_url = EaglePlatformIE._extract_url(webpage)
         if eagleplatform_url:
-            return self.url_result(smuggle_url(eagleplatform_url, {'referrer': url}), EaglePlatformIE.ie_key())
+            waitlist.append(self.url_result(smuggle_url(eagleplatform_url, {'referrer': url}), EaglePlatformIE.ie_key()))
 
         # Look for ClipYou (uses EaglePlatform) embeds
         mobj = re.search(
             r'<iframe[^>]+src="https?://(?P<host>media\.clipyou\.ru)/index/player\?.*\brecord_id=(?P<id>\d+).*"', webpage)
         if mobj is not None:
-            return self.url_result('eagleplatform:%(host)s:%(id)s' % mobj.groupdict(), 'EaglePlatform')
+            waitlist.append(self.url_result('eagleplatform:%(host)s:%(id)s' % mobj.groupdict(), 'EaglePlatform'))
 
         # Look for Pladform embeds
         pladform_url = PladformIE._extract_url(webpage)
         if pladform_url:
-            return self.url_result(pladform_url)
+            waitlist.append(self.url_result(pladform_url))
 
         # Look for Videomore embeds
         videomore_url = VideomoreIE._extract_url(webpage)
         if videomore_url:
-            return self.url_result(videomore_url)
+            waitlist.append(self.url_result(videomore_url))
 
         # Look for Webcaster embeds
         webcaster_url = WebcasterFeedIE._extract_url(self, webpage)
         if webcaster_url:
-            return self.url_result(webcaster_url, ie=WebcasterFeedIE.ie_key())
+            waitlist.append(self.url_result(webcaster_url, ie=WebcasterFeedIE.ie_key()))
 
         # Look for Playwire embeds
         mobj = re.search(
             r'<script[^>]+data-config=(["\'])(?P<url>(?:https?:)?//config\.playwire\.com/.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
 
         # Look for 5min embeds
         mobj = re.search(
             r'<meta[^>]+property="og:video"[^>]+content="https?://embed\.5min\.com/(?P<id>[0-9]+)/?', webpage)
         if mobj is not None:
-            return self.url_result('5min:%s' % mobj.group('id'), 'FiveMin')
+            waitlist.append(self.url_result('5min:%s' % mobj.group('id'), 'FiveMin'))
 
         # Look for Crooks and Liars embeds
         mobj = re.search(
             r'<(?:iframe[^>]+src|param[^>]+value)=(["\'])(?P<url>(?:https?:)?//embed\.crooksandliars\.com/(?:embed|v)/.+?)\1', webpage)
         if mobj is not None:
-            return self.url_result(mobj.group('url'))
+            waitlist.append(self.url_result(mobj.group('url')))
 
         # Look for NBC Sports VPlayer embeds
         nbc_sports_url = NBCSportsVPlayerIE._extract_url(webpage)
         if nbc_sports_url:
-            return self.url_result(nbc_sports_url, 'NBCSportsVPlayer')
+            waitlist.append(self.url_result(nbc_sports_url, 'NBCSportsVPlayer'))
 
         # Look for NBC News embeds
         nbc_news_embed_url = re.search(
             r'<iframe[^>]+src=(["\'])(?P<url>(?:https?:)?//www\.nbcnews\.com/widget/video-embed/[^"\']+)\1', webpage)
         if nbc_news_embed_url:
-            return self.url_result(nbc_news_embed_url.group('url'), 'NBCNews')
+            waitlist.append(self.url_result(nbc_news_embed_url.group('url'), 'NBCNews'))
 
         # Look for Google Drive embeds
         google_drive_url = GoogleDriveIE._extract_url(webpage)
         if google_drive_url:
-            return self.url_result(google_drive_url, 'GoogleDrive')
+            waitlist.append(self.url_result(google_drive_url, 'GoogleDrive'))
 
         # Look for UDN embeds
         mobj = re.search(
             r'<iframe[^>]+src="(?:https?:)?(?P<url>%s)"' % UDNEmbedIE._PROTOCOL_RELATIVE_VALID_URL, webpage)
         if mobj is not None:
-            return self.url_result(
-                compat_urlparse.urljoin(url, mobj.group('url')), 'UDNEmbed')
+            waitlist.append(self.url_result(
+                compat_urlparse.urljoin(url, mobj.group('url')), 'UDNEmbed'))
 
         # Look for Senate ISVP iframe
         senate_isvp_url = SenateISVPIE._search_iframe_url(webpage)
         if senate_isvp_url:
-            return self.url_result(senate_isvp_url, 'SenateISVP')
+            waitlist.append(self.url_result(senate_isvp_url, 'SenateISVP'))
 
         # Look for Kinja embeds
         kinja_embed_urls = KinjaEmbedIE._extract_urls(webpage, url)
         if kinja_embed_urls:
-            return self.playlist_from_matches(
-                kinja_embed_urls, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(
+                kinja_embed_urls, video_id, video_title))
 
         # Look for OnionStudios embeds
         onionstudios_url = OnionStudiosIE._extract_url(webpage)
         if onionstudios_url:
-            return self.url_result(onionstudios_url)
+            waitlist.append(self.url_result(onionstudios_url))
 
         # Look for ViewLift embeds
         viewlift_url = ViewLiftEmbedIE._extract_url(webpage)
         if viewlift_url:
-            return self.url_result(viewlift_url)
+            waitlist.append(self.url_result(viewlift_url))
 
         # Look for JWPlatform embeds
         jwplatform_urls = JWPlatformIE._extract_urls(webpage)
         if jwplatform_urls:
-            return self.playlist_from_matches(jwplatform_urls, video_id, video_title, ie=JWPlatformIE.ie_key())
+            waitlist.append(self.playlist_from_matches(jwplatform_urls, video_id, video_title, ie=JWPlatformIE.ie_key()))
 
         # Look for Digiteka embeds
         digiteka_url = DigitekaIE._extract_url(webpage)
         if digiteka_url:
-            return self.url_result(self._proto_relative_url(digiteka_url), DigitekaIE.ie_key())
+            waitlist.append(self.url_result(self._proto_relative_url(digiteka_url), DigitekaIE.ie_key()))
 
         # Look for Arkena embeds
         arkena_url = ArkenaIE._extract_url(webpage)
         if arkena_url:
-            return self.url_result(arkena_url, ArkenaIE.ie_key())
+            waitlist.append(self.url_result(arkena_url, ArkenaIE.ie_key()))
 
         # Look for Piksel embeds
         piksel_url = PikselIE._extract_url(webpage)
         if piksel_url:
-            return self.url_result(piksel_url, PikselIE.ie_key())
+            waitlist.append(self.url_result(piksel_url, PikselIE.ie_key()))
 
         # Look for Limelight embeds
         limelight_urls = LimelightBaseIE._extract_urls(webpage, url)
         if limelight_urls:
-            return self.playlist_result(
-                limelight_urls, video_id, video_title, video_description)
+            waitlist.append(self.playlist_result(
+                limelight_urls, video_id, video_title, video_description))
 
         # Look for Anvato embeds
         anvato_urls = AnvatoIE._extract_urls(self, webpage, video_id)
         if anvato_urls:
-            return self.playlist_result(
-                anvato_urls, video_id, video_title, video_description)
+            waitlist.append(self.playlist_result(
+                anvato_urls, video_id, video_title, video_description))
 
         # Look for AdobeTVVideo embeds
         mobj = re.search(
             r'<iframe[^>]+src=[\'"]((?:https?:)?//video\.tv\.adobe\.com/v/\d+[^"]+)[\'"]',
             webpage)
         if mobj is not None:
-            return self.url_result(
+            waitlist.append(self.url_result(
                 self._proto_relative_url(unescapeHTML(mobj.group(1))),
-                'AdobeTVVideo')
+                'AdobeTVVideo'))
 
         # Look for Vine embeds
         mobj = re.search(
             r'<iframe[^>]+src=[\'"]((?:https?:)?//(?:www\.)?vine\.co/v/[^/]+/embed/(?:simple|postcard))',
             webpage)
         if mobj is not None:
-            return self.url_result(
-                self._proto_relative_url(unescapeHTML(mobj.group(1))), 'Vine')
+            waitlist.append(self.url_result(
+                self._proto_relative_url(unescapeHTML(mobj.group(1))), 'Vine'))
 
         # Look for VODPlatform embeds
         mobj = re.search(
             r'<iframe[^>]+src=(["\'])(?P<url>(?:https?:)?//(?:(?:www\.)?vod-platform\.net|embed\.kwikmotion\.com)/[eE]mbed/.+?)\1',
             webpage)
         if mobj is not None:
-            return self.url_result(
-                self._proto_relative_url(unescapeHTML(mobj.group('url'))), 'VODPlatform')
+            waitlist.append(self.url_result(
+                self._proto_relative_url(unescapeHTML(mobj.group('url'))), 'VODPlatform'))
 
         # Look for Mangomolo embeds
         mobj = re.search(
@@ -3171,23 +3278,23 @@ class GenericIE(InfoExtractor):
                     'ie_key': 'MangomoloLive',
                     'id': mobj.group('channel_id'),
                 })
-            return info
+            waitlist.append(info)
 
         # Look for Instagram embeds
         instagram_embed_url = InstagramIE._extract_embed_url(webpage)
         if instagram_embed_url is not None:
-            return self.url_result(
-                self._proto_relative_url(instagram_embed_url), InstagramIE.ie_key())
+            waitlist.append(self.url_result(
+                self._proto_relative_url(instagram_embed_url), InstagramIE.ie_key()))
 
         # Look for LiveLeak embeds
         liveleak_urls = LiveLeakIE._extract_urls(webpage)
         if liveleak_urls:
-            return self.playlist_from_matches(liveleak_urls, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(liveleak_urls, video_id, video_title))
 
         # Look for 3Q SDN embeds
         threeqsdn_url = ThreeQSDNIE._extract_url(webpage)
         if threeqsdn_url:
-            return {
+            waitlist.append({
                 '_type': 'url_transparent',
                 'ie_key': ThreeQSDNIE.ie_key(),
                 'url': self._proto_relative_url(threeqsdn_url),
@@ -3195,80 +3302,80 @@ class GenericIE(InfoExtractor):
                 'description': video_description,
                 'thumbnail': video_thumbnail,
                 'uploader': video_uploader,
-            }
+            })
 
         # Look for VBOX7 embeds
         vbox7_url = Vbox7IE._extract_url(webpage)
         if vbox7_url:
-            return self.url_result(vbox7_url, Vbox7IE.ie_key())
+            waitlist.append(self.url_result(vbox7_url, Vbox7IE.ie_key()))
 
         # Look for DBTV embeds
         dbtv_urls = DBTVIE._extract_urls(webpage)
         if dbtv_urls:
-            return self.playlist_from_matches(dbtv_urls, video_id, video_title, ie=DBTVIE.ie_key())
+            waitlist.append(self.playlist_from_matches(dbtv_urls, video_id, video_title, ie=DBTVIE.ie_key()))
 
         # Look for Videa embeds
         videa_urls = VideaIE._extract_urls(webpage)
         if videa_urls:
-            return self.playlist_from_matches(videa_urls, video_id, video_title, ie=VideaIE.ie_key())
+            waitlist.append(self.playlist_from_matches(videa_urls, video_id, video_title, ie=VideaIE.ie_key()))
 
         # Look for 20 minuten embeds
         twentymin_urls = TwentyMinutenIE._extract_urls(webpage)
         if twentymin_urls:
-            return self.playlist_from_matches(
-                twentymin_urls, video_id, video_title, ie=TwentyMinutenIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                twentymin_urls, video_id, video_title, ie=TwentyMinutenIE.ie_key()))
 
         # Look for VideoPress embeds
         videopress_urls = VideoPressIE._extract_urls(webpage)
         if videopress_urls:
-            return self.playlist_from_matches(
-                videopress_urls, video_id, video_title, ie=VideoPressIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                videopress_urls, video_id, video_title, ie=VideoPressIE.ie_key()))
 
         # Look for Rutube embeds
         rutube_urls = RutubeIE._extract_urls(webpage)
         if rutube_urls:
-            return self.playlist_from_matches(
-                rutube_urls, video_id, video_title, ie=RutubeIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                rutube_urls, video_id, video_title, ie=RutubeIE.ie_key()))
 
         # Look for WashingtonPost embeds
         wapo_urls = WashingtonPostIE._extract_urls(webpage)
         if wapo_urls:
-            return self.playlist_from_matches(
-                wapo_urls, video_id, video_title, ie=WashingtonPostIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                wapo_urls, video_id, video_title, ie=WashingtonPostIE.ie_key()))
 
         # Look for Mediaset embeds
         mediaset_urls = MediasetIE._extract_urls(self, webpage)
         if mediaset_urls:
-            return self.playlist_from_matches(
-                mediaset_urls, video_id, video_title, ie=MediasetIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                mediaset_urls, video_id, video_title, ie=MediasetIE.ie_key()))
 
         # Look for JOJ.sk embeds
         joj_urls = JojIE._extract_urls(webpage)
         if joj_urls:
-            return self.playlist_from_matches(
-                joj_urls, video_id, video_title, ie=JojIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                joj_urls, video_id, video_title, ie=JojIE.ie_key()))
 
         # Look for megaphone.fm embeds
         mpfn_urls = MegaphoneIE._extract_urls(webpage)
         if mpfn_urls:
-            return self.playlist_from_matches(
-                mpfn_urls, video_id, video_title, ie=MegaphoneIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                mpfn_urls, video_id, video_title, ie=MegaphoneIE.ie_key()))
 
         # Look for vzaar embeds
         vzaar_urls = VzaarIE._extract_urls(webpage)
         if vzaar_urls:
-            return self.playlist_from_matches(
-                vzaar_urls, video_id, video_title, ie=VzaarIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                vzaar_urls, video_id, video_title, ie=VzaarIE.ie_key()))
 
         channel9_urls = Channel9IE._extract_urls(webpage)
         if channel9_urls:
-            return self.playlist_from_matches(
-                channel9_urls, video_id, video_title, ie=Channel9IE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                channel9_urls, video_id, video_title, ie=Channel9IE.ie_key()))
 
         vshare_urls = VShareIE._extract_urls(webpage)
         if vshare_urls:
-            return self.playlist_from_matches(
-                vshare_urls, video_id, video_title, ie=VShareIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                vshare_urls, video_id, video_title, ie=VShareIE.ie_key()))
 
         # Look for Mediasite embeds
         mediasite_urls = MediasiteIE._extract_urls(webpage)
@@ -3278,75 +3385,68 @@ class GenericIE(InfoExtractor):
                     compat_urlparse.urljoin(url, mediasite_url),
                     {'UrlReferrer': url}), ie=MediasiteIE.ie_key())
                 for mediasite_url in mediasite_urls]
-            return self.playlist_result(entries, video_id, video_title)
+            waitlist.append(self.playlist_result(entries, video_id, video_title))
 
         springboardplatform_urls = SpringboardPlatformIE._extract_urls(webpage)
         if springboardplatform_urls:
-            return self.playlist_from_matches(
+            waitlist.append(self.playlist_from_matches(
                 springboardplatform_urls, video_id, video_title,
-                ie=SpringboardPlatformIE.ie_key())
+                ie=SpringboardPlatformIE.ie_key()))
 
         yapfiles_urls = YapFilesIE._extract_urls(webpage)
         if yapfiles_urls:
-            return self.playlist_from_matches(
-                yapfiles_urls, video_id, video_title, ie=YapFilesIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                yapfiles_urls, video_id, video_title, ie=YapFilesIE.ie_key()))
 
         vice_urls = ViceIE._extract_urls(webpage)
         if vice_urls:
-            return self.playlist_from_matches(
-                vice_urls, video_id, video_title, ie=ViceIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                vice_urls, video_id, video_title, ie=ViceIE.ie_key()))
 
         xfileshare_urls = XFileShareIE._extract_urls(webpage)
         if xfileshare_urls:
-            return self.playlist_from_matches(
-                xfileshare_urls, video_id, video_title, ie=XFileShareIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                xfileshare_urls, video_id, video_title, ie=XFileShareIE.ie_key()))
 
         cloudflarestream_urls = CloudflareStreamIE._extract_urls(webpage)
         if cloudflarestream_urls:
-            return self.playlist_from_matches(
-                cloudflarestream_urls, video_id, video_title, ie=CloudflareStreamIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                cloudflarestream_urls, video_id, video_title, ie=CloudflareStreamIE.ie_key()))
 
         peertube_urls = PeerTubeIE._extract_urls(webpage, url)
         if peertube_urls:
-            return self.playlist_from_matches(
-                peertube_urls, video_id, video_title, ie=PeerTubeIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                peertube_urls, video_id, video_title, ie=PeerTubeIE.ie_key()))
 
         indavideo_urls = IndavideoEmbedIE._extract_urls(webpage)
         if indavideo_urls:
-            return self.playlist_from_matches(
-                indavideo_urls, video_id, video_title, ie=IndavideoEmbedIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                indavideo_urls, video_id, video_title, ie=IndavideoEmbedIE.ie_key()))
 
         apa_urls = APAIE._extract_urls(webpage)
         if apa_urls:
-            return self.playlist_from_matches(
-                apa_urls, video_id, video_title, ie=APAIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                apa_urls, video_id, video_title, ie=APAIE.ie_key()))
 
         foxnews_urls = FoxNewsIE._extract_urls(webpage)
         if foxnews_urls:
-            return self.playlist_from_matches(
-                foxnews_urls, video_id, video_title, ie=FoxNewsIE.ie_key())
-
-        sharevideos_urls = [sharevideos_mobj.group('url') for sharevideos_mobj in re.finditer(
-            r'<iframe[^>]+?\bsrc\s*=\s*(["\'])(?P<url>(?:https?:)?//embed\.share-videos\.se/auto/embed/\d+\?.*?\buid=\d+.*?)\1',
-            webpage)]
-        if sharevideos_urls:
-            return self.playlist_from_matches(
-                sharevideos_urls, video_id, video_title)
+            waitlist.append(self.playlist_from_matches(
+                foxnews_urls, video_id, video_title, ie=FoxNewsIE.ie_key()))
 
         viqeo_urls = ViqeoIE._extract_urls(webpage)
         if viqeo_urls:
-            return self.playlist_from_matches(
-                viqeo_urls, video_id, video_title, ie=ViqeoIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                viqeo_urls, video_id, video_title, ie=ViqeoIE.ie_key()))
 
         expressen_urls = ExpressenIE._extract_urls(webpage)
         if expressen_urls:
-            return self.playlist_from_matches(
-                expressen_urls, video_id, video_title, ie=ExpressenIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                expressen_urls, video_id, video_title, ie=ExpressenIE.ie_key()))
 
         zype_urls = ZypeIE._extract_urls(webpage)
         if zype_urls:
-            return self.playlist_from_matches(
-                zype_urls, video_id, video_title, ie=ZypeIE.ie_key())
+            waitlist.append(self.playlist_from_matches(
+                zype_urls, video_id, video_title, ie=ZypeIE.ie_key()))
 
         # Look for HTML5 media
         entries = self._parse_html5_media_entries(url, webpage, video_id, m3u8_id='hls')
@@ -3364,7 +3464,7 @@ class GenericIE(InfoExtractor):
                     })
             for entry in entries:
                 self._sort_formats(entry['formats'])
-            return self.playlist_result(entries, video_id, video_title)
+            waitlist.append(self.playlist_result(entries, video_id, video_title))
 
         jwplayer_data = self._find_jwplayer_data(
             webpage, video_id, transform_source=js_to_json)
@@ -3372,11 +3472,14 @@ class GenericIE(InfoExtractor):
             try:
                 info = self._parse_jwplayer_data(
                     jwplayer_data, video_id, require_title=False, base_url=url)
-                return merge_dicts(info, info_dict)
+                waitlist.append(merge_dicts(info, info_dict))
             except ExtractorError:
                 # See https://github.com/ytdl-org/youtube-dl/pull/16735
                 pass
 
+        if waitlist:
+            return self.playlist_result(waitlist, video_id, video_id)
+
         # Video.js embed
         mobj = re.search(
             r'(?s)\bvideojs\s*\(.+?\.src\s*\(\s*((?:\[.+?\]|{.+?}))\s*\)\s*;',
diff --git a/youtube_dl/extractor/hellonewdream.py b/youtube_dl/extractor/hellonewdream.py
new file mode 100644
index 000000000..de9d64a14
--- /dev/null
+++ b/youtube_dl/extractor/hellonewdream.py
@@ -0,0 +1,33 @@
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+
+
+class HelloNewDreamIE(InfoExtractor):
+    _VALID_URL = r'https?://anatafor\.hello-new-dream\.jp/share/\?uid=(?P<id>([0-9]{4})([0-9]{2})([0-9]{2})([0-9]{4})([a-z0-9]+))'
+    # https://anatafor.hello-new-dream.jp/data/2020/09/15/2225/48psv3dypxg9b35mbwx1/udata.json
+    UDATA_API_URL = 'https://anatafor.hello-new-dream.jp/data/%s/%s/%s/%s/%s/udata.json'
+    MIX_MP3_URL = 'https://anatafor.hello-new-dream.jp/data/%s/%s/%s/%s/%s/mix.mp3'
+    _TESTS = []
+
+    def _real_extract(self, url):
+        song_id = self._match_id(url)
+        groups = self._VALID_URL_RE.match(url).groups()[1:]
+        udata = self._download_json(self.UDATA_API_URL % groups, song_id)
+
+        # https://anatafor.hello-new-dream.jp/data/2020/09/15/0249/leillvlfe9f1w8v5mek5/mix.mp3
+        mp3_url = self.MIX_MP3_URL % groups
+        formats = [{
+            'url': mp3_url,
+            'format_id': 'mp3',
+            'ext': 'mp3',
+            'vcodec': 'none',
+        }]
+
+        return {
+            'id': song_id,
+            'display_id': song_id,
+            'title': '%s %s' % (udata['name_1'], udata['name_2']),
+            'formats': formats,
+            'description': '%s %s' % (udata['dream_1'], udata['dream_2']),
+        }
diff --git a/youtube_dl/extractor/ichinanalive.py b/youtube_dl/extractor/ichinanalive.py
new file mode 100644
index 000000000..692c784bd
--- /dev/null
+++ b/youtube_dl/extractor/ichinanalive.py
@@ -0,0 +1,176 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+from ..utils import ExtractorError, str_or_none, unified_strdate
+from ..compat import compat_str
+
+
+# the real service name of this extractor is "17live",
+#   but identifiers cannot start with numbers.
+# class name of this extractor is taken from official pronounce in Japanese,
+#   so it will be replace as: "1"="ichi", "7"="nana", "live"=as-is .
+class IchinanaLiveIE(InfoExtractor):
+    IE_NAME = '17live'
+    _VALID_URL = r'https?://(?:www\.)?17\.live/(?:live|profile/r)/(?P<id>\d+)'
+    _TEST = {
+        'url': 'https://17.live/live/580309',
+        'only_matching': True,
+    }
+
+    @classmethod
+    def suitable(cls, url):
+        return not IchinanaLiveClipIE.suitable(url) and super(IchinanaLiveIE, cls).suitable(url)
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        url = 'https://17.live/live/%s' % video_id
+        # self._download_webpage(url, video_id)
+
+        # this endpoint sometimes return code 420, which is not defined
+        enter = self._download_json(
+            'https://api-dsa.17app.co/api/v1/lives/%s/enter' % video_id, video_id,
+            headers={'Referer': url}, fatal=False, expected_status=lambda x: True,
+            data=b'\0')
+        if enter and enter.get('message') == 'ended':
+            raise ExtractorError('This live has ended.', expected=True)
+
+        view_data = self._download_json(
+            'https://api-dsa.17app.co/api/v1/lives/%s' % video_id, video_id,
+            headers={'Referer': url})
+
+        user_info = view_data.get('userInfo')
+        uploader = None
+        if user_info:
+            uploader = user_info.get('displayName') or user_info.get('openID')
+        like_count = view_data.get('receivedLikeCount')
+        view_count = view_data.get('viewerCount')
+        thumbnail = view_data.get('coverPhoto')
+        description = view_data.get('caption')
+        upload_date = unified_strdate(str_or_none(view_data.get('beginTime')))
+
+        video_urls = view_data.get('rtmpUrls')
+        if not video_urls:
+            raise ExtractorError('unable to extract live URL information')
+        formats = []
+        # it used to select an item with .provider == 5,
+        # but js code seems to select the first element
+        for (name, value) in video_urls[0].items():
+            if not isinstance(value, compat_str):
+                continue
+            if not value.startswith('http'):
+                continue
+            preference = 0.0
+            if 'web' in name:
+                preference -= 0.25
+            if 'High' in name:
+                preference += 1.0
+            if 'Low' in name:
+                preference -= 0.5
+            formats.append({
+                'format_id': name,
+                'url': value,
+                'preference': preference,
+                # 'ffmpeg' protocol is added by ytdl-patched, same as 'm3u8'
+                'protocol': 'ffmpeg',
+                'http_headers': {'Referer': url},
+                'ext': 'mp4',
+                'vcodec': 'h264',
+                'acodec': 'aac',
+            })
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': uploader or video_id,
+            'formats': formats,
+            'is_live': True,
+            'uploader': uploader,
+            'uploader_id': video_id,
+            'like_count': like_count,
+            'view_count': view_count,
+            'thumbnail': thumbnail,
+            'description': description,
+            'upload_date': upload_date,
+        }
+
+
+class IchinanaLiveClipIE(InfoExtractor):
+    IE_NAME = '17live:clip'
+    _VALID_URL = r'https?://(?:www\.)?17\.live/profile/r/(?P<uploader_id>\d+)/clip/(?P<id>[^/]+)'
+    _TEST = {
+        'url': 'https://17.live/profile/r/1789280/clip/1bHQSK8KUieruFXaCH4A4upCzlN',
+        'info_dict': {
+            'id': '1bHQSK8KUieruFXaCH4A4upCzlN',
+            'title': '„Éû„ÉÅ„Ç≥ÂÖàÁîüü¶ãClassüíã',
+            'description': '„Éû„ÉÅÊà¶Èöä„ÄÄÁ¨¨‰∏ÄÊ¨°„ÄÄ„Éê„Çπ„Çø„Éº„Ç≥„Éº„É´\nÁ∑èÈ°ç200‰∏ácoinÔºÅ\nÂãïÁîªÂà∂‰Ωú@„ÅÜ„Åâ„Éº„Åã„Éºüå±Walkerüé´',
+            'uploader_id': '1789280',
+        },
+    }
+
+    def _real_extract(self, url):
+        m = self._valid_url_re().match(url)
+        uploader_id, video_id = m.group('uploader_id'), m.group('id')
+        url = 'https://17.live/profile/r/%s/clip/%s' % (uploader_id, video_id)
+
+        view_data = self._download_json(
+            'https://api-dsa.17app.co/api/v1/clips/%s/view' % video_id, video_id,
+            headers={'Referer': url}, data=b'\0')
+
+        like_count = view_data.get('likeCount')
+        view_count = view_data.get('viewCount')
+        thumbnail = view_data.get('imageURL')
+        duration = view_data.get('duration')
+        description = view_data.get('caption')
+        upload_date = unified_strdate(str_or_none(view_data.get('createdAt')))
+
+        user_info = self._download_json(
+            'https://api-dsa.17app.co/api/v1/clips/%s/view' % video_id, video_id,
+            headers={'Referer': url}, fatal=False)
+        uploader = None
+        if user_info:
+            uploader = user_info.get('displayName') or user_info.get('openID')
+
+        formats = []
+        if view_data.get('videoURL'):
+            formats.append({
+                'url': view_data['videoURL'],
+                'preference': -1,
+            })
+        if view_data.get('transcodeURL'):
+            formats.append({
+                'url': view_data['transcodeURL'],
+                'preference': -1,
+            })
+        if view_data.get('srcVideoURL'):
+            # highest quality
+            formats.append({
+                'url': view_data['srcVideoURL'],
+                'preference': 1,
+            })
+
+        for fmt in formats:
+            fmt.update({
+                'ext': 'mp4',
+                'protocol': 'https',
+                'vcodec': 'h264',
+                'acodec': 'aac',
+                'http_headers': {'Referer': url},
+            })
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': uploader or video_id,
+            'formats': formats,
+            'uploader': uploader,
+            'uploader_id': video_id,
+            'like_count': like_count,
+            'view_count': view_count,
+            'thumbnail': thumbnail,
+            'duration': duration,
+            'description': description,
+            'upload_date': upload_date,
+        }
diff --git a/youtube_dl/extractor/instagram.py b/youtube_dl/extractor/instagram.py
index 12e10143c..9a54285b3 100644
--- a/youtube_dl/extractor/instagram.py
+++ b/youtube_dl/extractor/instagram.py
@@ -8,7 +8,6 @@ import re
 from .common import InfoExtractor
 from ..compat import (
     compat_str,
-    compat_HTTPError,
 )
 from ..utils import (
     ExtractorError,
@@ -144,35 +143,54 @@ class InstagramIE(InfoExtractor):
         video_id = mobj.group('id')
         url = mobj.group('url')
 
-        webpage = self._download_webpage(url, video_id)
-
         (media, video_url, description, thumbnail, timestamp, uploader,
          uploader_id, like_count, comment_count, comments, height,
          width) = [None] * 12
 
-        shared_data = self._parse_json(
-            self._search_regex(
-                r'window\._sharedData\s*=\s*({.+?});',
-                webpage, 'shared data', default='{}'),
-            video_id, fatal=False)
-        if shared_data:
-            media = try_get(
-                shared_data,
-                (lambda x: x['entry_data']['PostPage'][0]['graphql']['shortcode_media'],
-                 lambda x: x['entry_data']['PostPage'][0]['media']),
-                dict)
-        # _sharedData.entry_data.PostPage is empty when authenticated (see
-        # https://github.com/ytdl-org/youtube-dl/pull/22880)
-        if not media:
-            additional_data = self._parse_json(
-                self._search_regex(
-                    r'window\.__additionalDataLoaded\s*\(\s*[^,]+,\s*({.+?})\s*\)\s*;',
-                    webpage, 'additional data', default='{}'),
-                video_id, fatal=False)
-            if additional_data:
+        retries = self._downloader.params.get('extractor_retries', 3)
+        count = -1
+        last_error = None
+        while count < retries:
+            count += 1
+            if count:
+                self.report_warning('Failed to parse response. Retrying...')
+
+            try:
+                webpage = self._download_webpage(url, video_id)
+                shared_data = self._parse_json(
+                    self._search_regex(
+                        r'window\._sharedData\s*=\s*({.+?});',
+                        webpage, 'shared data', default='{}'),
+                    video_id, fatal=False)
+            except ExtractorError as e:
+                self.report_warning('%s' % e)
+                last_error = e
+                continue
+
+            if shared_data:
                 media = try_get(
-                    additional_data, lambda x: x['graphql']['shortcode_media'],
+                    shared_data,
+                    (lambda x: x['entry_data']['PostPage'][0]['graphql']['shortcode_media'],
+                     lambda x: x['entry_data']['PostPage'][0]['media']),
                     dict)
+            # _sharedData.entry_data.PostPage is empty when authenticated (see
+            # https://github.com/ytdl-org/youtube-dl/pull/22880)
+            if not media:
+                additional_data = self._parse_json(
+                    self._search_regex(
+                        r'window\.__additionalDataLoaded\s*\(\s*[^,]+,\s*({.+?})\s*\)\s*;',
+                        webpage, 'additional data', default='{}'),
+                    video_id, fatal=False)
+                if additional_data:
+                    media = try_get(
+                        additional_data, lambda x: x['graphql']['shortcode_media'],
+                        dict)
+            if media:
+                break
+
+        if last_error:
+            raise last_error
+
         if media:
             video_url = media.get('video_url')
             height = int_or_none(media.get('dimensions', {}).get('height'))
@@ -320,28 +338,37 @@ class InstagramPlaylistIE(InfoExtractor):
 
             # try all of the ways to generate a GIS query, and not only use the
             # first one that works, but cache it for future requests
+            media = None
+            last_error = None
             for gis_tmpl in gis_tmpls:
-                try:
-                    json_data = self._download_json(
-                        'https://www.instagram.com/graphql/query/', uploader_id,
-                        'Downloading JSON page %d' % page_num, headers={
-                            'X-Requested-With': 'XMLHttpRequest',
-                            'X-Instagram-GIS': hashlib.md5(
-                                ('%s:%s' % (gis_tmpl, variables)).encode('utf-8')).hexdigest(),
-                        }, query={
-                            'query_hash': self._QUERY_HASH,
-                            'variables': variables,
-                        })
-                    media = self._parse_timeline_from(json_data)
-                    self._gis_tmpl = gis_tmpl
+                retries = self._downloader.params.get('extractor_retries', 3)
+                count = -1
+                while count < retries:
+                    count += 1
+                    if count:
+                        self.report_warning('Failed to parse response. Retrying...')
+                    try:
+                        json_data = self._download_json(
+                            'https://www.instagram.com/graphql/query/', uploader_id,
+                            'Downloading JSON page %d' % page_num, headers={
+                                'X-Requested-With': 'XMLHttpRequest',
+                                'X-Instagram-GIS': hashlib.md5(
+                                    ('%s:%s' % (gis_tmpl, variables)).encode('utf-8')).hexdigest(),
+                            }, query={
+                                'query_hash': self._QUERY_HASH,
+                                'variables': variables,
+                            })
+                        media = self._parse_timeline_from(json_data)
+                        self._gis_tmpl = gis_tmpl
+                        break
+                    except ExtractorError as e:
+                        self.report_warning('%s' % e)
+                        last_error = e
+                        continue
+                if isinstance(media, dict):
                     break
-                except ExtractorError as e:
-                    # if it's an error caused by a bad query, and there are
-                    # more GIS templates to try, ignore it and keep trying
-                    if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403:
-                        if gis_tmpl != gis_tmpls[-1]:
-                            continue
-                    raise
+            if last_error:
+                raise last_error
 
             edges = media.get('edges')
             if not edges or not isinstance(edges, list):
diff --git a/youtube_dl/extractor/iwara.py b/youtube_dl/extractor/iwara.py
index 907d5fc8b..3ef0eb377 100644
--- a/youtube_dl/extractor/iwara.py
+++ b/youtube_dl/extractor/iwara.py
@@ -1,17 +1,23 @@
 # coding: utf-8
 from __future__ import unicode_literals
 
+import re
+import itertools
+
 from .common import InfoExtractor
-from ..compat import compat_urllib_parse_urlparse
+from ..compat import (
+    compat_urllib_parse_urlparse,
+)
 from ..utils import (
     int_or_none,
     mimetype2ext,
-    remove_end,
     url_or_none,
+    urljoin,
 )
 
 
 class IwaraIE(InfoExtractor):
+    IE_NAME = 'iwara'
     _VALID_URL = r'https?://(?:www\.|ecchi\.)?iwara\.tv/videos/(?P<id>[a-zA-Z0-9]+)'
     _TESTS = [{
         'url': 'http://iwara.tv/videos/amVwUl1EHpAD9RD',
@@ -69,8 +75,7 @@ class IwaraIE(InfoExtractor):
                 'age_limit': age_limit,
             }
 
-        title = remove_end(self._html_search_regex(
-            r'<title>([^<]+)</title>', webpage, 'title'), ' | Iwara')
+        title = self._html_search_regex(r'<title>(.+?) \| Iwara</title>', webpage, 'title')
 
         formats = []
         for a_format in video_data:
@@ -97,3 +102,131 @@ class IwaraIE(InfoExtractor):
             'age_limit': age_limit,
             'formats': formats,
         }
+
+
+class IwaraUserIE(InfoExtractor):
+    IE_NAME = 'iwara:user'
+    _VALID_URL = r'https?://(?:www\.|ecchi\.)?iwara\.tv/users/(?P<id>[^/]+)'
+    _TESTS = [{
+        # cond: videos < 40
+        'note': 'number of all videos page is just 1 page',
+        'url': 'https://ecchi.iwara.tv/users/infinityyukarip',
+        'info_dict': {
+            'title': 'Uploaded videos from Infinity_YukariP',
+            'id': 'infinityyukarip',
+            'uploader': 'Infinity_YukariP',
+            'uploader_id': 'infinityyukarip',
+        },
+        'playlist_mincount': 39,
+    }, {
+        # cond: videos < 10?
+        'note': 'no even all videos page',
+        'url': 'https://ecchi.iwara.tv/users/mmd-quintet',
+        'info_dict': {
+            'title': 'Uploaded videos from mmd quintet',
+            'id': 'mmd-quintet',
+            'uploader': 'mmd quintet',
+            'uploader_id': 'mmd-quintet',
+        },
+        'playlist_mincount': 6,
+    }, {
+        # cond: videos > 40
+        'note': 'has paging',
+        'url': 'https://ecchi.iwara.tv/users/theblackbirdcalls',
+        'info_dict': {
+            'title': 'Uploaded videos from TheBlackbirdCalls',
+            'id': 'theblackbirdcalls',
+            'uploader': 'TheBlackbirdCalls',
+            'uploader_id': 'theblackbirdcalls',
+        },
+        'playlist_mincount': 420,
+    }, {
+        # cond: Japanese chars in URL
+        'note': 'Japanese chars in URL',
+        'url': 'https://ecchi.iwara.tv/users/„Å∂„Åü‰∏º',
+        'info_dict': {
+            'title': 'Uploaded videos from „Å∂„Åü‰∏º',
+            'id': '„Å∂„Åü‰∏º',
+            'uploader': '„Å∂„Åü‰∏º',
+            'uploader_id': '„Å∂„Åü‰∏º',
+        },
+        'playlist_mincount': 170,
+    }]
+
+    @classmethod
+    def suitable(cls, url):
+        return super(IwaraUserIE, cls).suitable(url) and not re.search(
+            r'iwara\.tv/users/[^/]+/videos', url)
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage(url, video_id)
+        videos_url = self._search_regex(r'<a href="(/users/[^/]+/videos)">', webpage, 'all videos url', default=None)
+
+        uploader = self._search_regex(r'<h2>([^<]+?)</h2>', webpage, 'uploader name', default=video_id)
+        title = 'Uploaded videos from %s' % uploader
+
+        if not videos_url:
+            videos_webpage_iter = [webpage]
+        else:
+            videos_base_url = urljoin(url, videos_url)
+
+            def do_paging():
+                for i in itertools.count():
+                    if i == 0:
+                        videos_page_url = videos_base_url
+                    else:
+                        videos_page_url = urljoin(videos_base_url, '?page=%d' % i)
+                    videos_webpage = self._download_webpage(videos_page_url, video_id, note='Downloading video list %d' % (i + 1))
+                    yield videos_webpage
+                    if not '?page=%d' % (i + 1) in videos_webpage:
+                        break
+
+            videos_webpage_iter = do_paging()
+
+        results = []
+        for page in videos_webpage_iter:
+            for x in re.finditer(r'<a href="(/videos/[^"]+)">', page):
+                results.append(x.group(1))
+
+        playlist = self.playlist_result([self.url_result(urljoin(url, x))
+                                         for x in dict.fromkeys(results)], video_id, title)
+        playlist.update({
+            'uploader': uploader,
+            'uploader_id': video_id,
+        })
+        return playlist
+
+
+class IwaraUser2IE(InfoExtractor):
+    IE_NAME = 'iwara:user2'
+    _VALID_URL = r'https?://(?:www\.|ecchi\.)?iwara\.tv/users/(?P<id>[^/]+)/videos'
+    IE_DESC = False  # do not list this
+    _TESTS = [{
+        'note': 'number of all videos page is just 1 page',
+        'url': 'https://ecchi.iwara.tv/users/infinityyukarip/videos',
+        'info_dict': {},
+        'add_ie': [IwaraUserIE.ie_key()],
+    }, {
+        'note': 'no even all videos page',
+        'url': 'https://ecchi.iwara.tv/users/mmd-quintet/videos',
+        'info_dict': {},
+        'add_ie': [IwaraUserIE.ie_key()],
+    }, {
+        'note': 'has paging',
+        'url': 'https://ecchi.iwara.tv/users/theblackbirdcalls/videos',
+        'info_dict': {},
+        'add_ie': [IwaraUserIE.ie_key()],
+    }, {
+        'note': 'Japanese chars in URL',
+        'url': 'https://ecchi.iwara.tv/users/„Å∂„Åü‰∏º/videos',
+        'info_dict': {},
+        'add_ie': [IwaraUserIE.ie_key()],
+    }]
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage(url, video_id, note='Repairing URL')
+        videos_url = self._search_regex(r'<a href="(/users/.+?)"(?: title=".+?")? class="username">', webpage, 'user page url')
+        videos_url = urljoin(url, videos_url)
+        return self.url_result(videos_url, ie=IwaraUserIE.ie_key())
diff --git a/youtube_dl/extractor/javhub.py b/youtube_dl/extractor/javhub.py
new file mode 100644
index 000000000..a486069f9
--- /dev/null
+++ b/youtube_dl/extractor/javhub.py
@@ -0,0 +1,62 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import (
+    ExtractorError,
+    determine_ext,
+    decode_base,
+    scalar_to_bytes,
+    char_replace,
+)
+
+
+class JavhubIE(InfoExtractor):
+    IE_NAME = 'javhub'
+    _VALID_URL = r'https?://(?:ja\.)?javhub\.net/play/(?P<id>[^/]+)'
+
+    B58_TABLE_1 = '23456789ABCDEFGHJKLNMPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz1'
+    B58_TABLE_2 = '789ABCDEFGHJKLNMPQRSTUVWX23456YZabcdefghijkmnopqrstuvwxyz1'
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage(url, video_id)
+
+        data_src = self._search_regex(r'data-src="([^\"]+)"(?:>| data-track)', webpage, 'data-src')
+
+        playapi_post = char_replace(self.B58_TABLE_2, self.B58_TABLE_1, data_src).encode('utf8')
+
+        playapi_response = self._download_json(
+            'https://ja.javhub.net/playapi', video_id,
+            data=b'data=%s' % playapi_post, headers={'Referer': url})
+        mp4_url = playapi_response.get('playurl')
+        if not mp4_url:
+            raise ExtractorError('Server returned invalid data')
+        mp4_url = scalar_to_bytes(decode_base(mp4_url, self.B58_TABLE_2)).decode('utf8')
+
+        result = self._search_json_ld(webpage, video_id)
+        title = self._search_regex(r'<h5[^>]+?>(.+)</h5>', webpage, 'title', default=None)
+        if not title and result.get('title'):
+            title = result['title']
+        if not title:
+            title = self._html_extract_title(webpage, 'html title', default=None, fatal=False)
+
+        if title:
+            title = re.sub(r'^(?:ÁÑ°ÊñôÂãïÁîª|Watch Free Jav)\s+', '', title)
+
+        result.update({
+            'id': video_id,
+            'title': title,
+            'formats': [{
+                'url': mp4_url,
+                'protocol': 'http',
+                'ext': determine_ext(mp4_url),
+                'http_headers': {
+                    'Referer': url,
+                },
+            }],
+            'age_limit': 18,
+        })
+        return result
diff --git a/youtube_dl/extractor/mastodon/__init__.py b/youtube_dl/extractor/mastodon/__init__.py
new file mode 100644
index 000000000..654d2225c
--- /dev/null
+++ b/youtube_dl/extractor/mastodon/__init__.py
@@ -0,0 +1,5 @@
+from __future__ import unicode_literals
+
+from .mastodon import MastodonIE, MastodonUserIE, MastodonUserNumericIE
+
+__all__ = ['MastodonIE', 'MastodonUserIE', 'MastodonUserNumericIE']
diff --git a/youtube_dl/extractor/mastodon/instances.py b/youtube_dl/extractor/mastodon/instances.py
new file mode 100644
index 000000000..b4472f9a8
--- /dev/null
+++ b/youtube_dl/extractor/mastodon/instances.py
@@ -0,0 +1,4023 @@
+# coding: utf-8
+# AUTOMATICALLY GENERATED FILE. DO NOT EDIT.
+# Generated by ./devscripts/make_mastodon_instance_list.py
+from __future__ import unicode_literals
+
+instances = {
+    # list of instances here
+    "0.unicomplex.co",
+    "00x.club",
+    "0q0.xyz",
+    "0xd.0xa.pw",
+    "101010.pl",
+    "104.198.37.171",
+    "10x.sh",
+    "1234.as",
+    "123abc.tokyo",
+    "138.197.38.192",
+    "15m.icolectiva.org",
+    "15o2.de",
+    "18bdsm.club",
+    "1984.cz",
+    "1in9.net",
+    "1monthtodon.sweetduet.info",
+    "1oku.net",
+    "1tp.dpost.jp",
+    "1xx.io",
+    "22330.fr",
+    "2ch.site",
+    "2u4ujpair5gj6u6l.onion",
+    "3.nu",
+    "34.209.101.134",
+    "35.187.159.57",
+    "35o.poker",
+    "35xq.masto.host",
+    "39sounds.net",
+    "3dcgdon.net",
+    "3dscapture.net",
+    "444.hu",
+    "4chan.cmer.fr",
+    "4com.jp",
+    "4estate.media",
+    "4osuke.com",
+    "4qq.org",
+    "4toot.org",
+    "51attack.li",
+    "52.34.46.152",
+    "54.250.191.146",
+    "54.65.17.115",
+    "575don.club",
+    "575toot.jp",
+    "5py.de",
+    "5vsb.com",
+    "6449.work",
+    "6cuelujo3prytx3k.onion",
+    "7144.party",
+    "758.fm",
+    "765ml.com",
+    "88.om",
+    "89and91.space",
+    "89don.net",
+    "9kb.me",
+    "Equestria.Social",
+    "Mastodon.Engineering",
+    "a-kar.in",
+    "a.nom.pl",
+    "a4.io",
+    "aachen.social",
+    "aana.site",
+    "aaron.pk",
+    "aaronparecki.com",
+    "abcdefghijklmnopqrstu-v-w.xyz",
+    "abdl.io",
+    "abdl.link",
+    "ablative.stream",
+    "abuplujmhi.localtunnel.me",
+    "abyss.fun",
+    "acab.land",
+    "acg.mn",
+    "acid.wtf",
+    "activism.openworlds.info",
+    "activitypub.holdmybeer.solutions",
+    "adelaide.group",
+    "admins.mstdn.social",
+    "adney.land",
+    "adventure.octo.im",
+    "aether.re",
+    "afromunkee.xyz",
+    "afu.social",
+    "ahazysukiyakihour.tokyo",
+    "aidon.club",
+    "aipi.social",
+    "ajin.la",
+    "aka.my",
+    "akamanju.com",
+    "akaredon.com",
+    "akashiensis.m.to",
+    "akiba.tokyo.jp",
+    "akibaoh.tech",
+    "akitadon.info",
+    "akko.kalasarn.se",
+    "alcove.bnolet.me",
+    "aleph.land",
+    "alexanderfamily.org",
+    "alexbelgraver.nl",
+    "alive.bar",
+    "alixrossi.corsica",
+    "allnet.hfrc.de",
+    "allpro.social",
+    "allthings.club",
+    "alpc-island.m.to",
+    "amefootdon.com",
+    "amicale.net",
+    "ananc.us",
+    "anarchism.space",
+    "ancr.club",
+    "android-user.club",
+    "angel.innolan.net",
+    "angraecumnote.net",
+    "ani.work",
+    "anidon.com",
+    "anidon.m.to",
+    "animal-crossing.mastportal.info",
+    "animalliberation.social",
+    "anime.country",
+    "anime.mstdn.cloud",
+    "anime.website",
+    "animedj.club",
+    "animedon.tk",
+    "animefun.jp",
+    "anitwitter.com",
+    "anitwitter.moe",
+    "anormallostpod.ovh",
+    "another-guild.com",
+    "antabaka.me",
+    "anticapitalist.party",
+    "antioil.party",
+    "antisocial.narinimous.fr",
+    "aomoridon.info",
+    "ap.kios.cc",
+    "aperi.tube",
+    "appdot.net",
+    "apt.vucica.net",
+    "apub.foldplop.com",
+    "apub.localtunnel.me",
+    "aqua-graphic.blue",
+    "arashi.news",
+    "argos.aquilenet.fr",
+    "arjoranta.fi",
+    "arkham.cafe",
+    "arktos.social",
+    "arm.m.to",
+    "arrowp5210.m.to",
+    "artalley.social",
+    "artik.freemyip.com",
+    "aruk.as",
+    "ascll.net",
+    "ascraeus.org",
+    "asexual.space",
+    "asobi.world",
+    "asonix.dog",
+    "aspiechattr.me",
+    "aspirant.de",
+    "asse.world",
+    "assortedflotsam.com",
+    "astdn.tokyo",
+    "astral.ml",
+    "asukuru.com",
+    "asymptote.club",
+    "atdotatdotat.at",
+    "atilla.im",
+    "atomicfridge.net",
+    "atsumi-munakata.online",
+    "au.toot.gg",
+    "auaudon.kemonox.jp",
+    "augsburg.social",
+    "august-don.site",
+    "aus.social",
+    "ausglam.space",
+    "awaodori.tokyo",
+    "awoo.pub",
+    "aws.afn.social",
+    "awsmp.pl",
+    "aww.wf",
+    "azabani.com",
+    "azu.nsupdate.info",
+    "azumaridon.m.to",
+    "b612.me",
+    "babble.world",
+    "babelut.be",
+    "babymetal.party",
+    "backyard.cloud",
+    "badc0de.net",
+    "bagelbuddy.net",
+    "bagels.pizza",
+    "balkan.fedive.rs",
+    "balserver.xyz",
+    "bam.yt",
+    "banana.dog",
+    "band-mastdn.tokyo",
+    "bangdream.space",
+    "bantha.tatooine.space",
+    "banthamilk.blue",
+    "baptiste.gelez.xyz",
+    "barbatos.social",
+    "barcamp.social",
+    "bash-street-boys-and-girls.com",
+    "basil.asria.jp",
+    "baskedon.net",
+    "batclan.gnusocial.club",
+    "bbqmasters.life",
+    "beach.city",
+    "bear.community",
+    "bedstuy.ddns.net",
+    "beepboop.one",
+    "beer-kuzu.club",
+    "beerbeer.tokyo",
+    "belette.club",
+    "beppo.social",
+    "best-friends.chat",
+    "betamax.video",
+    "betatube.chiantos.org",
+    "betterletter.io",
+    "beudot07.net",
+    "beyblader.top",
+    "bghost.xyz",
+    "bgme.me",
+    "bidule.menf.in",
+    "bigbox.red",
+    "bigdoinks.online",
+    "bigshoulders.city",
+    "bikeshed.party",
+    "bildung.social",
+    "binbou.xyz",
+    "bioregional.cascadiafree.space",
+    "bipolarsupportgroup.net",
+    "birb.site",
+    "birdsite.link",
+    "biscuit.town",
+    "bitcoinersagainst.boats",
+    "bitcoinhackers.org",
+    "bitrot.iscute.ovh",
+    "blackice.online",
+    "blahut.tech",
+    "bldon.net",
+    "blessedgeeks.social",
+    "blo.server-on.net",
+    "blog.fami.ga",
+    "blog.jcg.re",
+    "blogs.jp",
+    "bloodandthunderleviathan.herokuapp.com",
+    "bloodmountain.herokuapp.com",
+    "blovice.bahnhof.cz",
+    "bluex.im",
+    "bms.stoicsounds.jp",
+    "boardgames-mstdn.com",
+    "bona.space",
+    "bong.social",
+    "bonn.social",
+    "boodon.jp",
+    "bookn.me",
+    "booktoot.club",
+    "bookwor.ms",
+    "bot.undernet.uy",
+    "bout.ca",
+    "brand-new-tokyo.m.to",
+    "bremensaki.com",
+    "brrrt.eu",
+    "bruder.space",
+    "bsd.network",
+    "btw.no",
+    "bukatsuiya.m.to",
+    "bungudon.jp",
+    "bunt.social",
+    "buntadon.jp",
+    "bunyip.space",
+    "burners.social",
+    "busaiku.com",
+    "busshi.moe",
+    "c-don.net",
+    "c.im",
+    "c2bdon.net",
+    "ca-os.com",
+    "caaa-reversible.m.to",
+    "cafe.awswan.com",
+    "cafetodon.com",
+    "caffeinatedfriends.club",
+    "cambodia.khmer.love",
+    "camixo.com",
+    "candim.eu",
+    "cannags.com",
+    "canon.mastodon-jp.club",
+    "cantos.social",
+    "capitalism.party",
+    "car-mastodon.link",
+    "cardon.jp",
+    "carnivore.social",
+    "castodon.jp",
+    "cat-from-outer.space",
+    "catdon.jp",
+    "catdon.life",
+    "catgram.jp",
+    "catholicism.rocks",
+    "catspace.xyz",
+    "cawfee.club",
+    "cb.ku.cx",
+    "centerforeveryday.technology",
+    "cerisara.duckdns.org",
+    "cervidae.space",
+    "chabant.social",
+    "chablis.social",
+    "chaos.social",
+    "chaosphere.hostdon.jp",
+    "charafre.noela.moe",
+    "chat.fasil.cloud",
+    "chatboys.me",
+    "chatterly.me",
+    "cheeky.red",
+    "chiawase.tokyo",
+    "chiba.fun",
+    "chiji.space",
+    "chilli.social",
+    "chipsandsalsa.ddns.net",
+    "chirotodon.com",
+    "chirpi.de",
+    "chirps.hoot.town",
+    "chitownmastodon.tech",
+    "chitsuart.online",
+    "chitter.xyz",
+    "chittr.me",
+    "chofudon.tokyo",
+    "chomp.life",
+    "choob.h.etbus.ch",
+    "christodon.com",
+    "chromabits.com",
+    "cigarcabin.com",
+    "ciprit.com",
+    "civiq.social",
+    "ckanhntqir.localtunnel.me",
+    "cksv.jp",
+    "clacks.link",
+    "clammdon.m.to",
+    "claris.cf",
+    "clashroyalemastodon.com",
+    "clc.plus",
+    "cleavey.timttmy.co.uk",
+    "climatejustice.social",
+    "clodostr.at",
+    "clovers.social",
+    "club.nw.fit.ac.jp",
+    "clubclubclub.club",
+    "cmu.party",
+    "cmx.famousedu.com",
+    "cn.tootist.net",
+    "coales.co",
+    "cocoronavi.com",
+    "code4lib.social",
+    "coders.social",
+    "codydh-mastodon.herokuapp.com",
+    "color.town",
+    "colorid.es",
+    "comfeyo.ga",
+    "comicscamp.club",
+    "comm.cx",
+    "comm.network",
+    "commu.xyz",
+    "compass-community.f5.si",
+    "computerfox.xyz",
+    "comunidade.fr",
+    "conjured.space",
+    "conservativecorner.social",
+    "coolmathgam.es",
+    "cooperdon.com",
+    "cosi.town",
+    "cosp.la",
+    "cosp.me",
+    "coste.video",
+    "counter.fedi.social",
+    "cptv.tcp.expert",
+    "crap.today",
+    "crazynoisybizarre.town",
+    "creative.rabbinicorn.com",
+    "creativity.cafe",
+    "critter.town",
+    "crtcollective.com",
+    "crunchywatch.uk",
+    "crusaders.m.to",
+    "crypt.lol",
+    "cryptid.club",
+    "cryptos.cloud",
+    "css-mastodon.ga",
+    "cthulhu.social",
+    "cuddleso.me",
+    "culatello.club",
+    "currydon.com",
+    "cursed.technology",
+    "cute.group",
+    "cutie.space",
+    "cuties.social",
+    "cutls.com",
+    "cuttlefi.sh",
+    "cvuhn4smoar5yb4caraimhg6lo6ynff4ksqfld3nhtrkrz3efafsyhqd.onion",
+    "cyber.hockey",
+    "cybergay.space",
+    "cyberia.jess.coffee",
+    "cybernude.org",
+    "cybr.es",
+    "cybre.space",
+    "cyclodon.net",
+    "cypher.social",
+    "cypherpunks.it",
+    "d.c-cha.cc",
+    "d.mob-con.com",
+    "d20hero.club",
+    "dacha.xmpp.moe",
+    "dajiaweibo.com",
+    "dancemusic-mstdn.com",
+    "darkest-timeline.com",
+    "darkrose.threshold-zero.com",
+    "darksocial.party",
+    "darmstadt.social",
+    "datappl.io",
+    "dazstudio.info",
+    "de.masto.host",
+    "death.horse",
+    "dededon.net",
+    "deep-learning.site",
+    "delusion.ariela.jp",
+    "demanderjustice.social",
+    "democracy.town",
+    "dent.gomertronic.com",
+    "deoke.ml",
+    "derp.cc",
+    "design.vu",
+    "dev.fsck.club",
+    "dev.glitch.social",
+    "dev.inanantoka.com",
+    "dev.knzk.me",
+    "dev.mstdn.club",
+    "dev.tanak3n.xyz",
+    "develry.be",
+    "devlinzed.com",
+    "dflatmajor.social",
+    "dgoeke.io",
+    "dhtls.net",
+    "dialup.express",
+    "dianying.tools",
+    "diaspodon.fr",
+    "die-partei.social",
+    "diefreien.club",
+    "digforfire.org",
+    "digipres.club",
+    "digitalcourage.social",
+    "digitalsoup.eu",
+    "directdon.jp",
+    "dirtytrans.club",
+    "dirus.io",
+    "disnidn.net",
+    "dissidents.social",
+    "dizl.de",
+    "dj.stoicsounds.jp",
+    "djanzu.tokyo",
+    "dnfc.fun",
+    "dobbs.town",
+    "dobesta.com",
+    "docker.masto.pt",
+    "dokushodon.life",
+    "doll.social",
+    "dolphin-town.herokuapp.com",
+    "domdom.tokyo",
+    "don.8mitsu.net",
+    "don.akashi.cloud",
+    "don.anqou.net",
+    "don.asama.net",
+    "don.auri.ga",
+    "don.crakac.com",
+    "don.gomasy.jp",
+    "don.h3z.jp",
+    "don.holy.yokohama",
+    "don.inux39.me",
+    "don.komoro.net",
+    "don.m2hq.net",
+    "don.mamemo.online",
+    "don.matchy.jp",
+    "don.mfz.jp",
+    "don.mlny.info",
+    "don.msng.info",
+    "don.muto.me",
+    "don.neso.tech",
+    "don.nifty.cc",
+    "don.notohiro.com",
+    "don.techfeed.io",
+    "don.wakamesoba98.net",
+    "don.waytt.ml",
+    "don.wiredpunch.com",
+    "donc.m.to",
+    "doncers.com",
+    "dondondon.ga",
+    "donphan.social",
+    "donsuke.biz",
+    "donteatanimals.org",
+    "dontest.ll4u.in",
+    "doshi.fr",
+    "doshidon.com",
+    "dotgr.id",
+    "doug.computer",
+    "doujin.tsuchinoko.tech",
+    "dpsmstdn.com",
+    "dq10.online",
+    "dragon.style",
+    "drake.network",
+    "dreo.io",
+    "dresden.network",
+    "drg.im",
+    "droogz.razvrat.org",
+    "drumbum42.com",
+    "dtp-mstdn.jp",
+    "duanzi.tools",
+    "duck.cafe",
+    "dudesa.me",
+    "dumb.land",
+    "dumbo.kr",
+    "dumbo.nabago.net",
+    "durel.org",
+    "dutchxs.com",
+    "dynlinux.io",
+    "earfolds.com",
+    "eat-inago-inago.com",
+    "eattherich.club",
+    "ebi.tendon.cc",
+    "ecodigital.social",
+    "ecosteader.com",
+    "edge.dtp-mstdn.jp",
+    "edge.mstdn.jp",
+    "ediot.social",
+    "edolas.world",
+    "educadon.jp",
+    "eesti.zone",
+    "ef67.myhome.cx",
+    "ef7.cc",
+    "efdn.club",
+    "egatu.work",
+    "ehr.m.to",
+    "eigadon.net",
+    "eigenmagic.net",
+    "eizi2002.m.to",
+    "eizodon.jp",
+    "ejmastodon.work",
+    "ekuro.jp",
+    "eldon.jp",
+    "eldqkdxhqw.localtunnel.me",
+    "eldritch.cafe",
+    "eldritchworld.nom.es",
+    "electro.social",
+    "elefant.social",
+    "elekk.xyz",
+    "elephant-bike.herokuapp.com",
+    "elephant.bike",
+    "elephant.bluecore.net",
+    "eletusk.club",
+    "elict.net",
+    "elle.garden",
+    "elparque.faccionlatina.org",
+    "emacs.li",
+    "emojidon.global",
+    "empathytech.net",
+    "empty.cafe",
+    "en.osm.town",
+    "ena.m.to",
+    "enbi.es",
+    "energyfintech.io",
+    "engekidon.net",
+    "engineered.space",
+    "ephemeral.glitch.social",
+    "equestria.social",
+    "erch.co",
+    "eriones-mstdn.com",
+    "ery.kr",
+    "esdin.me",
+    "esp.community",
+    "esquite.mazorca.org",
+    "estpls.m.to",
+    "eu1.mastoburn.com",
+    "eupublic.social",
+    "exceptional.party",
+    "experimental.mastodon.home.js4.in",
+    "explosion.party",
+    "facegirl.me",
+    "fairground.moe",
+    "fairy.life",
+    "falafel.win",
+    "family.gobbetti.name",
+    "fandom.ink",
+    "fandom.party",
+    "fanfare.horse",
+    "fart.social",
+    "farts.club",
+    "fastdon.com",
+    "fasterwhen.red",
+    "faxe.startx.de",
+    "fc.fed.im",
+    "fd.bashx.eu",
+    "fedi.absturztau.be",
+    "fedi.z0ne.moe",
+    "fedibird.com",
+    "fedivers.net",
+    "fediverse.worldbuilding.zone",
+    "feed.studio",
+    "fehacking.xyz",
+    "felesitas.cloud",
+    "fellies.social",
+    "fem.social",
+    "fern.surgeplay.com",
+    "ferrus.net",
+    "fetishdon.tk",
+    "fexel.digital",
+    "fiasco.social",
+    "ficta.org",
+    "fido.stephenbasile.com",
+    "fikaverse.club",
+    "fimidi.com",
+    "firedragonvideos.com",
+    "fissionator.com",
+    "fivewords.uk",
+    "fla.red",
+    "flagada.org",
+    "flanintheface.com",
+    "flashfic.stream",
+    "floatie.xyz",
+    "flower.afn.social",
+    "flowercast.me",
+    "fluffyheart.me",
+    "fnordica.de",
+    "fnya.ggtea.org",
+    "fo0bar.org",
+    "foo.sx",
+    "foresdon.jp",
+    "forexdon.org",
+    "fortressofdoom.me",
+    "forum.friendi.ca",
+    "forum.manga.tokyo",
+    "forza7.jp",
+    "fosstodon.org",
+    "fourthestate.social",
+    "fox.masto.host",
+    "foxesare.sexy",
+    "foxiepa.ws",
+    "fr.got-tty.org",
+    "fr.osm.social",
+    "framapiaf.org",
+    "free-le.info",
+    "freebird.social",
+    "freedom.horse",
+    "freedomofpress.rocks",
+    "freehold.earth",
+    "freeradical.zone",
+    "freespeech.firedragonstudios.com",
+    "freespeechextremist.com",
+    "frei.social",
+    "freunde.ma-nic.de",
+    "friend-book.org",
+    "friendica.aegibson.me",
+    "friendica.art3mis.de",
+    "friendica.astatu.berlin",
+    "friendica.cvh-server.de",
+    "friendica.dandys.space",
+    "friendica.eenoog.org",
+    "friendica.henze-online.de",
+    "friendica.i-am-b.org",
+    "friendica.jb-net.us",
+    "friendica.killerkitty.ch",
+    "friendica.me",
+    "friendica.raitisoja.com",
+    "friendica.social.my-wan.de",
+    "friendica.sonatagreen.com",
+    "friendica.westchat.de",
+    "friends.tennis",
+    "friloux.me",
+    "frogmob.life",
+    "frogtown.club",
+    "frontend.social",
+    "fsdon.com",
+    "fsmi.social",
+    "ftb.masto.host",
+    "ftb.moe",
+    "fudanshi.org",
+    "fujisawa.ne.jp",
+    "fujisawadon.com",
+    "fulda.social",
+    "fullstackgeek.tech",
+    "fumi.39.gy",
+    "functional.cafe",
+    "funkwhale.duckdns.org",
+    "funkwhale.ketchupma.io",
+    "funkwhale.lan.sigpipe.me",
+    "fur.cloud",
+    "furries.world",
+    "furry.wtf",
+    "furstdn.jp",
+    "fushinkinzu.club",
+    "futile.life",
+    "fuzjkodon.m.to",
+    "fvdz.localtunnel.me",
+    "fvhp-imastodon14.herokuapp.com",
+    "fvhp-mastodon-fvhp.c9users.io",
+    "fvhp-run.herokuapp.com",
+    "fvyzzxgofk.localtunnel.me",
+    "fx.cafe",
+    "g-miura.jp",
+    "g0v.social",
+    "gadget.inpocket.net",
+    "gageure.fr",
+    "gamba.osaka.jp",
+    "gamecreate.mstdn.cloud",
+    "gameing.zone",
+    "gameliberty.club",
+    "gamelinks007.net",
+    "gamemaking.social",
+    "gamers.social",
+    "games-community.xyz",
+    "garupadon.site",
+    "gauchiste.club",
+    "gaven.social",
+    "gawo.ga",
+    "gay.crime.team",
+    "gay.nsfw.onl",
+    "gaygay.me",
+    "gce.taricorp.net",
+    "gdrsocial.it",
+    "geeknews.chat",
+    "geeks.one",
+    "gelt.cz",
+    "genco.me",
+    "gender.town",
+    "genkai.tk",
+    "geno.social",
+    "gensokyo.cloud",
+    "gensokyo.social",
+    "gensokyo.town",
+    "gentleman-loser.com",
+    "geofox.org",
+    "geotdn.i-red.info",
+    "gg2don.com",
+    "ggtea.org",
+    "gifudon.jp",
+    "gigamastodon.com",
+    "giggity.ca",
+    "gingadon.com",
+    "girlcock.club",
+    "gla.fit",
+    "glaceon.social",
+    "glamdon.com",
+    "glammr.us",
+    "glasgow.social",
+    "glitch.social",
+    "glitterkitten.co.uk",
+    "gnu.acclaro.digital",
+    "gnu.town",
+    "gnusocial.codingquark.com",
+    "gnusocial.progval.net",
+    "gnusocial.tamanoir.foucry.net",
+    "gnutan.club",
+    "go.nsa.li",
+    "go5.dev",
+    "goatdaddy.net",
+    "godforsaken.website",
+    "gohan.org",
+    "goldandblack.club",
+    "goldandblack.xyz",
+    "golfdon.net",
+    "gonsphere.tk",
+    "gonta.net",
+    "good-dragon.com",
+    "gossip.hostsharing.net",
+    "gould.cx",
+    "gr.social",
+    "grand-duchy.net",
+    "graz.social",
+    "greek.fdi.ucm.es",
+    "gregtaole.fr",
+    "greylog.ru",
+    "grimoire.kanzakiranko.jp",
+    "grragaa.yuzu.tk",
+    "grurple.org",
+    "gs.2hu-ch.org",
+    "gs.anarres.info",
+    "gs.archaeme.tech",
+    "gs.lgbtpzn.org",
+    "gs.mariovavti.com",
+    "gs.monkeystew.net",
+    "gs.yvt.jp",
+    "gs.zy.lc",
+    "guac.live",
+    "gulp.cafe",
+    "gundam.masto.host",
+    "gunma.in",
+    "guu.so",
+    "gyudon.live",
+    "h.ftqq.com",
+    "h.kher.nl",
+    "h.tty.pw",
+    "h29ztdn.tottoriscout.org",
+    "hackadon.space",
+    "hackers.town",
+    "hackertribe.io",
+    "hackerzlab.net",
+    "haisai.m.to",
+    "hakendon.net",
+    "hakimus.de",
+    "hakuhodon.com",
+    "halki.info",
+    "hamamyu.jp",
+    "hamradio.space",
+    "hamtter.net",
+    "handon.club",
+    "happy-oss.y-zu.org",
+    "happy-social-life.fudanchii.net",
+    "happyband.es",
+    "hartley.cc",
+    "haruhi-mstdn.club",
+    "hasetsu.net",
+    "hash.my",
+    "hates.company",
+    "haxors.com",
+    "hckr.no",
+    "hearthtodon.com",
+    "hecatoncheir.net",
+    "heio.co",
+    "heislandmine.work",
+    "hello.2heng.xin",
+    "hellsite.site",
+    "henrygarner.im",
+    "herd.redfish.ca",
+    "herkenhoff.eu",
+    "herkenhoff.nl",
+    "hi.spooky.camp",
+    "hibi.ga",
+    "hiddan.net",
+    "higumadon.club",
+    "hilaritas.co",
+    "himalia.dnshome.de",
+    "himitsudon.com",
+    "hinomarudon.com",
+    "hiroga.net",
+    "hispagatos.space",
+    "hispatodon.club",
+    "hitb-masto.herokuapp.com",
+    "hitlers.win",
+    "hiveway.creatodon.online",
+    "hiveway.net",
+    "hiya.mizucoffee.net",
+    "hk.everydayedward.com",
+    "hobaugh.social",
+    "hodl.social",
+    "hogehoge.toycode.com",
+    "hojiro.herokuapp.com",
+    "hokutodon.co",
+    "hokutodon.com",
+    "holeliquors.com",
+    "hollandsepod.nl",
+    "homex.bounceme.net",
+    "homoo.social",
+    "hoot.video",
+    "horaidon.tokyo",
+    "horiedon.com",
+    "horizon.ayaka.moe",
+    "hostsharing.coop",
+    "hostux.coffee",
+    "hostux.social",
+    "hoteltoday.tokyo",
+    "hotmail.com",
+    "hqpf.site",
+    "hs-mastodon.root-s.xyz",
+    "hsgw.m.to",
+    "hstcsl.jp",
+    "hub.bka.li",
+    "hub.heraut.eu",
+    "hub.kosmospora.de",
+    "hub.mtf.party",
+    "hub.opensocial.africa",
+    "hub.plyuk.rocks",
+    "hub.sakuragawa.moe",
+    "hub.solarnight.net",
+    "hub.spaz.org",
+    "hub.vilarejo.pro.br",
+    "hub.volse.no",
+    "hub.xensen.net",
+    "hub.zilla.tech",
+    "hub.zy.lc",
+    "hubz.hfrc.de",
+    "hubzilla.0xfa.de",
+    "hubzilla.com.br",
+    "hubzilla.eu",
+    "hubzilla.sysadmindork.com",
+    "hulvr.com",
+    "hw.aqr.af",
+    "hwdon.jp",
+    "hyan.ink",
+    "hydroxyquinol.net",
+    "hyoga9f.m.to",
+    "hyperion.social",
+    "hyuki.herokuapp.com",
+    "hz.macgirvin.com",
+    "i.write.codethat.sucks",
+    "iamveryti.red",
+    "ibgr.m.to",
+    "ichmagdiesen.link",
+    "icosahedron.website",
+    "idevs.id",
+    "idf.social",
+    "idoldon.com",
+    "idolon.club",
+    "idreamof.ryukyu",
+    "ieji.de",
+    "ifrit.gaia.ff14-mstdn.xyz",
+    "ifwo.eu",
+    "igouv.fr",
+    "igreally.masto.host",
+    "igreally.social",
+    "ijn-dd.nl",
+    "ika.julika.jp",
+    "ika.moe",
+    "ika.queloud.net",
+    "ikasamastodon.k503.jp",
+    "im-in.space",
+    "imaginair.es",
+    "imastodon.net",
+    "imastodon.org",
+    "imcpwn.com",
+    "inari.opencocon.org",
+    "inbeta.community",
+    "indie.chat",
+    "indie.computer",
+    "indieweb.social",
+    "indigo.zone",
+    "indxio.info",
+    "infinimatix.net",
+    "infosec.exchange",
+    "ingeodon.com",
+    "inkopolis.cafe",
+    "inscope.social",
+    "insoumis-es.net",
+    "instance.business",
+    "internaut.me",
+    "intersect.hackershack.net",
+    "introvert.party",
+    "inumimi.net",
+    "invisible.cat",
+    "ioc.exchange",
+    "iopla.fr",
+    "ipno.us",
+    "ipv6.social.konosuke.jp",
+    "is.a.qute.dog",
+    "is.nota.live",
+    "isaomstdnwal01.azurewebsites.net",
+    "ischool.social",
+    "iscute.moe",
+    "iscute.ovh",
+    "ishikaridon.jp",
+    "iskr.dyndns.org",
+    "ispv-7.com",
+    "itabashi.0j0.jp",
+    "itsuo.jp",
+    "ity.org",
+    "ivrianochi.com",
+    "iwami-mastodon.herokuapp.com",
+    "iwatedon.net",
+    "ix.hfrc.de",
+    "iyher.club",
+    "iymtkfkwhc.localtunnel.me",
+    "iys.io",
+    "jan.m.to",
+    "janogdon.net",
+    "japaon.cf",
+    "jawns.club",
+    "jbstation165.asuscomm.com",
+    "jcmenhera.m.to",
+    "jelqing.men",
+    "jidoridon.com",
+    "jihou-tyan.m.to",
+    "jingo.drtecus.com",
+    "jingo.social",
+    "jitakurack.chotto.moe",
+    "jlg.link",
+    "john.town",
+    "jojomonvr.com",
+    "jonleibowitz.social",
+    "joraku.kyoto.jp",
+    "jorts.horse",
+    "josjosjos369.social",
+    "josli.social",
+    "journo.social",
+    "jp-mastodon.com",
+    "jpnews.site",
+    "js4.in",
+    "jstd.me",
+    "jstdn.herokuapp.com",
+    "jubi.life",
+    "julika.jp",
+    "junkhub.org",
+    "justicewarrior.social",
+    "k0ta.net",
+    "kabutodon.com",
+    "kafuka.me",
+    "kagayaki.m.to",
+    "kagrumez.lerk.io",
+    "kaisendon.masto.host",
+    "kakijin.com",
+    "kamiyacho.net",
+    "kancolle-yokosuka.xyz",
+    "kancolle.social",
+    "kanmoku.net",
+    "kanoa.de",
+    "karakara.m.to",
+    "karass.ideali.sh",
+    "karl.marx.pm",
+    "kashiwadon.net",
+    "kasuka.hostdon.jp",
+    "katasumi.no-monogatari.com",
+    "katu.cursed-images.xyz",
+    "kawasaki-city.social",
+    "kawen.space",
+    "kazvam.com",
+    "kbtdn.sweak.net",
+    "kcdon.jp",
+    "kcmo.social",
+    "keiba.social",
+    "keithjgrant.com",
+    "kekkai.net",
+    "kelnet.social",
+    "kemoner-don.tokyo",
+    "kemono-friends.masto.host",
+    "kemonodon.club",
+    "kicou.info",
+    "kigdon.xyz",
+    "killmi.st",
+    "kiminona.co",
+    "kimonosou.tokyo",
+    "kindred.at",
+    "kindred.haus",
+    "kindtech.visonthe.net",
+    "kingdown.social",
+    "kintonedon.com",
+    "kinugasa.me",
+    "kiraako.work",
+    "kirakiratter.com",
+    "kirapower.ichigo-hoshimiya.com",
+    "kirishimalab21.xyz",
+    "kiritan.work",
+    "kitadon.com",
+    "kith.kitchen",
+    "kittenfactory.eccentriccat.com",
+    "kitty.town",
+    "kiwaitsu.m.to",
+    "kloudboy.xyz",
+    "kmail.i-red.info",
+    "kmmtn.com",
+    "kmogue.eldritch.cafe",
+    "kmyd.moe",
+    "knar.ph",
+    "knotes.club",
+    "knzkoniisan.club",
+    "ko.gginin.today",
+    "koba.kokoa.site",
+    "koetodon.com",
+    "kokedon.xyz",
+    "kokokokko.com",
+    "kokuchidon.net",
+    "kolbe-liang2.m.to",
+    "kolektiva.social",
+    "komula.space",
+    "konf.yt",
+    "konkat.jp",
+    "konkatsu.mastd.me",
+    "kopiti.am",
+    "koreadon.com",
+    "koreus.social",
+    "koshiendon.com",
+    "kosmos.social",
+    "kottman.xyz",
+    "koutyan.net",
+    "koya.m.to",
+    "koyu.space",
+    "koyuston.tk",
+    "kpodon.jp",
+    "kpop.social",
+    "krefeld.club",
+    "krefeld.life",
+    "kroo.network",
+    "ksalsk.m.to",
+    "ktstdn.m.to",
+    "kuhatweet.kallelaine.com",
+    "kumi.zone",
+    "kuncheff.social",
+    "kurage.cc",
+    "kurakake.net",
+    "kurosawa-ruby.xyz",
+    "kvby.xyz",
+    "kwasumire.info",
+    "kwen.to",
+    "kyokoi.ddns.net",
+    "kyot.me",
+    "kyoto-citygrid.org",
+    "kyunkyun.moe",
+    "kyushumura.global",
+    "l-w-i-don.net",
+    "la1.jp",
+    "labotadon.net",
+    "labs.tachibana.cool",
+    "lagom.agoni.news",
+    "lain.haus",
+    "laserdisc.party",
+    "latedon.sui-hei.net",
+    "lavraievie.social",
+    "layer8.space",
+    "lazyatom.social",
+    "leaf.style",
+    "learn-english.site",
+    "lecker.coffee",
+    "lediver.se",
+    "left.community",
+    "legadolibre.mamalibre.com.ar",
+    "lejaua6.club",
+    "lermer.nl",
+    "lervo.de",
+    "lesbiab.masto.host",
+    "lesbiab.space",
+    "lesbian.nsfw.onl",
+    "lesbianschool.com",
+    "lesswrong.io",
+    "letsalllovela.in",
+    "letsrulethe.world",
+    "leviathan.robot-disco.net",
+    "lewd.town",
+    "lexpierce.social",
+    "lgbt.io",
+    "lgbt6.club",
+    "lgbtjcat.com",
+    "lgbtq.cool",
+    "lgbtqia.is",
+    "lhub.social",
+    "libcore.org",
+    "libellula.criptica.org",
+    "liber.cx",
+    "liberalism.masto.host",
+    "libertalia.world",
+    "libertarian.chat",
+    "librenet.xyz",
+    "libretooth.gr",
+    "licca.m.to",
+    "lilydon.com",
+    "linaro.tech",
+    "linernotes.club",
+    "linuxjobs.social",
+    "linuxrocks.online",
+    "listen.gallery",
+    "literatur.social",
+    "livers.jp",
+    "livesconnect.com",
+    "llii.ga",
+    "local.iwamidon.tech",
+    "localmstcdn.anima-mystica.org",
+    "lofi.one",
+    "login.m.to",
+    "loli.mk",
+    "loliravioli.club",
+    "lomo.mstdn.tokyo",
+    "lopezjuan.com",
+    "lor.sh",
+    "lord.sh",
+    "lordinateur.tech",
+    "lorraine.blue",
+    "lostallofmy.faith",
+    "lou.lt",
+    "lovelinus.club",
+    "lovelive-anime.tk",
+    "loves.pizza",
+    "lsngl.us",
+    "ltdon.herokuapp.com",
+    "ltu.social",
+    "lucida-note.info",
+    "ludosphere.fr",
+    "lux.blue",
+    "m.0xf.at",
+    "m.4ac.me",
+    "m.aky.sh",
+    "m.aqr.af",
+    "m.azusa.san.moe",
+    "m.baer.im",
+    "m.bonzoesc.net",
+    "m.cmx.im",
+    "m.devolio.net",
+    "m.divita.eu",
+    "m.fal.moe",
+    "m.fork.sh",
+    "m.g3l.org",
+    "m.geraffel.net",
+    "m.gochiusa.net",
+    "m.gretaoto.ca",
+    "m.gxc.io",
+    "m.hitorino.moe",
+    "m.hxbus.net",
+    "m.ir1s.com",
+    "m.iskr.jp",
+    "m.jmshyns.com",
+    "m.jugg.xyz",
+    "m.kagucho.net",
+    "m.kemoshi.co",
+    "m.kmr.me",
+    "m.laitues.net",
+    "m.livetube.cc",
+    "m.loovto.net",
+    "m.massy.city",
+    "m.mig5.net",
+    "m.moe.cat",
+    "m.moriya.faith",
+    "m.mrx.im",
+    "m.mtjm.eu",
+    "m.mushus.net",
+    "m.nda.li",
+    "m.netwkr.net",
+    "m.nintendojo.fr",
+    "m.opsnotice.xyz",
+    "m.paulbutler.org",
+    "m.pira.jp",
+    "m.planz.asia",
+    "m.plop.cc",
+    "m.pmmm.jp",
+    "m.pref.yokohama",
+    "m.r4o.jp",
+    "m.relaychat.party",
+    "m.rex.gs",
+    "m.rthome.me",
+    "m.rweekly.org",
+    "m.sixfoisneuf.fr",
+    "m.socialjustice.engineering",
+    "m.sysi.work",
+    "m.test.maas.network",
+    "m.th23.org",
+    "m.themsp.org",
+    "m.u7fa9.org",
+    "m.uncate.org",
+    "m1.masdon.life",
+    "m2.ma3ki.net",
+    "m4570.xyz",
+    "m4sk.in",
+    "m57d.hun73r.xyz",
+    "m6n.jp",
+    "m6n.kigurumi.fun",
+    "m6n.onsen.tech",
+    "ma.fono.jp",
+    "ma.luna.fyi",
+    "machida.yokohama",
+    "machteburch.social",
+    "madiatorownia.nohost.me",
+    "madrid.how",
+    "maeon4n76fxp65og.onion",
+    "mag-mstdn.tki.jp",
+    "magi.systems",
+    "magic-tail.net",
+    "magicalgirl.party",
+    "magik.me",
+    "magurodon.net",
+    "mah.m.to",
+    "maho.app",
+    "mainburg.hallertau.social",
+    "maiome.xyz",
+    "makerdon.org",
+    "makesocialboops.com",
+    "makestuff.club",
+    "makiroll.space",
+    "malfunctioning.technology",
+    "malu.today",
+    "maly.io",
+    "mammochon.survivebox.net",
+    "mammout.bzh",
+    "mammouth.inframed.net",
+    "mammouth.quebec",
+    "mammut.buzz",
+    "mammut.fsck.jp",
+    "mammut.red",
+    "mammut.space",
+    "mammut.zasha.style",
+    "mammuth.ryo-saeba.xyz",
+    "mamoot.party",
+    "mamot.fr",
+    "mamut.social",
+    "mandodon.com",
+    "mangadon.info",
+    "mangadon.net",
+    "manhater.io",
+    "manhole.club",
+    "mania.systems",
+    "manimani.m.to",
+    "manowar.social",
+    "manx.social",
+    "mao.daizhige.org",
+    "maoh.company",
+    "marcincieslak.com",
+    "markdon.ml",
+    "maron.blue",
+    "martian.social",
+    "martymcgui.re",
+    "marupost.jp",
+    "marxism.party",
+    "marxist.social",
+    "mas.4ray.co",
+    "mas.alixrossi.corsica",
+    "mas.home.monsiteinternet.org",
+    "mas.maerou.com",
+    "mas.phiy.me",
+    "mas.to",
+    "masatodon.click",
+    "masdis.com",
+    "maskneko.com",
+    "mast.division25.xyz",
+    "mast.e2.lc",
+    "mast.eu.org",
+    "mast.kaikretschmann.de",
+    "mast.moe",
+    "mast.newfield1001.de",
+    "mast.tayori.org",
+    "mast.tokyo",
+    "mast.udn.jp",
+    "mast.udon.moe",
+    "mast.webschoolchallenge.com",
+    "mast0d0n.fr",
+    "mastadon.tescher.me",
+    "mastchan.org",
+    "mastdn.lovesaemi.daemon.asia",
+    "mastdn.me",
+    "mastdn.okinawa",
+    "mastedm.club",
+    "masthead.social",
+    "masto.beer",
+    "masto.cryptoworld.is",
+    "masto.cuddlr.org",
+    "masto.dog",
+    "masto.donte.com.br",
+    "masto.downey.net",
+    "masto.fslhome.org",
+    "masto.ga",
+    "masto.gb300.info",
+    "masto.io",
+    "masto.jews.international",
+    "masto.nine-hells.net",
+    "masto.ninja",
+    "masto.pt",
+    "masto.raildecake.fr",
+    "masto.razrnet.fr",
+    "masto.rezo2france.fr",
+    "masto.rxc.gdn",
+    "masto.se",
+    "masto.tech",
+    "masto.tttie.ga",
+    "masto.unixporn.pro",
+    "masto.yellowkeycard.net",
+    "mastoc.net",
+    "mastochizuru.xyz",
+    "mastod.life",
+    "mastoden.com",
+    "mastodev.shark.moe",
+    "mastodo.no",
+    "mastodol.jp",
+    "mastodon-anigif.net",
+    "mastodon-choco.jp",
+    "mastodon-dev.oresys.nagoya",
+    "mastodon-forstee.herokuapp.com",
+    "mastodon-hss.ml",
+    "mastodon-jp.org",
+    "mastodon-matsuura.herokuapp.com",
+    "mastodon-nabeen.herokuapp.com",
+    "mastodon-nagano.info",
+    "mastodon-ovh.social",
+    "mastodon-robamimi.club",
+    "mastodon-srv.gq",
+    "mastodon-staging.figel.at",
+    "mastodon-startup.org",
+    "mastodon-test-1.ohineri.me",
+    "mastodon-test.wirednet.jp",
+    "mastodon-tiray.me",
+    "mastodon-toyama.xyz",
+    "mastodon.0-z-0.com",
+    "mastodon.103.ms",
+    "mastodon.1984.cz",
+    "mastodon.2printers1cups.com",
+    "mastodon.a3wks.info",
+    "mastodon.acc.sunet.se",
+    "mastodon.acc.umu.se",
+    "mastodon.acgdoge.net",
+    "mastodon.acticiel.org",
+    "mastodon.ajy.co",
+    "mastodon.aki017.info",
+    "mastodon.akita.jp",
+    "mastodon.al",
+    "mastodon.alexrio.fr",
+    "mastodon.alfheim.ca",
+    "mastodon.alnair.blue",
+    "mastodon.anti-globalism.org",
+    "mastodon.antoineve.me",
+    "mastodon.anttirt.net",
+    "mastodon.appne.com",
+    "mastodon.ar.al",
+    "mastodon.art",
+    "mastodon.asia",
+    "mastodon.astrr.ru",
+    "mastodon.atikoro.net",
+    "mastodon.aventer.biz",
+    "mastodon.babycastles.com",
+    "mastodon.baconpotato.net",
+    "mastodon.baeck.at",
+    "mastodon.baucum.me",
+    "mastodon.bayern",
+    "mastodon.bbqmasters.life",
+    "mastodon.bearmulk.us",
+    "mastodon.benoit-alessandroni.fr",
+    "mastodon.berlin",
+    "mastodon.bgta.net",
+    "mastodon.bida.im",
+    "mastodon.bitbank.cc",
+    "mastodon.black-cats.fr",
+    "mastodon.blade.red",
+    "mastodon.blake-hofer.net",
+    "mastodon.blanboom.org",
+    "mastodon.blue",
+    "mastodon.brussels",
+    "mastodon.bunkerchan.xyz",
+    "mastodon.bushu-denno.com",
+    "mastodon.business",
+    "mastodon.c-view.club",
+    "mastodon.camp",
+    "mastodon.capitaines.fr",
+    "mastodon.cardina1.red",
+    "mastodon.carper.ca",
+    "mastodon.cc",
+    "mastodon.cemea.org",
+    "mastodon.center",
+    "mastodon.cfapps.io",
+    "mastodon.cfw4.tk",
+    "mastodon.cgx.me",
+    "mastodon.chaosfield.at",
+    "mastodon.chaospott.de",
+    "mastodon.cheesepark.io",
+    "mastodon.chilos.jp",
+    "mastodon.chocolicornes.org",
+    "mastodon.cipherbliss.com",
+    "mastodon.cisti.org",
+    "mastodon.civleaguejp.net",
+    "mastodon.cl",
+    "mastodon.cloud",
+    "mastodon.cloud-ace.jp",
+    "mastodon.club",
+    "mastodon.co.nz",
+    "mastodon.codes",
+    "mastodon.codingfield.com",
+    "mastodon.coffee",
+    "mastodon.coletivos.org",
+    "mastodon.com.br",
+    "mastodon.com.de",
+    "mastodon.com.tw",
+    "mastodon.communick.com",
+    "mastodon.compassion.online",
+    "mastodon.conradkramer.com",
+    "mastodon.conxtor.com",
+    "mastodon.cool",
+    "mastodon.cosmicanimal.jp",
+    "mastodon.crazynewworld.net",
+    "mastodon.crossfamilyweb.com",
+    "mastodon.crypt.lol",
+    "mastodon.ctseuro.com",
+    "mastodon.cusae.com",
+    "mastodon.cyber-tribal.com",
+    "mastodon.daiko.fr",
+    "mastodon.danbruno.net",
+    "mastodon.davidpeach.co.uk",
+    "mastodon.dcc-jpl.com",
+    "mastodon.ddns.net",
+    "mastodon.deafpros.com",
+    "mastodon.dekloo.net",
+    "mastodon.delurk.com",
+    "mastodon.derbuihan.tk",
+    "mastodon.dereferenced.org",
+    "mastodon.design",
+    "mastodon.desmu.fr",
+    "mastodon.desync.net",
+    "mastodon.die-partei-reutlingen.de",
+    "mastodon.diglateam3.com",
+    "mastodon.direct",
+    "mastodon.dissem.ch",
+    "mastodon.dominicdewolfe.com",
+    "mastodon.dqfan.club",
+    "mastodon.dremtech.fr",
+    "mastodon.dupr.at",
+    "mastodon.dustinwilson.com",
+    "mastodon.dynamic.s-tomo.jp",
+    "mastodon.e217.net",
+    "mastodon.earth",
+    "mastodon.ebiryu.tech",
+    "mastodon.ecb-third-games.ga",
+    "mastodon.edvgarbe.de",
+    "mastodon.elephas.cloud",
+    "mastodon.eliotberriot.com",
+    "mastodon.elmit.com",
+    "mastodon.engineering",
+    "mastodon.eric.ovh",
+    "mastodon.etalab.gouv.fr",
+    "mastodon.ethibox.fr",
+    "mastodon.eus",
+    "mastodon.euskirchen.digital",
+    "mastodon.everydayimshuflin.com",
+    "mastodon.evolix.org",
+    "mastodon.exchange",
+    "mastodon.expert",
+    "mastodon.famiapp.com",
+    "mastodon.fedi.quebec",
+    "mastodon.fish",
+    "mastodon.fishing",
+    "mastodon.flights",
+    "mastodon.fluffel.io",
+    "mastodon.forza7.jp",
+    "mastodon.foundry.wepn.social",
+    "mastodon.freetls.fastly.net",
+    "mastodon.frontierfoundry.co",
+    "mastodon.fruitopology.net",
+    "mastodon.ftfl.ca",
+    "mastodon.fun",
+    "mastodon.futabachannel.com",
+    "mastodon.gamecircle.nova.0am.jp",
+    "mastodon.gamedev.place",
+    "mastodon.garbage-juice.com",
+    "mastodon.gargantia.fr",
+    "mastodon.gcp.dmmlabs.com",
+    "mastodon.geekshell.fr",
+    "mastodon.gegeweb.org",
+    "mastodon.geofox.org",
+    "mastodon.gg",
+    "mastodon.gifclip.info",
+    "mastodon.gnieh.org",
+    "mastodon.gocco.co.jp",
+    "mastodon.gougere.fr",
+    "mastodon.green",
+    "mastodon.greenpeace.ch",
+    "mastodon.groningendigitalcity.com",
+    "mastodon.hackerlab.fr",
+    "mastodon.hafen.io",
+    "mastodon.hartley.cc",
+    "mastodon.hasameli.com",
+    "mastodon.hato.online",
+    "mastodon.havas.jp",
+    "mastodon.hekki.info",
+    "mastodon.herebedragons.io",
+    "mastodon.hexaly.se",
+    "mastodon.hgsn.info",
+    "mastodon.hirokinko.net",
+    "mastodon.hitotsu.me",
+    "mastodon.home.js4.in",
+    "mastodon.homeip.net",
+    "mastodon.host",
+    "mastodon.hovkluster.eu",
+    "mastodon.hovkluster.se",
+    "mastodon.hugopoi.net",
+    "mastodon.husq.tk",
+    "mastodon.hutin-moise.com",
+    "mastodon.hydrosaas.com",
+    "mastodon.i2p",
+    "mastodon.ikidane-nippon.com",
+    "mastodon.immae.eu",
+    "mastodon.indoorsman.ee",
+    "mastodon.infra.de",
+    "mastodon.ingress-enl.jp",
+    "mastodon.internot.no",
+    "mastodon.iriseden.eu",
+    "mastodon.irish",
+    "mastodon.itunix.eu",
+    "mastodon.iut-larochelle.fr",
+    "mastodon.izzz.fr",
+    "mastodon.jalgi.eus",
+    "mastodon.jamesmwright.com",
+    "mastodon.jtwp470.net",
+    "mastodon.juggler.jp",
+    "mastodon.jumpgate.us",
+    "mastodon.ka0.co",
+    "mastodon.kakunpc.com",
+    "mastodon.kaonet-fr.net",
+    "mastodon.kebree.fr",
+    "mastodon.ketchupma.io",
+    "mastodon.kida.io",
+    "mastodon.kinesin.org",
+    "mastodon.kirimi.net",
+    "mastodon.kleph.eu",
+    "mastodon.kosebamse.com",
+    "mastodon.koteitan.com",
+    "mastodon.kresco.de",
+    "mastodon.kujiu.org",
+    "mastodon.kumano-ryo.tech",
+    "mastodon.kuroserver.net",
+    "mastodon.kuzuore.net",
+    "mastodon.kxn4t.tech",
+    "mastodon.la",
+    "mastodon.lachman.tk",
+    "mastodon.land",
+    "mastodon.lang-mfr.de",
+    "mastodon.lat",
+    "mastodon.lavergne.online",
+    "mastodon.le-gras.fr",
+    "mastodon.le-palantir.com",
+    "mastodon.lemarchand.io",
+    "mastodon.leonadi.de",
+    "mastodon.lerelaisdupatriote.fr",
+    "mastodon.lermer.nl",
+    "mastodon.lertsenem.com",
+    "mastodon.lesamarien.fr",
+    "mastodon.libre-entreprise.com",
+    "mastodon.libresilicon.com",
+    "mastodon.lignux.com",
+    "mastodon.limilo.com",
+    "mastodon.link",
+    "mastodon.linuxbox.ninja",
+    "mastodon.linuxkompis.se",
+    "mastodon.linuxquimper.org",
+    "mastodon.lndvll.se",
+    "mastodon.lol",
+    "mastodon.loomcom.com",
+    "mastodon.lt",
+    "mastodon.lu",
+    "mastodon.lubar.me",
+    "mastodon.luxiferapp.com",
+    "mastodon.lw1.at",
+    "mastodon.lyokotech.com",
+    "mastodon.m0t0k1ch1.com",
+    "mastodon.m3.nsk.io",
+    "mastodon.machique.st",
+    "mastodon.macsnet.cz",
+    "mastodon.madrid",
+    "mastodon.maho-do.jp",
+    "mastodon.mail.at",
+    "mastodon.mania.systems",
+    "mastodon.maop.mx",
+    "mastodon.maromaro.co.jp",
+    "mastodon.matcha-soft.com",
+    "mastodon.matemann.de",
+    "mastodon.me.uk",
+    "mastodon.mechanicalmischief.com",
+    "mastodon.megahiza.website",
+    "mastodon.megkuma.com",
+    "mastodon.mensoif.cf",
+    "mastodon.mezashi.info",
+    "mastodon.mfjt.jp",
+    "mastodon.mg",
+    "mastodon.mhserver.xyz",
+    "mastodon.michaeljdeeb.com",
+    "mastodon.microdata.co.uk",
+    "mastodon.migennes.net",
+    "mastodon.milliondoubts.com",
+    "mastodon.mit.edu",
+    "mastodon.mita.me",
+    "mastodon.mitsu9.com",
+    "mastodon.mocademia.jp",
+    "mastodon.moruorange.com",
+    "mastodon.moshsh-mate.com",
+    "mastodon.motcha.tech",
+    "mastodon.motorcycling.life",
+    "mastodon.mpy.ovh",
+    "mastodon.mserdar.net",
+    "mastodon.mtndevelopment.com",
+    "mastodon.multimob.be",
+    "mastodon.murata.jp.net",
+    "mastodon.my",
+    "mastodon.nakanod.net",
+    "mastodon.narazaki.net",
+    "mastodon.neilcastelino.com",
+    "mastodon.nekomimi.jp",
+    "mastodon.nerdsniping.net",
+    "mastodon.nestegg.net",
+    "mastodon.nesven.eu",
+    "mastodon.newtown.chiba.jp",
+    "mastodon.night.coffee",
+    "mastodon.nil.nu",
+    "mastodon.nine-hells.net",
+    "mastodon.ninetailed.uk",
+    "mastodon.nitech.online",
+    "mastodon.niu.ne.jp",
+    "mastodon.nl",
+    "mastodon.no-cloud.fr",
+    "mastodon.nogods.be",
+    "mastodon.noizycat.com",
+    "mastodon.nopdev.com",
+    "mastodon.not-enough.space",
+    "mastodon.now.im",
+    "mastodon.nrev.org",
+    "mastodon.nu",
+    "mastodon.nullfu.org",
+    "mastodon.nuzgo.net",
+    "mastodon.nzoss.nz",
+    "mastodon.observer",
+    "mastodon.oi7.de",
+    "mastodon.okdtsk.info",
+    "mastodon.okkikki.tokyo",
+    "mastodon.on-o.com",
+    "mastodon.online",
+    "mastodon.ooxxcc.com",
+    "mastodon.open.legal",
+    "mastodon.opencloud.lu",
+    "mastodon.opportunis.me",
+    "mastodon.oresys.nagoya",
+    "mastodon.org.uk",
+    "mastodon.organisationtressecrete.com",
+    "mastodon.osaka",
+    "mastodon.oss.nagoya",
+    "mastodon.osyakasyama.me",
+    "mastodon.owls.io",
+    "mastodon.p2pquake.net",
+    "mastodon.paas.jp",
+    "mastodon.parleur.net",
+    "mastodon.partecipa.digital",
+    "mastodon.partofthething.com",
+    "mastodon.pasteq.fr",
+    "mastodon.pe",
+    "mastodon.pericles.world",
+    "mastodon.pirateparty.be",
+    "mastodon.pl",
+    "mastodon.plein.org",
+    "mastodon.pne.club",
+    "mastodon.postmoderns.info",
+    "mastodon.potager.org",
+    "mastodon.pro",
+    "mastodon.productionservers.net",
+    "mastodon.project47.xyz",
+    "mastodon.puffin.puffin.rocks",
+    "mastodon.qowala.org",
+    "mastodon.qth.fr",
+    "mastodon.qubena.com",
+    "mastodon.radicalityincident.com",
+    "mastodon.radio",
+    "mastodon.radiofreerobotron.net",
+    "mastodon.radiosonline.cloud",
+    "mastodon.rainbownerds.de",
+    "mastodon.randulo.com",
+    "mastodon.ranranhome.site",
+    "mastodon.raryosu.info",
+    "mastodon.recurse.com",
+    "mastodon.red",
+    "mastodon.redpan.tk",
+    "mastodon.repl.info",
+    "mastodon.revconnect.com",
+    "mastodon.rfc1149.net",
+    "mastodon.rhazdon.com",
+    "mastodon.robotstart.info",
+    "mastodon.rock-hosting.net",
+    "mastodon.roflcopter.fr",
+    "mastodon.rosenberg-watt.com",
+    "mastodon.rousset.nom.fr",
+    "mastodon.rta.vn",
+    "mastodon.ruhrmail.de",
+    "mastodon.rw",
+    "mastodon.s-yurama.com",
+    "mastodon.sail42.fr",
+    "mastodon.sakaki333.com",
+    "mastodon.salon.social",
+    "mastodon.sandwich.net",
+    "mastodon.sardo.work",
+    "mastodon.sarkastic.org",
+    "mastodon.saucissefrites.com",
+    "mastodon.schep.me",
+    "mastodon.scoffoni.net",
+    "mastodon.scot",
+    "mastodon.sdf.org",
+    "mastodon.se",
+    "mastodon.sean666888.tk",
+    "mastodon.seifert-online.eu",
+    "mastodon.seijin.jp",
+    "mastodon.sergal.org",
+    "mastodon.server-on.net",
+    "mastodon.service.cybernetics.club",
+    "mastodon.sinkuu.xyz",
+    "mastodon.sk",
+    "mastodon.social",
+    "mastodon.spdns.org",
+    "mastodon.sportsfans.social",
+    "mastodon.srv.hagen.coffee",
+    "mastodon.srvmaison.fr.nf",
+    "mastodon.starling.zone",
+    "mastodon.stonith.org",
+    "mastodon.storel.li",
+    "mastodon.stsf.tokyo",
+    "mastodon.su",
+    "mastodon.suinot.org",
+    "mastodon.survival-machines.fr",
+    "mastodon.suzuoki.social",
+    "mastodon.svartalfaheimr.net",
+    "mastodon.swmnu.net",
+    "mastodon.sxpert.org",
+    "mastodon.syleogroup.fr",
+    "mastodon.sylvaniansagamies.tokyo",
+    "mastodon.sysreturn.net",
+    "mastodon.systems",
+    "mastodon.syui.cf",
+    "mastodon.tahvok.com",
+    "mastodon.tarumin.server-on.net",
+    "mastodon.tchnics.de",
+    "mastodon.techandmemes.dynip.online",
+    "mastodon.technosorcery.net",
+    "mastodon.teckl.xyz",
+    "mastodon.tedomum.net",
+    "mastodon.tetaneutral.net",
+    "mastodon.tetrisfreak.org",
+    "mastodon.thedecentralists.com",
+    "mastodon.thefourthdev.work",
+    "mastodon.thegraveyard.org",
+    "mastodon.thepacket.exchange",
+    "mastodon.therianthro.pe",
+    "mastodon.thorium.cc",
+    "mastodon.ting.systems",
+    "mastodon.tis-w.mydns.jp",
+    "mastodon.titoux.info",
+    "mastodon.tmp1024.com",
+    "mastodon.top",
+    "mastodon.training",
+    "mastodon.transneptune.net",
+    "mastodon.tricast.org",
+    "mastodon.triggerphra.se",
+    "mastodon.triofan.com",
+    "mastodon.ts-novels.jp",
+    "mastodon.tsuchinoko.tech",
+    "mastodon.tsurai.jp",
+    "mastodon.tttproject.de",
+    "mastodon.tux-mania.de",
+    "mastodon.twittolabel.tech",
+    "mastodon.uelfte.ninja",
+    "mastodon.umeki.shop",
+    "mastodon.un-zero-un.net",
+    "mastodon.undernet.uy",
+    "mastodon.unfollow.today",
+    "mastodon.universe.kingdown.fr",
+    "mastodon.unixe.de",
+    "mastodon.uno",
+    "mastodon.utwente.nl",
+    "mastodon.uwah.moe",
+    "mastodon.uwan.net",
+    "mastodon.uy",
+    "mastodon.valletta.io",
+    "mastodon.wackwack.net",
+    "mastodon.wafflec.one",
+    "mastodon.waifus.eu",
+    "mastodon.wakin.site",
+    "mastodon.walkingmountains.fr",
+    "mastodon.weaponvsac.space",
+    "mastodon.wetsnow.social",
+    "mastodon.wimjaap.nl",
+    "mastodon.wingi.net",
+    "mastodon.xpne.info",
+    "mastodon.xsteadfastx.org",
+    "mastodon.xyz",
+    "mastodon.yaat.org",
+    "mastodon.yamaoka-takeshi.xyz",
+    "mastodon.yannikenss.de",
+    "mastodon.yantou.co",
+    "mastodon.yodan.ninja",
+    "mastodon.yokohama",
+    "mastodon.yoyo.org",
+    "mastodon.yumulab.org",
+    "mastodon.yuu26.com",
+    "mastodon.z27.ch",
+    "mastodon.zemows.org",
+    "mastodon.zenfolie.org",
+    "mastodon.zenger.nl",
+    "mastodon.zestysoft.com",
+    "mastodon.ziez.eu",
+    "mastodon.zunda.ninja",
+    "mastodon.zzpro.net",
+    "mastodon2.maromaro.co.jp",
+    "mastodon48.com",
+    "mastodonar.club",
+    "mastodonargentina.club",
+    "mastodonchile.club",
+    "mastodoncr.com",
+    "mastodoncuba.club",
+    "mastodones.club",
+    "mastodonevry.ovh",
+    "mastodonfrance.com",
+    "mastodonhub.xyz",
+    "mastodonian.city",
+    "mastodonkilledtwitter.tk",
+    "mastodonrussia.ru",
+    "mastodons.jp",
+    "mastodonserver.se",
+    "mastodont.cat",
+    "mastodont.neon.moe",
+    "mastodont.social",
+    "mastodonte.me",
+    "mastodontech.de",
+    "mastodonten.de",
+    "mastodontest.codingfield.com",
+    "mastodontest.pastleo.me",
+    "mastodonti.co",
+    "mastodontrial.pagekite.me",
+    "mastodontti.fi",
+    "mastodontti.social",
+    "mastodonu.herokuapp.com",
+    "mastodoooooon.herokuapp.com",
+    "mastodos.com",
+    "mastodot.com",
+    "mastofant.de",
+    "mastohun.tk",
+    "mastoydon.com",
+    "mastube.jp",
+    "mastwi.herokuapp.com",
+    "masutodon.ga",
+    "masutodon.net",
+    "mathed.xyz",
+    "mathtodon.com",
+    "mathys.io",
+    "matitodon.com",
+    "matsuyama.m.to",
+    "matt-social.co.uk",
+    "mattchew.ninja",
+    "mattips.online",
+    "mattsinstance.social",
+    "maxlaumeister.com",
+    "mayodon.club",
+    "mazion.ga",
+    "mazu.social",
+    "mazzo.masto.host",
+    "mbl.social",
+    "mbp.limited.systems",
+    "mbstdn.tokyo",
+    "mcb.today",
+    "mcnamarii.town",
+    "mcr.cool",
+    "mctetsudou.net",
+    "md.arg.vc",
+    "md.berezowski.de",
+    "md.chorus-st.com",
+    "md.ggtea.org",
+    "md.iammer.net",
+    "md.jigensha.info",
+    "md.landbox.info",
+    "md.mana-ka.com",
+    "md.net-p.org",
+    "md.nii.so",
+    "md.os.vu",
+    "md.rhythmania.net",
+    "md.s3t.jp",
+    "md.social",
+    "md.tret.jp",
+    "md.volatile.fed.im",
+    "md.xps2.net",
+    "md.yutasan.co",
+    "mdn.etvox.com",
+    "mdn.fun",
+    "mdn.hinaloe.net",
+    "mdon.ee",
+    "me.eloli.moe",
+    "me.ns.ci",
+    "meatpi.es",
+    "media.zat.im",
+    "mediartodon.net",
+    "medievalist.masto.host",
+    "medjed.com",
+    "medsos.web.id",
+    "meemu.org",
+    "meet.lecker.coffee",
+    "meet.the-be.at",
+    "megalodon.tekicha.org",
+    "meganekeesu.tokyo",
+    "mego.bar",
+    "meilleurtube.delire.party",
+    "mel.social",
+    "melb.social",
+    "mellified.men",
+    "mementomori.space",
+    "memetastic.space",
+    "memework.ga",
+    "memework.org",
+    "memoryandthought.me",
+    "menhera.social",
+    "mental.social",
+    "mentor.tsuchinoko.tech",
+    "meow.m.to",
+    "meow.social",
+    "meow.ws",
+    "mercury.social",
+    "meron.m.to",
+    "metal.odon.space",
+    "metalhead.club",
+    "metuba.hostdon.jp",
+    "mhc.social",
+    "mhxx.hostdon.jp",
+    "mhz.social",
+    "mi-chan.ml",
+    "mi.pede.rs",
+    "mi5.jp",
+    "miaou.drycat.fr",
+    "miblog.life",
+    "michael.ajaphotos.net",
+    "microblog.mjd.id.au",
+    "microblog.pub",
+    "microswit.ch",
+    "mikeshelby.noip.me",
+    "miklb.com",
+    "mikumiku.moe",
+    "mikumikudance.cloud",
+    "mikune.com",
+    "mimumedon.com",
+    "mindful.masto.host",
+    "minecraft-pe.lol",
+    "minidon.bacardi55.org",
+    "minimes.hacklab.science",
+    "minohdon.jp",
+    "mint-n-er.de",
+    "minter.uno",
+    "mirumu.net",
+    "mirza.ee",
+    "misanthropy.space",
+    "misanthropy.wang",
+    "mishin.m.to",
+    "miso.social",
+    "misskey.inct-densan.club",
+    "misskey.motcha.tech",
+    "misskey.tmin.cf",
+    "misskey.wtf",
+    "mist.so",
+    "mistermi.me",
+    "mistresskurohime.club",
+    "misty-island.info",
+    "misty-mastodon.herokuapp.com",
+    "mixremix.cc",
+    "miyagidon.club",
+    "mkd1.m544.net",
+    "mldn.jp",
+    "mmder.net",
+    "mmmm.tokyo",
+    "mmo-rp.de",
+    "mmodon.online",
+    "mn.ms",
+    "mnmsdn.com",
+    "mo.x2io.com",
+    "moar.wine",
+    "mobile.st",
+    "mobtodon.m.to",
+    "moe.cat",
+    "moeism.me",
+    "mofu.kemo.no",
+    "mograph.social",
+    "mogumogucombo.m.to",
+    "mon.keypat.ch",
+    "monado.ren",
+    "monastery.social",
+    "mond-basis.eu",
+    "monsterpit.net",
+    "monstodon.info",
+    "montgomery.fr",
+    "moose.land",
+    "mopped.space",
+    "mordor.social",
+    "moresci.sale",
+    "morgn.net",
+    "morichan.tokyo",
+    "moseisley.club",
+    "mostodon.cloud",
+    "mostodon.info",
+    "mostodon.net",
+    "mosw.work",
+    "motodn.jp",
+    "motorsports.m.to",
+    "moytura.org",
+    "moztodon.nl",
+    "mr.am",
+    "mr.lees.greater.hong.kong.international",
+    "mrt.al",
+    "ms.yeti-factory.org",
+    "msdon.net",
+    "msgdn.xyz",
+    "msgo.m.to",
+    "mspsocial.net",
+    "mst-roa.m544.net",
+    "mst.dolpen.net",
+    "mst.nanaaki.com",
+    "mst.ongstar.jp",
+    "mst.saten.info",
+    "mst.trynary.net",
+    "mst.uoga.net",
+    "mst3k.interlinked.me",
+    "mstbot-inst.xyz",
+    "mstd.0x77.ml",
+    "mstd.tokyo",
+    "mstdn-ac.ryukyu",
+    "mstdn-babymetal.com",
+    "mstdn-cycle.com",
+    "mstdn-d.info",
+    "mstdn-ff14.jp",
+    "mstdn-football.jp",
+    "mstdn-kichijoji.tokyo",
+    "mstdn-kr.com",
+    "mstdn-minpaku.jp",
+    "mstdn-nct.com",
+    "mstdn-prfm.herokuapp.com",
+    "mstdn-pso2.online",
+    "mstdn-r18.h3z.jp",
+    "mstdn-server.jp",
+    "mstdn-t.herokuapp.com",
+    "mstdn-tech.jp",
+    "mstdn-test.degica.com",
+    "mstdn-tutokoa.m.to",
+    "mstdn-tw.com",
+    "mstdn-uragi.jp",
+    "mstdn-vn.com",
+    "mstdn-west.jp",
+    "mstdn-workers.com",
+    "mstdn.114514.xyz",
+    "mstdn.3mivon.net",
+    "mstdn.8350x.net",
+    "mstdn.acewebservices.co.uk",
+    "mstdn.activepeace.work",
+    "mstdn.adriftonthenet.com",
+    "mstdn.aliyo.me",
+    "mstdn.allout.site",
+    "mstdn.alterna-cloud.com",
+    "mstdn.anima-mystica.org",
+    "mstdn.anqou.net",
+    "mstdn.applichan.com",
+    "mstdn.appne.com",
+    "mstdn.aquaplus.jp",
+    "mstdn.archi",
+    "mstdn.asami.red",
+    "mstdn.awm.jp",
+    "mstdn.ayano.jp",
+    "mstdn.b-shock.org",
+    "mstdn.bari-ikutsu.com",
+    "mstdn.barippi.com",
+    "mstdn.beastnet.works",
+    "mstdn.beer",
+    "mstdn.bizocean.co.jp",
+    "mstdn.blue",
+    "mstdn.bossmandj.me",
+    "mstdn.boxertwin.info",
+    "mstdn.brdr.jp",
+    "mstdn.cafe",
+    "mstdn.calculator29.com",
+    "mstdn.cc",
+    "mstdn.cf",
+    "mstdn.chat",
+    "mstdn.chordwiki.org",
+    "mstdn.cirnoq.org",
+    "mstdn.ckitbara.info",
+    "mstdn.cloud.themaymeow.com",
+    "mstdn.cometeo.com",
+    "mstdn.conoha.quine.codes",
+    "mstdn.cosplayer.com",
+    "mstdn.creatorsnight.com",
+    "mstdn.cygnan.com",
+    "mstdn.dance",
+    "mstdn.dank.software",
+    "mstdn.dasoran.net",
+    "mstdn.debate.info-labs.jp",
+    "mstdn.denkitv.net",
+    "mstdn.dev.ark.jp",
+    "mstdn.dolphinbox.net",
+    "mstdn.dream-seed.com",
+    "mstdn.dwscdv3.com",
+    "mstdn.dyndns.org",
+    "mstdn.ekesete.net",
+    "mstdn.elephas.cloud",
+    "mstdn.endurancesports.club",
+    "mstdn.es",
+    "mstdn.escapism.jp",
+    "mstdn.f72u.net",
+    "mstdn.farpoint.jp",
+    "mstdn.feelingso.blue",
+    "mstdn.felice.biz",
+    "mstdn.felice.biz.",
+    "mstdn.ffffff.link",
+    "mstdn.firstforest.jp",
+    "mstdn.fr",
+    "mstdn.fujii-yuji.net",
+    "mstdn.fuku.gq",
+    "mstdn.fukuoka.jp",
+    "mstdn.fun",
+    "mstdn.gaijinsize.com",
+    "mstdn.gamerslab.com",
+    "mstdn.glorificatio.org",
+    "mstdn.gnous.eu",
+    "mstdn.goonytoons.com",
+    "mstdn.gtx.dynu.net",
+    "mstdn.guru",
+    "mstdn.h3z.jp",
+    "mstdn.hakai-macaron.com",
+    "mstdn.hanabi-life.net",
+    "mstdn.haneto.org",
+    "mstdn.haru2036.com",
+    "mstdn.haun.jp",
+    "mstdn.hirfm.net",
+    "mstdn.hisurga.com",
+    "mstdn.hiyuki2578.net",
+    "mstdn.ho-chi-minh.info",
+    "mstdn.hokkaido.jp",
+    "mstdn.hostdon.jp",
+    "mstdn.ht164.jp",
+    "mstdn.hyogo.jp",
+    "mstdn.i-fromjapan.com",
+    "mstdn.i-red.info",
+    "mstdn.ibaraki.jp",
+    "mstdn.id",
+    "mstdn.idolhack.com",
+    "mstdn.ikasekai.com",
+    "mstdn.ikebuku.ro",
+    "mstdn.image-space.info",
+    "mstdn.imi.moe",
+    "mstdn.io",
+    "mstdn.ipz.jp",
+    "mstdn.iskr.jp",
+    "mstdn.it-infra.jp",
+    "mstdn.itmedia.co.jp",
+    "mstdn.jaws-ug.okinawa",
+    "mstdn.jp",
+    "mstdn.jp.net",
+    "mstdn.k-ba.net",
+    "mstdn.kanagu.info",
+    "mstdn.kawamr.com",
+    "mstdn.kemono-friends.info",
+    "mstdn.kemonox.jp",
+    "mstdn.kida.io",
+    "mstdn.kidsblock.jp",
+    "mstdn.kinshikou.com",
+    "mstdn.kitamurakz.com",
+    "mstdn.kobys.net",
+    "mstdn.kurosuke.org",
+    "mstdn.kwmr.info",
+    "mstdn.kyoto",
+    "mstdn.lablus.com",
+    "mstdn.lalafell.org",
+    "mstdn.lesamarien.fr",
+    "mstdn.liliso.com",
+    "mstdn.local.umedasp.co.jp",
+    "mstdn.lotty.jp",
+    "mstdn.love",
+    "mstdn.m4sk.in",
+    "mstdn.madpainter.info",
+    "mstdn.masuei.net",
+    "mstdn.maud.io",
+    "mstdn.mayobus.me",
+    "mstdn.mcpe.jp",
+    "mstdn.mechkey.jp",
+    "mstdn.mell0w-5phere.net",
+    "mstdn.mexico.jp",
+    "mstdn.mikumiku.moe",
+    "mstdn.minarin.moe",
+    "mstdn.mini4wd-engineer.com",
+    "mstdn.misoca.jp",
+    "mstdn.misosi.ru",
+    "mstdn.mk39.xyz",
+    "mstdn.mnwsngk.info",
+    "mstdn.mobilehackerz.jp",
+    "mstdn.mochiwasa.xyz",
+    "mstdn.moe-max.jp",
+    "mstdn.monster-girl.homelinux.net",
+    "mstdn.mt-sys.net",
+    "mstdn.muto.me",
+    "mstdn.mx",
+    "mstdn.myifn.de",
+    "mstdn.myzkstr.tech",
+    "mstdn.nagasaki.jp",
+    "mstdn.nahcnuj.work",
+    "mstdn.nanamachi.net",
+    "mstdn.naruh.com",
+    "mstdn.naruniwa.cc",
+    "mstdn.nature.0j0.jp",
+    "mstdn.neigepluie.net",
+    "mstdn.nekonote.cc",
+    "mstdn.nemsia.org",
+    "mstdn.nere9.help",
+    "mstdn.netwhood.online",
+    "mstdn.nhaddag.com",
+    "mstdn.nicolog.jp",
+    "mstdn.nightowl.jp",
+    "mstdn.ninja",
+    "mstdn.nishiwaseda.net",
+    "mstdn.nl",
+    "mstdn.noritsuna.jp",
+    "mstdn.ntk.so",
+    "mstdn.okayama.jp",
+    "mstdn.okin-jp.net",
+    "mstdn.okinawa.jp",
+    "mstdn.okumin.com",
+    "mstdn.omaera.org",
+    "mstdn.onl",
+    "mstdn.online",
+    "mstdn.onosendai.jp",
+    "mstdn.open2ch.net",
+    "mstdn.osaka",
+    "mstdn.otofu.xyz",
+    "mstdn.otyakai.xyz",
+    "mstdn.ouk.jp",
+    "mstdn.paris",
+    "mstdn.peachybat.com",
+    "mstdn.phonolo.gy",
+    "mstdn.picospec.co.jp",
+    "mstdn.pics",
+    "mstdn.plusminus.io",
+    "mstdn.poppo-ya.com",
+    "mstdn.poyo.me",
+    "mstdn.prfm.jp",
+    "mstdn.protoxin.net",
+    "mstdn.qunaud.xyz",
+    "mstdn.raitisoja.com",
+    "mstdn.res.ac",
+    "mstdn.ribbit.xyz",
+    "mstdn.rinsuki.net",
+    "mstdn.ropo.jp",
+    "mstdn.rukin.me",
+    "mstdn.ryanak.xyz",
+    "mstdn.s7t.de",
+    "mstdn.sahagyo.com",
+    "mstdn.sakamori.jp",
+    "mstdn.sample.pw",
+    "mstdn.sanam.xyz",
+    "mstdn.sanin.link",
+    "mstdn.sastudio.jp",
+    "mstdn.satzz.me",
+    "mstdn.sauhor.jp",
+    "mstdn.sc",
+    "mstdn.schoolidol.club",
+    "mstdn.serotoninpower.club",
+    "mstdn.serv-ops.com",
+    "mstdn.sh",
+    "mstdn.shisaku.tokyo",
+    "mstdn.shizuoka.jp",
+    "mstdn.si",
+    "mstdn.sigpipe.link",
+    "mstdn.sk",
+    "mstdn.social",
+    "mstdn.sodefrin.xyz",
+    "mstdn.soysoftware.net",
+    "mstdn.superspeed-fall.com",
+    "mstdn.syachiku.net",
+    "mstdn.syaroshi.co",
+    "mstdn.syoshida.org",
+    "mstdn.syui.cf",
+    "mstdn.t11i.jp",
+    "mstdn.tac42.net",
+    "mstdn.tako774.net",
+    "mstdn.tentere.net",
+    "mstdn.thedesk.top",
+    "mstdn.thenetherlands.jp",
+    "mstdn.tinko.club",
+    "mstdn.togawa.cs.waseda.ac.jp",
+    "mstdn.tokyo",
+    "mstdn.tokyocameraclub.com",
+    "mstdn.tomocraft.tech",
+    "mstdn.trashkids.org",
+    "mstdn.tribly.de",
+    "mstdn.tty.jp",
+    "mstdn.untan.xyz",
+    "mstdn.userzap.de",
+    "mstdn.utbs.info",
+    "mstdn.v5ox.com",
+    "mstdn.vartkw.xyz",
+    "mstdn.web0000.jp",
+    "mstdn.web4u.jp",
+    "mstdn.wildtree.jp",
+    "mstdn.wiuse.work",
+    "mstdn.wonchukiss.me",
+    "mstdn.world",
+    "mstdn.x-serv.ovh",
+    "mstdn.xn--h1ahnbk7d.xn--p1ai",
+    "mstdn.y-zu.org",
+    "mstdn.yagi2.com",
+    "mstdn.yakitamago.info",
+    "mstdn.yamachan.org",
+    "mstdn.yantene.net",
+    "mstdn.yorozu-sys.net",
+    "mstdn.yumetomo.land",
+    "mstdn.yuwinet.com",
+    "mstdn.yyuuiikk.org",
+    "mstdn.zoddo.fr",
+    "mstdn.zuiho.moe",
+    "mstdn.zuyadon.tk",
+    "mstdn1.ssc-web.net",
+    "mstdn18.jp",
+    "mstdn2017.club",
+    "mstdncafe.com",
+    "mstdngirls.net",
+    "mstdnpaoon.info",
+    "mstdnsrv.moe.hm",
+    "mstdntest.t11i.jp",
+    "mstdntest1.dnaf.moe",
+    "mstdntr.com",
+    "mstdon.kazuki.xyz",
+    "mstdon.net",
+    "mstdon.robox.org",
+    "mstdona.net",
+    "mtd.ax9.eu",
+    "mtg.masto.host",
+    "mtldn.tokyo",
+    "mtu7.com",
+    "muenchen.social",
+    "muensterland.social",
+    "mugwort.xyz",
+    "muhroads.party",
+    "murmur.red",
+    "muscledon.net",
+    "music.tcit.fr",
+    "musicdn.jp",
+    "musicdon.info",
+    "musicdon.net",
+    "musicdonjp.m.to",
+    "must-don.herokuapp.com",
+    "mustardon.tokyo",
+    "mustdon.herokuapp.com",
+    "mustodon.bocchi.tokyo",
+    "mustodon.xyz",
+    "muu.2dx.red",
+    "muwiter.m.to",
+    "mvuylphrvw.localtunnel.me",
+    "my-gnusocial.de",
+    "my-mstdn.herokuapp.com",
+    "my.brick.camp",
+    "my.dirtyhobby.xyz",
+    "myd.kyoto.jp",
+    "mydon.ml",
+    "myfreckle.com",
+    "myriad.domains",
+    "myriad.social",
+    "myright.social",
+    "mysim.online",
+    "mytoot.de",
+    "n2.federati.net",
+    "naf.m.to",
+    "naganu.com",
+    "nagas.xyz",
+    "nagayodon.com",
+    "nagoyadon.jp",
+    "nantes.social",
+    "napear.m.to",
+    "narusaka.herokuapp.com",
+    "natudon-fishing.net",
+    "natudon-outdoor.net",
+    "naver.com",
+    "nayukana.info",
+    "ndanda.m.to",
+    "ndyk.de",
+    "nearly.social",
+    "neckbeard.xyz",
+    "nectarine.center",
+    "nederland.social",
+    "neesimastdn.com",
+    "negitorodon.rkun.cf",
+    "nejiamasi.com",
+    "neko.pics",
+    "nerd.town",
+    "nerdculture.de",
+    "nerdica.net",
+    "nerds.party",
+    "neris.jp",
+    "neritodon.xyz",
+    "netaka.net",
+    "nethole.us",
+    "network.reyesbros.fr",
+    "neumastodon.com",
+    "neverno.one",
+    "news-sokuhou.jp",
+    "newsbots.eu",
+    "newtro.club",
+    "newtype.institute",
+    "newyorkdon.net",
+    "nexxt.social",
+    "nghieng.net",
+    "nhcrossing.com",
+    "niadon.club",
+    "nice.community",
+    "nice.toote.rs",
+    "nicepeople.social",
+    "nickwood.ninja",
+    "nicra.fr",
+    "niedersachsen.social",
+    "nightmare.dance",
+    "niigata.minnna.xyz",
+    "nijiura.xyz",
+    "nikatsudon.m.to",
+    "nikvalls.pet",
+    "nildon.com",
+    "nishinomiya.in.net",
+    "nitech.online",
+    "nitic-mstdn.hostdon.jp",
+    "nittc.tokyo",
+    "niu.moe",
+    "niwatoriman.me",
+    "nixeneko.info",
+    "nixnet.social",
+    "nn77.m.to",
+    "noagendasocial.com",
+    "nobert.zone",
+    "noc.social",
+    "nodotsam.xyz",
+    "nog20.social",
+    "noisebridge.social",
+    "nokotaro.com",
+    "nokotaro.work",
+    "nomadon.nuevo.jp",
+    "nomlishdon.racing-lagoon.info",
+    "nomofomo.social",
+    "nonsensoleum.net",
+    "norden.social",
+    "notbird.site",
+    "notebook.yokohama",
+    "nothingtohide.net.au",
+    "notiz.blog",
+    "notmastodon.xyz",
+    "notspam.tk",
+    "noupti.me",
+    "noveldon.com",
+    "novelist.nagoya",
+    "nposns.com",
+    "nq5jmc5rsyo4fiph.onion",
+    "nrw.social",
+    "nsfw.social",
+    "nth.io",
+    "ntv.jikkyo.tv",
+    "nuernberg.social",
+    "nuka.tech",
+    "nul.tokyo",
+    "nulled.red",
+    "nullnet.us",
+    "nullpo.info",
+    "nuvens.pt",
+    "nyaa.io",
+    "nyan.mdn.hinaloe.net",
+    "nyanpass.m.to",
+    "o3o.ca",
+    "oberpfalz.social",
+    "objective.ninja",
+    "oc.lastorder.xyz",
+    "occitanie.social",
+    "oceadon.com",
+    "ocr.social",
+    "octodon.social",
+    "odakyu.app",
+    "offilth.stream",
+    "ofuton.io",
+    "ogran.club",
+    "ohai.su",
+    "oidemasetodon.com",
+    "oidon.herokuapp.com",
+    "okachan.club",
+    "okapi.kemo.no",
+    "oki.sakuradon.jp",
+    "okinawa-mstdn.okinawa",
+    "oldbytes.space",
+    "omg.haxors.club",
+    "omg.rly.wtf",
+    "omochi.xyz",
+    "on.vu",
+    "one.wildada.com",
+    "onion.social",
+    "onlineashtray.com",
+    "onlyfeds.cc",
+    "onthesecond.com",
+    "ontological.city",
+    "ooshio.o-man.co",
+    "open-don.m.to",
+    "openalgeria.org",
+    "openbiblio.social",
+    "opendev.social",
+    "openweb.social",
+    "oppai.pub",
+    "oransns.com",
+    "orb.an6.us",
+    "ore.no.imo.uto.moe",
+    "orebrotribune.online",
+    "organicdesign.pub",
+    "origamion.net",
+    "oriwebdon.com",
+    "orlando.community",
+    "orz.uno",
+    "orzhov.net",
+    "osaka.m.to",
+    "osakana.cloud",
+    "oslo.town",
+    "osna.social",
+    "osomatsu.mastportal.info",
+    "osrc.pw",
+    "ostatus.axross.io",
+    "ostatus.blessedgeeks.jp",
+    "ostatus.blessedgeeks.org",
+    "ostatus.isidai.com",
+    "ostatus.lardbucket.org",
+    "ostatus.matarillo.com",
+    "ostatus.taiyolab.com",
+    "ostatus.ttanimichi.com",
+    "ostatus.yoh2.ddo.jp",
+    "osutodon.tokyo",
+    "otajo.org",
+    "otajo.tk",
+    "otajodon.com",
+    "otonadon.m.to",
+    "otoya.space",
+    "oudon.durasite.net",
+    "our-today-homework.com",
+    "ourlittle.space",
+    "outsiders.network",
+    "overthinking.club",
+    "owlparliament.com",
+    "oxidized.systems",
+    "oyakodon.m.to",
+    "p-tube.h3z.jp",
+    "p.hostux.social",
+    "p.sysi.work",
+    "p.umbriel.fr",
+    "p9don.tokyo",
+    "pachi.house",
+    "pachyder.me",
+    "pachyderm.herokuapp.com",
+    "pages.raitisoja.com",
+    "paintacu.be",
+    "pantdon.site",
+    "pantsu.biz",
+    "pao.kotora-music.com",
+    "pao.moe",
+    "paoon.club",
+    "paoon.social",
+    "paopao.rkun.cf",
+    "paors.lv",
+    "paoru.jp",
+    "paotter.com",
+    "park.eldoom.me",
+    "pars.ee",
+    "party-don.com",
+    "party.ochakai.moe",
+    "pawbs.club",
+    "pawoo.net",
+    "pcgamer.social",
+    "peer.ecutsa.fr",
+    "peer.newsmovies.online",
+    "peer.tube",
+    "peertube.angristan.xyz",
+    "peertube.antifablitzkrieg.de",
+    "peertube.azkware.net",
+    "peertube.darktech.org",
+    "peertube.datagueule.tv",
+    "peertube.devosi.org",
+    "peertube.dolphinbox.net",
+    "peertube.drycat.fr",
+    "peertube.erdyyn.net",
+    "peertube.extremely.online",
+    "peertube.gaialabs.ch",
+    "peertube.geekael.fr",
+    "peertube.geekshell.fr",
+    "peertube.guimik.fr",
+    "peertube.gwendalavir.eu",
+    "peertube.heberge.fr",
+    "peertube.joyrex.net",
+    "peertube.maly.io",
+    "peertube.mastodon.host",
+    "peertube.mes-courriers.fr",
+    "peertube.mickaelftw.net",
+    "peertube.mindpalace.io",
+    "peertube.moe",
+    "peertube.pointsecu.fr",
+    "peertube.servebeer.com",
+    "peertube.sl-network.fr",
+    "peertube.solidev.net",
+    "peertube.tifox.fr",
+    "peertube.touhoppai.moe",
+    "peertube.valvin.fr",
+    "peertube.video",
+    "peertube.vitruvian.services",
+    "peertube.viviers-fibre.net",
+    "peertube.wrk.ru",
+    "peertube2.cpy.re",
+    "peertube3.cpy.re",
+    "pero.knoyism.net",
+    "perorin.jp",
+    "persadon.com",
+    "petit-rabbits.net",
+    "pforzelona.club",
+    "philippine.m.to",
+    "phirat.club",
+    "photodn.net",
+    "photog.social",
+    "phpc.social",
+    "phpman.red",
+    "phq.m.to",
+    "phreedom.tk",
+    "physi.cc",
+    "piano.masto.host",
+    "picmin.net",
+    "pico8.social",
+    "pipes.social",
+    "pipou.academy",
+    "pirtube.cz",
+    "pkkm.me",
+    "pl.ajin.la",
+    "pl.aurieh.me",
+    "pl.calavere.me",
+    "pl.im-in.space",
+    "pl.jort.space",
+    "pl.letsalllovela.in",
+    "pl.nudie.social",
+    "pl.quic.fr",
+    "pl.veryamt.com",
+    "planet.moe",
+    "planetafr0.org",
+    "plankton.cz",
+    "planner.social",
+    "plasticmodels.tokyo",
+    "playstation-mstdn.com",
+    "playvicious.social",
+    "ple.ggtea.org",
+    "pleasehug.me",
+    "pler.gtx.dynu.net",
+    "pler.l4p1n.met-hardware.fr",
+    "pleroma-dev.mimikun.jp",
+    "pleroma-in.ouda.space",
+    "pleroma.anduin.net",
+    "pleroma.arcseconds.net",
+    "pleroma.chirno.tech",
+    "pleroma.codingdoodles.com",
+    "pleroma.ctseuro.com",
+    "pleroma.damaron.net",
+    "pleroma.dbuild.xyz",
+    "pleroma.ddns.net",
+    "pleroma.distsn.org",
+    "pleroma.fr",
+    "pleroma.functional.technology",
+    "pleroma.funkymonkey.org",
+    "pleroma.fuwafuwa.moe",
+    "pleroma.gdgd.jp.net",
+    "pleroma.gza.jp",
+    "pleroma.heypumpk.in",
+    "pleroma.hootiegibbon.co.uk",
+    "pleroma.huttiesroow.nl",
+    "pleroma.inux39.me",
+    "pleroma.ivystech.com",
+    "pleroma.jumanji.co",
+    "pleroma.kajalinifi.de",
+    "pleroma.kamp.site",
+    "pleroma.kazuhiko.kitamura.name",
+    "pleroma.kitamurakz.com",
+    "pleroma.libretux.com",
+    "pleroma.macsnet.cz",
+    "pleroma.marud.fr",
+    "pleroma.mastodon.host",
+    "pleroma.net.ru",
+    "pleroma.neulandschmie.de",
+    "pleroma.nuke.moe",
+    "pleroma.omaera.org",
+    "pleroma.ombreport.info",
+    "pleroma.oniichanylo2tsi4.onion",
+    "pleroma.oook.fr",
+    "pleroma.otter.sh",
+    "pleroma.potatofrom.space",
+    "pleroma.rkun.cf",
+    "pleroma.shadowkat.net",
+    "pleroma.site",
+    "pleroma.soykaf.com",
+    "pleroma.taketodon.com",
+    "pleroma.tama.pro",
+    "pleroma.tbny.space",
+    "pleroma.teleassist.fr",
+    "pleroma.theru.dk",
+    "pleroma.theru.org",
+    "pleroma.thog.eu",
+    "pleroma.ur.gs",
+    "pleroma.uwah.moe",
+    "pleroma.zapto.org",
+    "plock.social",
+    "plrm.ht164.jp",
+    "plrm.onigiridon.com",
+    "plume.mastodon.host",
+    "plural.cafe",
+    "plush.city",
+    "plusone.network",
+    "plustodon.net",
+    "plx.pw",
+    "pnw.social",
+    "poc.vefto.org",
+    "pochi46.com",
+    "pod.datamol.org",
+    "podcast.nagoya",
+    "podcast.style",
+    "pointless.net",
+    "pointsof.moe",
+    "pokedon.org",
+    "pokemastodon.net",
+    "pokemon.mastportal.info",
+    "pokemon.men",
+    "polska.masto.host",
+    "polyglot.city",
+    "pom.masto.host",
+    "pom18.top",
+    "pompom.m.to",
+    "pone.social",
+    "pony.social",
+    "ponzu-mstdn.com",
+    "pop-in-don.com",
+    "populas.no",
+    "postgrestodon.magicannon.com",
+    "potaodon.audio",
+    "potatofrom.space",
+    "pouet.april.org",
+    "pouet.chapril.org",
+    "pouet.chouech.org",
+    "pouet.couchet.org",
+    "pouet.draconis.me",
+    "pouet.freetorrent.fr",
+    "pouet.it",
+    "pouet.l-internet.fr",
+    "pouet.me",
+    "pouet.meutel.net",
+    "pouet.outils-conviviaux.fr",
+    "pouet.pastix.fr",
+    "pouet.social",
+    "pouet.space",
+    "pouet.yolops.net",
+    "pouets.ovh",
+    "pounced-on.me",
+    "ppl.town",
+    "pr.fed.im",
+    "precure.ml",
+    "presidentielle.tech",
+    "prfm-mstdn.herokuapp.com",
+    "price.masto.host",
+    "pritter.tk",
+    "pritter.work",
+    "privacy.fyi",
+    "prog.m.to",
+    "prosports.space",
+    "ps.s10y.eu",
+    "pso2.club",
+    "psodon.com",
+    "pteranodon.net",
+    "ptube.ranranhome.info",
+    "pub.miniverse.social",
+    "publog.stuifzandapp.com",
+    "puniman.co",
+    "punjabi.social",
+    "puppo.space",
+    "pups.social",
+    "puredon.matcha-soft.com",
+    "puter.games",
+    "puz.fun",
+    "pvagner.tk",
+    "pwning.social",
+    "pwnsdx.pw",
+    "pxil.club",
+    "pynq.limited.systems",
+    "q-chan.m.to",
+    "qatuno.de",
+    "qdon.space",
+    "qjffcarjjq.localtunnel.me",
+    "qoto.org",
+    "quarrel2.social",
+    "quasselfritze.de",
+    "queer.af",
+    "queer.chat",
+    "queer.hacktivis.me",
+    "queer.party",
+    "quey.org",
+    "quimas.work",
+    "quitgram.com",
+    "quixotic.info",
+    "quizdon.com",
+    "quod.pl",
+    "quotient.space",
+    "quoto.social",
+    "qwanturank.social",
+    "qwerty.city",
+    "r.ioto.us",
+    "r5.nz",
+    "rabbit.zone",
+    "rabe.masto.host",
+    "radical.town",
+    "radiosocial.org",
+    "raeven.net",
+    "raeven.social",
+    "rafting.io",
+    "raggedfeathers.com",
+    "raim0713.m.to",
+    "rainbowdash.net",
+    "raize.in",
+    "rally.social",
+    "ramendon.jp",
+    "raptol.net",
+    "rasly.cf",
+    "raven.dog",
+    "rayn.bo",
+    "rcsocial.net",
+    "re.dym.sh",
+    "real-escape.jp",
+    "realize.be",
+    "reallygay.party",
+    "rebeltoot.com",
+    "reclaim.technology",
+    "recode.macro.tokyo",
+    "red.confederac.io",
+    "redroo.ml",
+    "redsnow.io",
+    "refactorcamp.org",
+    "regret.horse",
+    "relativity.cafe",
+    "relay.soy",
+    "remicck.club",
+    "republikrenyonez.sytes.net",
+    "researchbuzz.masto.host",
+    "resipsa.social",
+    "resistance.social",
+    "resistodon.com",
+    "retro.social",
+    "rettiwtkcuf.social",
+    "reveal.today",
+    "rho.insom.me.uk",
+    "rihhptvizt.localtunnel.me",
+    "rikadon.club",
+    "riotgurl.club",
+    "risingsun.red",
+    "rivals.space",
+    "riverwest.xyz",
+    "rivqr.xyz",
+    "rkun.cf",
+    "rly.wtf",
+    "ro-mastodon.puyo.jp",
+    "roar.draboros.net",
+    "robloxcommunity.social",
+    "rollenspiel.social",
+    "roma.sftblw.moe",
+    "romancelandia.club",
+    "romandie.social",
+    "rookie.tsuchinoko.tech",
+    "roppongi-gohan.tsuchinoko.tech",
+    "roughseas.xyz",
+    "rows.io",
+    "rpimstdn.hima-jin.info",
+    "rthelp.rta.vn",
+    "ruanjian.tools",
+    "ruby.social",
+    "ruhr.social",
+    "rumia.xyz",
+    "rundfunker.net",
+    "rva.party",
+    "rx14.co.uk",
+    "rxt.social",
+    "ryanhle.com",
+    "ryecroft21.info",
+    "ryonatodon.com",
+    "s-ninja.net",
+    "s.brined.fish",
+    "s.cctracker.net",
+    "s.geno.is",
+    "s.knusper-land.de",
+    "s.libreden.net",
+    "s.meo.ws",
+    "s.ovalerio.net",
+    "s.r53.me",
+    "s.sigpipe.me",
+    "s.taek.us",
+    "s.teh.ninja",
+    "sabertooth.masto.host",
+    "sadgirl.club",
+    "saidon.net",
+    "saihate.m.to",
+    "saitama-stdn.com",
+    "saja.freemyip.com",
+    "sakaba.space",
+    "sakejp.tokyo",
+    "sakoku.jp",
+    "sakuradon.m.to",
+    "salesforce.social",
+    "salon.social",
+    "salted.fish",
+    "salty.vet",
+    "samiandalex.com",
+    "san-junipero.gimme-sympathy.org",
+    "sanctuary.heypumpk.in",
+    "sandbox.skoji.jp",
+    "sanjuanislands.social",
+    "sanskritm.xyz",
+    "sansschatten.com",
+    "satzcoal.com",
+    "sawakai.space",
+    "sc.fetus.jp",
+    "sc.shun1s.com",
+    "scalie.business",
+    "scalie.club",
+    "scamdemic.party",
+    "schlechter.host",
+    "schleuss.online",
+    "schmarty.net",
+    "scholar.social",
+    "school.m.to",
+    "schuppentier.org",
+    "scicomm.xyz",
+    "scl.zmb.cm",
+    "scotland.computer",
+    "scramble.city",
+    "screech.social",
+    "scuba.masdon.life",
+    "sdfn-01.ninjawedding.org",
+    "sdmesh.social",
+    "seacow.social",
+    "sealeo.social",
+    "sechodon.dip.jp",
+    "securitymastod.one",
+    "sedryk.info",
+    "seikin.tv",
+    "seimeiyuuki.m.to",
+    "seizemeans.com",
+    "selfcare.masto.host",
+    "senridon.com",
+    "sentient.cloud",
+    "serval.club",
+    "server.gkbrk.com",
+    "servercan.xyz",
+    "setagaya.place-life.com",
+    "setl.ist",
+    "sfba.social",
+    "shadow.pagekite.me",
+    "shadowsynth.xyz",
+    "shadowverdon.info",
+    "shadowverse-mstdn.jp",
+    "share.tube",
+    "shelter.moe",
+    "shestak.me",
+    "shigadon.com",
+    "shigusegubu.club",
+    "shiisaba.dip.jp",
+    "shimekiri-mustdone.com",
+    "shinonomemilk.com",
+    "shio.moe",
+    "shironeko.pro",
+    "shitcoin.land",
+    "shitter.me",
+    "shkval.net",
+    "shmead.co.uk",
+    "shonoda.tokyo",
+    "shortnote.masto.host",
+    "showoffto.us",
+    "si.pmpm.pw",
+    "siege.rhino.cards",
+    "sikke.fi",
+    "silverhaze.eu",
+    "simstim.club",
+    "sin-gakuseikikai-otaku.m.to",
+    "sin.tyaku.com",
+    "sin1mstdn.dip.jp",
+    "sinblr.com",
+    "singersongwritodon.com",
+    "sio.masto.host",
+    "sirius.social",
+    "sisuclub.com",
+    "sizedon.com",
+    "skeptikon.fr",
+    "skett.masto.host",
+    "skinheads.co",
+    "skins.gay",
+    "skobuffs.club",
+    "skrt.social",
+    "slashine.onl",
+    "sldon.com",
+    "sldon.jp",
+    "sleeklounge.com",
+    "sleeping.town",
+    "slime.global",
+    "smalltalk.annp.jp",
+    "smeap.com",
+    "smiley.thespinning.top",
+    "smux.be",
+    "sn.gunmonkeynet.netindex.php",
+    "sn.jonkman.ca",
+    "sn.theru.org",
+    "snabelen.no",
+    "snap.photo",
+    "snatched.space",
+    "snouts.online",
+    "sns.infraegg.com",
+    "sns.netaka.net",
+    "sns3rd.scalingo.io",
+    "sobaudon.space",
+    "soc.bckly.com",
+    "soc.flyingcube.tech",
+    "soc.giga.is",
+    "soc.hopto.me",
+    "soc.hyena.network",
+    "soc.krashboyz.org",
+    "soc.louiz.org",
+    "soc.mtb.wtf",
+    "soc.safebook.space",
+    "soc.se.gfault.ca",
+    "soc.shbbl.ru",
+    "soc.sinkuu.xyz",
+    "soc.uiae.at",
+    "soccerdon.net",
+    "socel.net",
+    "socia.esperant.io",
+    "social-u.tokyo",
+    "social.0day.agency",
+    "social.0ko.me",
+    "social.0x80.org",
+    "social.48bin.net",
+    "social.adlerweb.info",
+    "social.aiqwest.com",
+    "social.alabasta.net",
+    "social.alex73630.xyz",
+    "social.alifein.tech",
+    "social.alternativebit.fr",
+    "social.ancel.io",
+    "social.andresen.mx",
+    "social.anoxinon.de",
+    "social.anthro.cc",
+    "social.apreslanu.it",
+    "social.arbleizez.bzh",
+    "social.archi",
+    "social.art-software.fr",
+    "social.ash.bzh",
+    "social.atoh.me",
+    "social.atypique.net",
+    "social.auge.cat",
+    "social.aura.ovo.run",
+    "social.avareborn.de",
+    "social.backtick.town",
+    "social.baddps.com",
+    "social.ballpointcarrot.net",
+    "social.bankmann.name",
+    "social.bau-ha.us",
+    "social.beepboop.ga",
+    "social.bignum.org",
+    "social.bim.land",
+    "social.bobcall.me",
+    "social.brandongiesing.com",
+    "social.brennanbenkert.com",
+    "social.bruniau.net",
+    "social.buckket.org",
+    "social.bytesexual.net",
+    "social.bytestemplar.com",
+    "social.cannon.pw",
+    "social.catgirlsin.space",
+    "social.chilliet.eu",
+    "social.chinwag.org",
+    "social.chronal.net",
+    "social.chrpaul.de",
+    "social.cmp.is",
+    "social.coletivos.org",
+    "social.collabora.digital",
+    "social.cologne",
+    "social.common.se",
+    "social.communia.org",
+    "social.computerfox.xyz",
+    "social.conglomer.net",
+    "social.conquerworld.fr",
+    "social.coop",
+    "social.ctsollars.com",
+    "social.curiousminds.net",
+    "social.curta.red",
+    "social.daft.host",
+    "social.damillora.com",
+    "social.dasnetzundich.de",
+    "social.datensturm.net",
+    "social.delort.gdn",
+    "social.dev-wiki.de",
+    "social.device5.co.uk",
+    "social.devio.us",
+    "social.devloprog.org",
+    "social.diskseven.com",
+    "social.diva.exchange",
+    "social.donniewest.com",
+    "social.dotstar.plus",
+    "social.drastical.tech",
+    "social.driou.tech",
+    "social.dropbear.xyz",
+    "social.drox.zone",
+    "social.dsbrooks.me",
+    "social.dssr.ch",
+    "social.edbob.org",
+    "social.eifelcoworking.space",
+    "social.elbmatsch.de",
+    "social.ephemerecreative.ca",
+    "social.etc-services.de",
+    "social.exan.tech",
+    "social.eyesight.jp",
+    "social.fab-l3.org",
+    "social.familie-tux.de",
+    "social.farend.co.jp",
+    "social.farhan.codes",
+    "social.feder8.ru",
+    "social.fehmer.tech",
+    "social.finkhaeuser.de",
+    "social.floragxth.com",
+    "social.fractalco.re",
+    "social.freechristian.life",
+    "social.frogeye.fr",
+    "social.fruitscale.com",
+    "social.fulgur.xyz",
+    "social.gaos.org",
+    "social.gbox.work",
+    "social.gemuplay.com",
+    "social.generativists.com",
+    "social.genxp.net",
+    "social.gestaltzerfall.net",
+    "social.gilbert.world",
+    "social.gl-como.it",
+    "social.glados.ch",
+    "social.gnu.rocks",
+    "social.gould.cx",
+    "social.govt.is",
+    "social.grimmbart.org",
+    "social.guimik.fr",
+    "social.gwadalug.org",
+    "social.hacktivis.me",
+    "social.hangup.eu",
+    "social.hardwarepunk.de",
+    "social.helios42.de",
+    "social.henriksen.is",
+    "social.henzi.org",
+    "social.hoga.fr",
+    "social.homunyan.com",
+    "social.hostsharing.net",
+    "social.hoyle.me.uk",
+    "social.hyuki.net",
+    "social.ibstyle.info",
+    "social.icewind.nl",
+    "social.illegalpornography.com",
+    "social.imchip.be",
+    "social.imirhil.fr",
+    "social.indigophoenix.net",
+    "social.inditoot.com",
+    "social.integritymo.de",
+    "social.intensifi.es",
+    "social.iofoundry.eu",
+    "social.ioserv.space",
+    "social.irrwitz.com",
+    "social.ivystech.com",
+    "social.jabbadu.de",
+    "social.jasongreen.net",
+    "social.java.nrw",
+    "social.jeffjeff.us",
+    "social.jesuislibre.net",
+    "social.jgachelin.fr",
+    "social.jlelse.me",
+    "social.jrm.cc",
+    "social.k40s.net",
+    "social.kaiser.me",
+    "social.kajalinifi.de",
+    "social.keenfamily.us",
+    "social.kikuzuki.org",
+    "social.kimamass.com",
+    "social.kippenbergs.de",
+    "social.kleinheld.ch",
+    "social.korot.ru",
+    "social.koyu.space",
+    "social.krylc.cloud",
+    "social.ksite.de",
+    "social.kymo.org",
+    "social.l4p1n.met-hardware.fr",
+    "social.lab.cultura.gov.br",
+    "social.lafermenumerique.com",
+    "social.laiguana.org",
+    "social.leftic.club",
+    "social.lescorpsdereve.space",
+    "social.librem.one",
+    "social.linux.pizza",
+    "social.linuxserver.pro",
+    "social.lkw.tf",
+    "social.lleialtat.cat",
+    "social.logilab.org",
+    "social.lovetux.net",
+    "social.lri.ovh",
+    "social.magnier.io",
+    "social.manulanglois.fr",
+    "social.markwaters.eu",
+    "social.masharih.me",
+    "social.mashek.net",
+    "social.masto.host",
+    "social.mastodon.com.au",
+    "social.mastoverse.me",
+    "social.matej-lach.me",
+    "social.matsuuratomoya.com",
+    "social.mecanis.me",
+    "social.mediacast.ca",
+    "social.meekchopp.es",
+    "social.mikutter.hachune.net",
+    "social.minego.net",
+    "social.mmk2410.org",
+    "social.mochi.academy",
+    "social.moeoverflow.org",
+    "social.mofu2charger-listenradio.net",
+    "social.mootech.eu",
+    "social.mordicux.xyz",
+    "social.morphux.org",
+    "social.mousqueton.ru",
+    "social.myconan.net",
+    "social.nah.re",
+    "social.nassi.me",
+    "social.neoliferp.fr",
+    "social.nerdheim.de",
+    "social.netplusw.com",
+    "social.nexum.hu",
+    "social.nixnet.services",
+    "social.noff.co",
+    "social.nofftopia.com",
+    "social.noisyspot.jp",
+    "social.noostache.fr",
+    "social.nowa.re",
+    "social.ntic.fr",
+    "social.nyanlout.re",
+    "social.omniatv.com",
+    "social.opendan.net",
+    "social.opennerds.org",
+    "social.orangespatula.com",
+    "social.oupsman.fr",
+    "social.over-world.org",
+    "social.oviked.xyz",
+    "social.paco.to",
+    "social.paddlefish.net",
+    "social.panthermodern.net",
+    "social.papill0n.org",
+    "social.parocus.de",
+    "social.pastix.fr",
+    "social.patapouf.xyz",
+    "social.pbb.lc",
+    "social.peixe.co",
+    "social.piecemaker.rocks",
+    "social.piratenpartei.koeln",
+    "social.politicaconciencia.org",
+    "social.ponyfrance.net",
+    "social.pop42.net",
+    "social.potatofrom.space",
+    "social.prezman.fr",
+    "social.privacytools.io",
+    "social.pueseso.club",
+    "social.qunagi.net",
+    "social.rastapuls.com",
+    "social.reputatio.us",
+    "social.rievo.net",
+    "social.rushon.xyz",
+    "social.saarland",
+    "social.sdr.haus",
+    "social.seattle.wa.us",
+    "social.shadowfacts.net",
+    "social.shaun.net",
+    "social.shun1s.com",
+    "social.sickstack.com",
+    "social.simcu.com",
+    "social.simonoener.com",
+    "social.sitedethib.com",
+    "social.slat.org",
+    "social.snargol.com",
+    "social.snopyta.org",
+    "social.spiwit.net",
+    "social.spudalicio.us",
+    "social.spunkiedesign.com",
+    "social.srv.space",
+    "social.starmade.de",
+    "social.stephanmaus.de",
+    "social.stoablick.de",
+    "social.strykers.xyz",
+    "social.stuartbrand.co.uk",
+    "social.surfnet.space",
+    "social.symphonie-of-code.fr",
+    "social.tachibana.cool",
+    "social.talk.coffee",
+    "social.taniho.net",
+    "social.targaryen.house",
+    "social.tchncs.de",
+    "social.tcit.fr",
+    "social.tedliou.com",
+    "social.thedixons.net",
+    "social.theliturgists.com",
+    "social.thevillage.chat",
+    "social.thisisjoes.site",
+    "social.tictech.info",
+    "social.tigefa.space",
+    "social.tilde.team",
+    "social.tinysubversions.com",
+    "social.titouan.co",
+    "social.troll.academy",
+    "social.tulsa.ok.us",
+    "social.typica.us",
+    "social.udona.fr",
+    "social.ufeff.club",
+    "social.uhoreg.ca",
+    "social.upho.net",
+    "social.vcity.de",
+    "social.veroone.fr",
+    "social.vikings.net",
+    "social.vincentux.fr",
+    "social.weho.st",
+    "social.wieser-hv.de",
+    "social.wilboard.nl",
+    "social.wilcox.pub",
+    "social.willistonschools.org",
+    "social.wirefull.org",
+    "social.wxcafe.net",
+    "social.xmob.me",
+    "social.xn--wxa.zone",
+    "social.xthemage.net",
+    "social.yournearestbar.com",
+    "social.yyy.scot",
+    "social.zn80.net",
+    "social.zoar.cloud",
+    "sociala.me",
+    "socialbook.vip",
+    "socialcs.xyz",
+    "sociale.network",
+    "socially.constructed.space",
+    "sociallyspeaking.online",
+    "socialtuna.net",
+    "sociamodo.com",
+    "societal.co",
+    "society.oftrolls.com",
+    "solaria.space",
+    "solo-outdon.club",
+    "someonewho.codes",
+    "somewhy.com",
+    "soscet.network",
+    "sosh.network",
+    "soteria.mastodon.host",
+    "soundcafe.nl",
+    "sousaku.club",
+    "soykaf.com",
+    "sozen.network",
+    "sozial.derguhl.de",
+    "spaced.in",
+    "spacejerk.fr",
+    "spacerock.jp",
+    "spanner.works",
+    "speakitfree.com",
+    "speedequalsdistanceovertime.com",
+    "spicychicken.club",
+    "spiritoot.space",
+    "sportsball.rocks",
+    "spring-has-come.tk",
+    "spydar007.com",
+    "squeegee.one",
+    "squid.cafe",
+    "squope.net",
+    "ss4.masdon.life",
+    "st.foresdon.jp",
+    "staging.thinkers.ac",
+    "star-pharm.net",
+    "stare.pro",
+    "stat.ink",
+    "stat.tokyo",
+    "station.opencultures.life",
+    "status.dissidence.ovh",
+    "status.imirhil.fr",
+    "status.kaimi.cc",
+    "status.ldjb.uk",
+    "status.loquat.moe",
+    "status.matsuuratomoya.com",
+    "status.obscuritus.ca",
+    "status.pfefferle.org",
+    "status.pointless.one",
+    "status.thecelticsyndicate.com",
+    "status.unixmail.fr",
+    "statusnet.i2p.rocks",
+    "steamstdn.com",
+    "stegodon.me",
+    "stellar.planet.moe",
+    "stephenking.club",
+    "stereodon.social",
+    "stoneartprod.xyz",
+    "stophorny.online",
+    "stork.ml",
+    "straylight.expectnomore.net",
+    "streiter.io",
+    "sts.ditatompel.com",
+    "stuff.aurieh.me",
+    "stupid.industries",
+    "subculdon.com",
+    "subculdon.jp",
+    "succ.faith",
+    "such.social",
+    "sumanko.ml",
+    "summoners-riftodon.jp",
+    "sunbeam.city",
+    "sunshinegardens.org",
+    "suntogun.remujin.com",
+    "super-famicom.com",
+    "svrdn.drillion.net",
+    "swast.club",
+    "sweettitties.scalingo.io",
+    "switter.at",
+    "switter.nl",
+    "syachiku.net",
+    "sync.appservice.fr",
+    "sys.kawi.fr",
+    "systerserver.town",
+    "syui-ml.herokuapp.com",
+    "syui.ml",
+    "syunm1n.cf",
+    "t-syachi.com",
+    "t.cascadians.net",
+    "t.con.sh",
+    "t.cypv4.com",
+    "t.d65.xyz",
+    "t.digigame.org",
+    "t.unihubs.com",
+    "tabletop.social",
+    "takahashi.social",
+    "take.limemo.net",
+    "takumi.fun",
+    "tale.cafe",
+    "talk.econudes.org",
+    "talk.houbahouba.de",
+    "talker.to",
+    "talkr.justanother.party",
+    "talkr.online",
+    "tama.pro",
+    "tamudon.com",
+    "tanakadon.com",
+    "tantooine.myfirewall.orgfriendica",
+    "taote.uk",
+    "tardis.world",
+    "taruntarun.net",
+    "taslug.org.au",
+    "tattsun-lab.com",
+    "tatu.la",
+    "taygete.nothink.jp",
+    "tcu-mstdn.tk",
+    "team-d.jp",
+    "teatime.afternoonrobot.co.uk",
+    "tecce.club",
+    "tech.lgbt",
+    "techdon.herokuapp.com",
+    "techdon.info",
+    "techgeek.tokyo",
+    "techn.ical.ist",
+    "technologeek.me",
+    "tegedon.net",
+    "tehduck.club",
+    "tekkadan.me",
+    "tekkadon.manimani.cc",
+    "tentacle.social",
+    "teo.taiha.net",
+    "terabato.fun",
+    "tescher.me",
+    "teslam.in",
+    "test.afn.social",
+    "test.animedon.tk",
+    "test.ika.queloud.net",
+    "test.lithium03.info",
+    "test.lovelive-ss.com",
+    "test.mstdon.app",
+    "test.tegedon.net",
+    "test.vocalodon.net",
+    "testdon.herokuapp.com",
+    "testdon.mt-sys.net",
+    "testingmstdn.abcang.net",
+    "testtube.ortg.de",
+    "teswww.msdnaart.net",
+    "tetsugaku.place",
+    "tgp.jp",
+    "the-hash.m.to",
+    "the.goofs.space",
+    "the.hoe.zone",
+    "the6ix.space",
+    "thecave.social",
+    "thechurchofmemes.com",
+    "thecolouroutof.space",
+    "theferret.social",
+    "thefire.work",
+    "theicon.stream",
+    "thenullpointer.net",
+    "theoldergamers.social",
+    "thepeanut.farm",
+    "thepractice.space",
+    "theres.life",
+    "thes.eus.jp",
+    "thevillastraylight.com",
+    "thezombie.net",
+    "thezone.zone",
+    "thicc.horse",
+    "thinkers.ac",
+    "thinkerview.video",
+    "this.feelslike.life",
+    "this.mouse.rocks",
+    "thndrhub.hopto.org",
+    "thraeryn.net",
+    "tibibibimbap.m.to",
+    "tigre-bleu.net",
+    "tilde.zone",
+    "timshomepage.net",
+    "tinnies.club",
+    "tiny.tilde.website",
+    "tinyfed.com",
+    "tk2-203-11173.vs.sakura.ne.jp",
+    "tkmb.tokyo",
+    "tmi.fyi",
+    "tmp-mstdn.cloud",
+    "tobane.m.to",
+    "todon.eu",
+    "todon.ploud.fr",
+    "tokamstdn.jp",
+    "toktan.org",
+    "tokushima.cloud",
+    "tokyo.mastodon-servers.net",
+    "tokyoidolfestival.m.to",
+    "tokyomx.jikkyo.tv",
+    "tomitodon.huideyeren.info",
+    "tonos.io",
+    "too.tl",
+    "tooot.im",
+    "toot-lab.reclaim.technology",
+    "toot.3stadt.com",
+    "toot.amblin.io",
+    "toot.aquilenet.fr",
+    "toot.blue",
+    "toot.cafe",
+    "toot.cat",
+    "toot.chemnitz.social",
+    "toot.dessert.coffee",
+    "toot.draconis.me",
+    "toot.drup.no",
+    "toot.fail",
+    "toot.fedilab.app",
+    "toot.gibberfish.org",
+    "toot.hogehuga.com",
+    "toot.io",
+    "toot.irth.pl",
+    "toot.jokke.space",
+    "toot.kashishokunin.com",
+    "toot.katuemon.com",
+    "toot.kif.rocks",
+    "toot.koeln",
+    "toot.krinetzki.de",
+    "toot.lain.moe",
+    "toot.louiscap.io",
+    "toot.love",
+    "toot.mad-scientist.club",
+    "toot.mastoc.fr",
+    "toot.matereal.eu",
+    "toot.memtech.website",
+    "toot.mst-dn.me",
+    "toot.my",
+    "toot.network",
+    "toot.nx-pod.de",
+    "toot.okaris.de",
+    "toot.party",
+    "toot.portes-imaginaire.org",
+    "toot.psyco.fr",
+    "toot.pub",
+    "toot.redeagle.me",
+    "toot.rjl.li",
+    "toot.shizentai.jp",
+    "toot.shoes",
+    "toot.si",
+    "toot.site",
+    "toot.snowgoons.ro",
+    "toot.thoughtworks.com",
+    "toot.tibidoo.com",
+    "toot.turbo.chat",
+    "toot.tzim.net",
+    "toot.wales",
+    "toot.whatever.cz",
+    "toot.wiredpunch.com",
+    "toot.works",
+    "toot.x91.de",
+    "toot.yukimochi.jp",
+    "toot.ziroh.be",
+    "toot.zone",
+    "tooter.club",
+    "tooter.in",
+    "tooter.it",
+    "tooter.masto.host",
+    "tooter.social",
+    "tooter.space",
+    "tooting.ch",
+    "tooting.intensifi.es",
+    "tootme.de",
+    "tootmin.network",
+    "tootodon.xyz",
+    "toots.benpro.fr",
+    "toots.corzntin.fr",
+    "toots.evilchi.li",
+    "toots.herokuapp.com",
+    "toots.logomancy.net",
+    "toots.slothy.win",
+    "toots.social",
+    "toots.space",
+    "tootville.com",
+    "tootzone.herokuapp.com",
+    "topbol.com",
+    "tophattedcat.co.ukfriend",
+    "topic-master.com",
+    "torontomusic.cloud",
+    "touhey.org",
+    "toyamastdon.net",
+    "tradon.jp",
+    "trans.town",
+    "transglobalstudies.org",
+    "transglobalstudies.social",
+    "transmom.love",
+    "trashtalk.jp",
+    "travel-friends.chat",
+    "travelpandas.fr",
+    "treehouse.technopagans.de",
+    "trev.pub",
+    "trey9407.social",
+    "troet.cafe",
+    "troet.space",
+    "trpg-link.tk",
+    "trpg.cloud",
+    "truecolors.space",
+    "trunk.mad-scientist.club",
+    "try.bunyip.space",
+    "tsuki.network",
+    "tsunali.cherokeesofidaho.org",
+    "tsuruga.net",
+    "tt.vvxq.ca",
+    "tube.ac-lyon.fr",
+    "tube.aquilenet.fr",
+    "tube.bruniau.net",
+    "tube.cccp.io",
+    "tube.comm.network",
+    "tube.conferences-gesticulees.net",
+    "tube.darfweb.eu",
+    "tube.h.cccp.io",
+    "tube.hoga.fr",
+    "tube.kdy.ch",
+    "tube.kher.nl",
+    "tube.libox.fr",
+    "tube.mastofant.de",
+    "tube.mochi.academy",
+    "tube.nemsia.org",
+    "tube.openalgeria.org",
+    "tube.opportunis.me",
+    "tube.otogamer.me",
+    "tube.otter.sh",
+    "tube.p2p.legal",
+    "tube.postblue.info",
+    "tube.traydent.info",
+    "tube.xn--krsgw--n73t.com",
+    "tuner.1242.com",
+    "turtleapparel.eu",
+    "tusk.schoollibraries.net",
+    "tusk.social",
+    "tusk.what.re",
+    "tusks.co",
+    "tutut.delire.party",
+    "tv.stupid.industries",
+    "tvdon.rt-trend.jp",
+    "tvdon.tv",
+    "tvman.net",
+    "tweety.icu",
+    "twingyeo.kr",
+    "twinkaga.in",
+    "twipo.net",
+    "twit.jp",
+    "twitterchan.net",
+    "twoot.space",
+    "twooter.me",
+    "tymoon.eu",
+    "typing.sexy",
+    "typrout.ml",
+    "tzcafe.com",
+    "u.4queens.tk",
+    "udk.moe",
+    "udub.club",
+    "uelfte.club",
+    "uenodon.com",
+    "ukadon.shillest.net",
+    "ukrainian.social",
+    "ulman.social",
+    "ultra.fail",
+    "ultrix.me",
+    "un56ixzx7euwyr43.onion",
+    "unadon.club",
+    "unbound.social",
+    "unebulle.space",
+    "unidon.asmodeus.red",
+    "unipar.online",
+    "unique-inet.tokyo",
+    "univdon.com",
+    "unnerv.jp",
+    "unspecified.social",
+    "untwitter.cf",
+    "ura-mstdn.com",
+    "us.clayto.com",
+    "us.peertube.network",
+    "usm.hootiegibbon.co.uk",
+    "utdon.herokuapp.com",
+    "utodon.jp",
+    "utopia.cool",
+    "uwu.social",
+    "v.kretschmann.social",
+    "v1x3n.net",
+    "v2.nyoki.club",
+    "v22017122292958322.goodsrv.de",
+    "v240rc1.maud.io",
+    "valhalla.mamalibre.com.ar",
+    "valleypost.us",
+    "vandelay.systems",
+    "vapers.jp",
+    "vasilakisfil.social",
+    "vastodon.com",
+    "vdmstdn.com",
+    "veah.cocoa.moe",
+    "vegetadon.tokyo",
+    "vid.leotindall.com",
+    "video.4ray.co",
+    "video.anormallostpod.ovh",
+    "video.deadsuperhero.com",
+    "video.hispagatos.org",
+    "video.lqdn.fr",
+    "video.lw1.at",
+    "video.obermui.de",
+    "video.pyrignis.fr",
+    "video.susan.party",
+    "video.techknowlogick.com",
+    "video.teddybeard.eu",
+    "video.tedomum.net",
+    "video.typica.us",
+    "videonaute.fr",
+    "videos.benpro.fr",
+    "videos.cloudfrancois.fr",
+    "videos.firedragonstudios.com",
+    "videos.lecygnenoir.info",
+    "videos.mjkeen.com",
+    "videos.renardrebelle.fr",
+    "videos.stolon.fr",
+    "videos.tcit.fr",
+    "vis.social",
+    "vkdn.jp",
+    "vln.evillair.io",
+    "vm.mstdn.media",
+    "vmrpc.net",
+    "vocalodon.net",
+    "voi.social",
+    "voidptr.wtf",
+    "voluntary.world",
+    "voluntaryaction.network",
+    "vorlon.space",
+    "vox.es",
+    "voxpop.mediacast.ca",
+    "vps78926.vps.ovh.ca",
+    "vucic.social",
+    "vucica.net",
+    "vulpine.club",
+    "vulturion.com",
+    "vypr.space",
+    "w3.freechinaweibo.com",
+    "w3r.jp",
+    "wa1.net",
+    "waifu.one",
+    "wales2600.com",
+    "walkaway.space",
+    "walkers.social",
+    "walrein.m.to",
+    "waltdn.com",
+    "wandering.shop",
+    "wangchunlei.li",
+    "warubure.online",
+    "wasteland.pro",
+    "wastodon.herokuapp.com",
+    "we.procrastinate.work",
+    "weakend-mstdn.net",
+    "wearemastodon.com",
+    "weaselshit.com",
+    "webcommunity.club",
+    "weeaboo.space",
+    "weeb.cloud",
+    "weebs.moe",
+    "week.dgdk.net",
+    "weird.tf",
+    "welldn.net",
+    "wellness.so",
+    "welt.all.de",
+    "werewolf.masto.host",
+    "weststar.name",
+    "whiskycat.m.to",
+    "whisper.tf",
+    "whispr.hoenn.me",
+    "whitespashe.uk",
+    "wickedtotally.com",
+    "willacy.rocks",
+    "winfinidon.winfiniscript.ga",
+    "witches.academy",
+    "witches.live",
+    "witches.town",
+    "witchey.club",
+    "wizards.town",
+    "wizfox.jp",
+    "wkfg.me",
+    "wobscale.social",
+    "wogan.im",
+    "wolfhowl.me",
+    "word.builders",
+    "worldfactorydon.com",
+    "wp-social.net",
+    "wpnett.com",
+    "wqcefp.online",
+    "writing.exchange",
+    "wrongthink.net",
+    "wsup.social",
+    "wubrg.social",
+    "wug.fun",
+    "www.abdl.io",
+    "www.birddon.com",
+    "www.blimps.xyz",
+    "www.chatalk.club",
+    "www.coazergues.info",
+    "www.culatello.club",
+    "www.duanzijiazu.com",
+    "www.fairlyfriendly.net",
+    "www.fidgetykid.club",
+    "www.freechinaweibo.com",
+    "www.ksu-mastodon.com",
+    "www.l1f.de",
+    "www.masto.pt",
+    "www.mastodon.flights",
+    "www.mastodon.su",
+    "www.mastodontic.club",
+    "www.mstd.tokyo",
+    "www.mstddntfdn.online",
+    "www.mumeiserver.club",
+    "www.nekotodon.com",
+    "www.oneli.net",
+    "www.owlparliament.com",
+    "www.pritter.tk",
+    "www.quranite.net",
+    "www.rainbowdash.net",
+    "www.seiyu-mstdn.club",
+    "www.sizuma.website",
+    "www.sociale.network",
+    "www.sosial.eu",
+    "www.yaskey.tokyo",
+    "www.yiny.org",
+    "wxw.moe",
+    "x.mevo.xyz",
+    "x0r.be",
+    "xarxa.cloud",
+    "xi6uivuxxqvjzxx26bumnw6i542umnbqhwbahlj3m5zphk6a5iox6uid.onion",
+    "xmstdn.com",
+    "xn--0n8ha.ws",
+    "xn--69aa8bzb.xn--y9a3aq",
+    "xn--fiqwix98h.jp",
+    "xn--fiz.xn--kst.jp",
+    "xn--gdk7d.de-liker.com",
+    "xn--hea.nz",
+    "xn--ipwu70e9hg.com",
+    "xn--kckk1cy297bor8a.jp",
+    "xn--n8jycee5a4lmeyevltfzc2sja1jw105ewz3i.club",
+    "xn--pnibles-bya.xn--transposes-i7a.eu",
+    "xn--tat-9la.xn--alatoire-c1a.net",
+    "xn--twttr-7raz.com",
+    "xn--uiq.upsilo.net",
+    "xn--uiq450e.club",
+    "xn--w22a.jp",
+    "xn--wmq.jp",
+    "xn--zck4azd638n.com",
+    "xoxo.zone",
+    "xxx.m.to",
+    "xyla.cn",
+    "y.m.to",
+    "y6.org",
+    "yagi.stodon.com",
+    "yakiudon.club",
+    "yakyudon.com",
+    "yakyudon.net",
+    "yamachat.net",
+    "yamagadon.club",
+    "yamagadon.com",
+    "yamagatadon.com",
+    "yamaken.jp",
+    "yamanosusu.me",
+    "yamastodon.com",
+    "yamastodon.jp",
+    "yeahnahcunt.club",
+    "yestadon.site",
+    "yfmmastodon.dip.jp",
+    "yiff.life",
+    "yiff.rocks",
+    "yiff.social",
+    "ykzts.technology",
+    "yoitsu.moe",
+    "yokosima.ddns.net",
+    "yomidon.okinawa",
+    "yontengop.com",
+    "yooidon.jp",
+    "yoshis.woolly.world",
+    "yoso.hostdon.jp",
+    "yotsuba.work",
+    "you-think-your-fake-numbers-are-impressive.well-this-instance-contains-all-living-humans.lubar.me",
+    "yougaku20c.m.to",
+    "youlog.net",
+    "yourtube.yourtilde.com",
+    "youtuberdon.net",
+    "yowapeda.me",
+    "ysd.be",
+    "yso.pet",
+    "yudetarou.club",
+    "yuikaoridon.m.to",
+    "yukari.cafe",
+    "yukari.cloud",
+    "yukarin.club",
+    "yuki.tech",
+    "yukiya.m.to",
+    "yukoayu.m.to",
+    "yuruchara.info",
+    "yuruyuri.family",
+    "yutoridon.com",
+    "yuwinet.win",
+    "yysk.icu",
+    "z-gentoo.ddns.net",
+    "z-socialgame.mstdn.cloud",
+    "z.ian.sh",
+    "zaksoup.club",
+    "zee.li",
+    "zeka.cloud",
+    "zenet.work",
+    "zettelkasten.social",
+    "zfadd.is",
+    "zh-cn.toots.social",
+    "zibksgkpuo.localtunnel.me",
+    "zik.openalgeria.org",
+    "ziroh.be",
+    "zom.ddns.net",
+    "zone53.champemedylan.fr",
+    "zou.social",
+    "zoudon.jp",
+    "zuyadon.tk",
+    "zzz.cat",
+}
+
+__all__ = ['instances']
diff --git a/youtube_dl/extractor/mastodon/mastodon.py b/youtube_dl/extractor/mastodon/mastodon.py
new file mode 100644
index 000000000..317c68889
--- /dev/null
+++ b/youtube_dl/extractor/mastodon/mastodon.py
@@ -0,0 +1,249 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .instances import instances
+from ..common import InfoExtractor
+from ...utils import ExtractorError, clean_html, preferredencoding
+from ...compat import compat_str
+
+
+known_valid_instances = set()
+
+
+class MastodonBaseIE(InfoExtractor):
+
+    @classmethod
+    def suitable(cls, url):
+        mobj = re.match(cls._VALID_URL, url)
+        if not mobj:
+            return False
+        prefix = mobj.group('prefix')
+        hostname = mobj.group('domain')
+        return cls._test_mastodon_instance(None, hostname, True, prefix)
+
+    @staticmethod
+    def _test_mastodon_instance(ie, hostname, skip, prefix):
+        hostname = hostname.encode('idna')
+        if not isinstance(hostname, compat_str):
+            hostname = hostname.decode(preferredencoding())
+
+        if hostname in instances:
+            return True
+        if hostname in known_valid_instances:
+            return True
+
+        # HELP: more cases needed
+        if hostname in ['medium.com', 'lbry.tv']:
+            return False
+
+        # continue anyway if "mastodon:" is added to URL
+        if prefix:
+            return True
+        # without --check-mastodon-instance,
+        #   skip further instance check
+        if skip:
+            return False
+
+        ie.report_warning('Testing if %s is a Mastodon instance because it is not listed in either instances.social or joinmastodon.org.' % hostname)
+
+        try:
+            # try /api/v1/instance
+            api_request_instance = ie._download_json(
+                'https://%s/api/v1/instance' % hostname, hostname,
+                note='Testing Mastodon API /api/v1/instance')
+            if api_request_instance.get('uri') != hostname:
+                return False
+            if not api_request_instance.get('title'):
+                return False
+
+            # try /api/v1/directory
+            api_request_directory = ie._download_json(
+                'https://%s/api/v1/directory' % hostname, hostname,
+                note='Testing Mastodon API /api/v1/directory')
+            if not isinstance(api_request_directory, (tuple, list)):
+                return False
+        except (IOError, ExtractorError):
+            return False
+
+        # this is probably mastodon instance
+        known_valid_instances.add(hostname)
+        return True
+
+
+class MastodonIE(MastodonBaseIE):
+    IE_NAME = 'mastodon'
+    _VALID_URL = r'(?P<prefix>(?:mastodon|mstdn|mtdn):)?https?://(?P<domain>[a-zA-Z0-9._-]+)/(?:@(?P<username>[a-zA-Z0-9_-]+)|web/statuses)/(?P<id>\d+)'
+    _TESTS = [{
+        'note': 'embed video without NSFW',
+        'url': 'https://mstdn.jp/@nao20010128nao/105395495018076252',
+        'info_dict': {
+            'id': '105395495018076252',
+            'title': '„Å¶„Åô„ÇÑ\nhttps://www.youtube.com/watch?v=jx0fBBkaF1w',
+            'uploader': 'nao20010128nao',
+            'uploader_id': 'nao20010128nao',
+            'age_limit': 0,
+        },
+    }, {
+        'note': 'embed video with NSFW',
+        'url': 'https://mstdn.jp/@nao20010128nao/105395503690401921',
+        'info_dict': {
+            'id': '105395503690401921',
+            'title': 'Mastodon„ÉÄ„Ç¶„É≥„É≠„Éº„ÉÄ„Éº„ÅÆ„ÉÜ„Çπ„Éà„Ç±„Éº„ÇπÁî®„Å™„ÅÆ„ÅßÂà•„Å´Ê≥®ÊÑèË¶ÅÁ¥†ÁÑ°„ÅÑ„Çà',
+            'uploader': 'nao20010128nao',
+            'uploader_id': 'nao20010128nao',
+            'age_limit': 18,
+        },
+    }, {
+        'note': 'uploader_id not present in URL',
+        'url': 'https://mstdn.jp/web/statuses/105395503690401921',
+        'info_dict': {
+            'id': '105395503690401921',
+            'title': 'Mastodon„ÉÄ„Ç¶„É≥„É≠„Éº„ÉÄ„Éº„ÅÆ„ÉÜ„Çπ„Éà„Ç±„Éº„ÇπÁî®„Å™„ÅÆ„ÅßÂà•„Å´Ê≥®ÊÑèË¶ÅÁ¥†ÁÑ°„ÅÑ„Çà',
+            'uploader': 'nao20010128nao',
+            'uploader_id': 'nao20010128nao',
+            'age_limit': 18,
+        },
+    }, {
+        'note': 'has YouTube as card',
+        'url': 'https://mstdn.jp/@vaporeon/105389634797745542',
+        'add_ie': ['Youtube'],
+        'info_dict': {},
+    }, {
+        'note': 'has radiko as card',
+        'url': 'https://mstdn.jp/@vaporeon/105389280534065010',
+        'only_matching': True,
+    }, {
+        'url': 'https://pawoo.net/@iriomote_yamaneko/105370643258491818',
+        'only_matching': True,
+    }, {
+        'note': 'uploader_id has only one character',
+        'url': 'https://mstdn.kemono-friends.info/@m/103997543924688111',
+        'info_dict': {
+            'id': '103997543924688111',
+            'uploader_id': 'm',
+        },
+    }]
+
+    def _real_extract(self, url):
+        mobj = re.match(self._VALID_URL, url)
+        domain = mobj.group('domain')
+        uploader_id = mobj.group('username')
+        video_id = mobj.group('id')
+
+        api_response = self._download_json('https://%s/api/v1/statuses/%s' % (domain, video_id), video_id)
+
+        formats = []
+        thumbnail, description = None, None
+        for atch in api_response.get('media_attachments', []):
+            if atch.get('type') != 'video':
+                continue
+            meta = atch.get('meta')
+            if not meta:
+                continue
+            thumbnail = meta.get('preview_url')
+            description = atch.get('description')
+            formats.append({
+                'format_id': atch.get('id'),
+                'protocol': 'http',
+                'url': atch.get('url'),
+                'fps': meta.get('fps'),
+                'width': meta.get('width'),
+                'height': meta.get('height'),
+                'duration': meta.get('duration'),
+            })
+            break
+
+        account, uploader = api_response.get('account'), None
+        if account:
+            uploader = account.get('display_name')
+            uploader_id = uploader_id or account.get('username')
+
+        age_limit = 0
+        if api_response.get('sensitive'):
+            age_limit = 18
+
+        card = api_response.get('card')
+        if not formats and card:
+            return self.url_result(card.get('url'))
+
+        return {
+            'id': video_id,
+            'title': clean_html(api_response.get('content')),
+            'description': description,
+            'formats': formats,
+            'thumbnail': thumbnail,
+            'uploader': uploader,
+            'uploader_id': uploader_id,
+            'age_limit': age_limit,
+        }
+
+
+class MastodonUserIE(MastodonBaseIE):
+    IE_NAME = 'mastodon:user'
+    _VALID_URL = r'(?P<prefix>(?:mastodon|mstdn|mtdn):)?https?://(?P<domain>[a-zA-Z0-9._-]+)/@(?P<id>[a-zA-Z0-9_-]+)/?(?:\?.*)?$'
+    _TESTS = [{
+        'url': 'https://mstdn.jp/@kris57',
+        'info_dict': {
+            'title': 'Toots from @kris57@mstdn.jp',
+            'id': 'kris57',
+        },
+        'playlist_mincount': 261,
+    }, {
+        'url': 'https://pawoo.net/@iriomote_yamaneko',
+        'info_dict': {
+            'title': 'Toots from @iriomote_yamaneko@pawoo.net',
+            'id': 'iriomote_yamaneko',
+        },
+        'playlist_mincount': 80500,
+    }]
+
+    def _real_extract(self, url):
+        mobj = re.match(self._VALID_URL, url)
+        domain = mobj.group('domain')
+        user_id = mobj.group('id')
+
+        # FIXME: filter toots with video or youtube attached
+        # TODO: replace to api calls if possible
+        results = []
+        index = 1
+        next_url = 'https://%s/@%s' % (domain, user_id)
+        while True:
+            webpage = self._download_webpage(next_url, user_id, note='Downloading page %d' % index)
+            for matches in re.finditer(r'(?x)<a class=(["\'])(?:.*?\s+)*status__relative-time(?:\s+.*)*\1\s+(?:rel=(["\'])noopener\2)?\s+href=(["\'])(https://%s/@%s/(\d+))\3>'
+                                       % (re.escape(domain), re.escape(user_id)), webpage):
+                _, _, _, url, video_id = matches.groups()
+                results.append(self.url_result(url, id=video_id))
+            next_url = self._search_regex(
+                # other instances may have different tags
+                # r'<div\s+class=(["\'])entry\1>.*?<a\s+class=(["\'])(?:.*\s+)*load-more(?:\s+.*)*\2\s+href=(["\'])(.+)\3>.+</a></div>\s*</div>',
+                r'class=\"load-more load-gap\" href=\"([^\"]+)\">.+<\/a><\/div>\s*<\/div>',
+                webpage, 'next cursor url', default=None, fatal=False)
+            if not next_url:
+                break
+            index += 1
+
+        return self.playlist_result(results, user_id, 'Toots from @%s@%s' % (user_id, domain))
+
+
+class MastodonUserNumericIE(MastodonBaseIE):
+    IE_NAME = 'mastodon:user:numeric_id'
+    _VALID_URL = r'(?P<prefix>(?:mastodon|mstdn|mtdn):)?https?://(?P<domain>[a-zA-Z0-9._-]+)/web/accounts/(?P<id>\d+)/?'
+    _TESTS = [{
+        'url': 'https://mstdn.jp/web/accounts/330076',
+        'only_matching': True,
+    }]
+
+    def _real_extract(self, url):
+        mobj = re.match(self._VALID_URL, url)
+        prefix = mobj.group('prefix')
+        domain = mobj.group('domain')
+        user_id = mobj.group('id')
+
+        if not prefix and not self._test_mastodon_instance(domain):
+            return self.url_result(url, ie='Generic')
+
+        api_response = self._download_json('https://%s/api/v1/accounts/%s' % (domain, user_id), user_id)
+        username = api_response.get('username')
+        return self.url_result('https://%s/@%s' % (domain, username), video_id=username)
diff --git a/youtube_dl/extractor/meta.py b/youtube_dl/extractor/meta.py
index cdb46e163..6126e1ecd 100644
--- a/youtube_dl/extractor/meta.py
+++ b/youtube_dl/extractor/meta.py
@@ -8,6 +8,9 @@ from ..utils import (
     int_or_none,
     ExtractorError,
 )
+from ..compat import (
+    compat_urllib_parse_urlparse,
+)
 
 
 class METAIE(InfoExtractor):
@@ -61,7 +64,8 @@ class METAIE(InfoExtractor):
                 'duration': int_or_none(self._og_search_property(
                     'video:duration', webpage, default=None)),
             }
-            if 'youtube.com/' in video_url:
+            parsed_url = compat_urllib_parse_urlparse(video_url)
+            if parsed_url.hostname.endswith('youtube.com'):
                 info.update({
                     '_type': 'url_transparent',
                     'ie_key': 'Youtube',
diff --git a/youtube_dl/extractor/mildom.py b/youtube_dl/extractor/mildom.py
new file mode 100644
index 000000000..5fd2da73d
--- /dev/null
+++ b/youtube_dl/extractor/mildom.py
@@ -0,0 +1,252 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from datetime import datetime
+import itertools
+import json
+import base64
+
+from .common import InfoExtractor
+from ..utils import (
+    std_headers,
+    update_url_query,
+    try_get,
+)
+from ..compat import compat_str
+
+
+class MildomBaseIE(InfoExtractor):
+    _GUEST_ID = None
+    _DISPATCHER_CONFIG = None
+
+    def _call_api(self, url, video_id, query={}, note='Downloading JSON metadata', init=False):
+        url = update_url_query(url, self._common_queries(query, init=init))
+        return self._download_json(url, video_id, note=note)['body']
+
+    def _common_queries(self, query={}, init=False):
+        dc = self._fetch_dispatcher_config()
+        r = {
+            'timestamp': self.iso_timestamp(),
+            '__guest_id': '' if init else self.guest_id(),
+            '__location': dc['location'],
+            '__country': dc['country'],
+            '__cluster': dc['cluster'],
+            '__platform': 'web',
+            '__la': 'ja',
+            '__pcv': 'v2.9.44',
+            'sfr': 'pc',
+            'accessToken': '',
+        }
+        r.update(query)
+        return r
+
+    def _fetch_dispatcher_config(self):
+        if not self._DISPATCHER_CONFIG:
+            tmp = self._download_json(
+                'https://disp.mildom.com/serverListV2', 'initialization',
+                note='Downloading dispatcher_config', data=json.dumps({
+                    'protover': 0,
+                    'data': base64.b64encode(json.dumps({
+                        'fr': 'web',
+                        'sfr': 'pc',
+                        'devi': 'Windows',
+                        'la': 'ja',
+                        'gid': None,
+                        'loc': '',
+                        'clu': '',
+                        'wh': '1919*810',  # don't google this magic number!
+                        'rtm': self.iso_timestamp(),
+                        'ua': std_headers['User-Agent'],
+                    }).encode('utf8')).decode('utf8').replace('\n', ''),
+                }).encode('utf8'))
+            self._DISPATCHER_CONFIG = self._parse_json(base64.b64decode(tmp['data']), 'initialization')
+        return self._DISPATCHER_CONFIG
+
+    @staticmethod
+    def iso_timestamp():
+        'new Date().toISOString()'
+        return datetime.utcnow().isoformat()[0:-3] + 'Z'
+
+    def guest_id(self):
+        'getGuestId'
+        if not self._GUEST_ID:
+            self._GUEST_ID = try_get(
+                self, (
+                    lambda x: x._call_api(
+                        'https://cloudac.mildom.com/nonolive/gappserv/guest/h5init', 'initialization',
+                        note='Downloading guest token', init=True)['guest_id'] or None,
+                    lambda x: x._get_cookies('https://www.mildom.com').get('gid').value,
+                    lambda x: x._get_cookies('https://m.mildom.com').get('gid').value,
+                ), compat_str) or ''
+        return self._GUEST_ID
+
+
+class MildomIE(MildomBaseIE):
+    IE_NAME = 'mildom'
+    IE_DESC = 'Record ongoing live by specific user in Mildom'
+    _VALID_URL = r'https?://(?:(?:www|m)\.)?mildom\.com/(?P<id>\d+)'
+
+    def _real_extract(self, url):
+        user_id = self._match_id(url)
+        url = 'https://www.mildom.com/%s' % user_id
+
+        webpage = self._download_webpage(url, user_id)
+
+        enterstudio = self._call_api(
+            'https://cloudac.mildom.com/nonolive/gappserv/live/enterstudio', user_id,
+            note='Downloading live metadata', query={'user_id': user_id})
+        video_id = enterstudio.get('log_id') or user_id
+
+        # e.g. Minecraft
+        title = try_get(
+            enterstudio, (
+                lambda x: self._html_search_meta('twitter:description', webpage),
+                lambda x: x['anchor_intro'],
+            ), compat_str)
+        # e.g. me playing Minecraft
+        description = try_get(
+            enterstudio, (
+                lambda x: x['intro'],
+                lambda x: x['live_intro'],
+            ), compat_str)
+        # e.g. Donald F. McDonald
+        uploader = try_get(
+            enterstudio, (
+                lambda x: self._html_search_meta('twitter:title', webpage),
+                lambda x: x['loginname'],
+            ), compat_str)
+
+        servers = self._call_api(
+            'https://cloudac.mildom.com/nonolive/gappserv/live/liveserver', video_id,
+            note='Downloading live server list', query={
+                'user_id': user_id,
+                'live_server_type': 'hls',
+            })
+
+        m3u8_url = servers['stream_server'] + '/%s_master.m3u8' % user_id
+        formats = self._extract_m3u8_formats(m3u8_url, video_id, 'mp4', headers={
+            'Referer': 'https://www.mildom.com/',
+            'Origin': 'https://www.mildom.com',
+        }, note='Downloading m3u8 information')
+        for fmt in formats:
+            fmt.setdefault('http_headers', {}).update({
+                'Referer': 'https://www.mildom.com/',
+                'Origin': 'https://www.mildom.com',
+            })
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'description': description,
+            'uploader': uploader,
+            'uploader_id': user_id,
+            'formats': formats,
+            'is_live': True,
+        }
+
+
+class MildomVodIE(MildomBaseIE):
+    IE_NAME = 'mildom:vod'
+    IE_DESC = 'Download a VOD in Mildom'
+    _VALID_URL = r'https?://(?:(?:www|m)\.)?mildom\.com/playback/(?P<user_id>\d+)/(?P<id>(?P=user_id)-[a-zA-Z0-9]+)'
+
+    def _real_extract(self, url):
+        m = self._valid_url_re().match(url)
+        user_id, video_id = m.group('user_id'), m.group('id')
+        url = 'https://www.mildom.com/playback/%s/%s' % (user_id, video_id)
+
+        webpage = self._download_webpage(url, video_id)
+
+        autoplay = self._call_api(
+            'https://cloudac.mildom.com/nonolive/videocontent/playback/getPlaybackDetail', video_id,
+            note='Downloading playback metadata', query={
+                'v_id': video_id,
+            })['playback']
+
+        # e.g. Minecraft
+        title = try_get(
+            autoplay, (
+                lambda x: self._html_search_meta('og:description', webpage),
+                lambda x: x['title'],
+            ), compat_str)
+        # e.g. me playing Minecraft
+        description = try_get(
+            autoplay, (
+                lambda x: x['video_intro'],
+            ), compat_str)
+        # e.g. Donald F. McDonald
+        uploader = try_get(
+            autoplay, (
+                lambda x: x['author_info']['login_name'],
+            ), compat_str)
+
+        formats = [{
+            'url': autoplay['audio_url'],
+            'format_id': 'audio',
+            'protocol': 'm3u8_native',
+            'vcodec': 'none',
+            'acodec': 'aac',
+        }]
+        for fmt in autoplay['video_link']:
+            formats.append({
+                'format_id': 'video-%s' % fmt['name'],
+                'url': fmt['url'],
+                'protocol': 'm3u8_native',
+                'width': fmt['level'] * autoplay['video_width'] // autoplay['video_height'],
+                'height': fmt['level'],
+                'vcodec': 'h264',
+                'acodec': 'aac',
+            })
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'description': description,
+            'uploader': uploader,
+            'uploader_id': user_id,
+            'formats': formats,
+        }
+
+
+# User's ongoing live can be done via MildomIE, so this is only for VODs
+class MildomUserVodIE(MildomBaseIE):
+    IE_NAME = 'mildom:user:vod'
+    IE_DESC = 'Download all VODs from specific user in Mildom'
+    _VALID_URL = r'https?://(?:(?:www|m)\.)?mildom\.com/profile/(?P<id>\d+)'
+    _TESTS = [{
+        'url': 'https://www.mildom.com/profile/10093333',
+        'info_dict': {
+            'id': '10093333',
+            'title': 'Uploads from „Å≠„Åì„Å∞„Åü„Åë',
+        },
+        'playlist_mincount': 351,
+    }]
+
+    def _real_extract(self, url):
+        user_id = self._match_id(url)
+
+        self._downloader.report_warning('To download ongoing live, please use "https://www.mildom.com/%s" instead. This will list up VODs belonging to user.' % user_id)
+
+        profile = self._call_api(
+            'https://cloudac.mildom.com/nonolive/gappserv/user/profileV2', user_id,
+            query={'user_id': user_id}, note='Downloading user profile')['user_info']
+
+        results = []
+        for page in itertools.count(1):
+            reply = self._call_api(
+                'https://cloudac.mildom.com/nonolive/videocontent/profile/playbackList',
+                user_id, note='Downloading page %d' % page, query={
+                    'user_id': user_id,
+                    'page': page,
+                    'limit': '30',
+                })
+            if not reply:
+                break
+            results.extend(
+                self.url_result('https://www.mildom.com/playback/%s/%s' % (user_id, x['v_id']), ie=MildomVodIE.ie_key())
+                for x in reply)
+        return self.playlist_result(results, user_id, 'Uploads from %s' % profile['loginname'])
diff --git a/youtube_dl/extractor/mirrativ.py b/youtube_dl/extractor/mirrativ.py
new file mode 100644
index 000000000..ca882c0ae
--- /dev/null
+++ b/youtube_dl/extractor/mirrativ.py
@@ -0,0 +1,95 @@
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+from ..utils import ExtractorError
+
+
+class MirrativBaseIE(InfoExtractor):
+    def assert_error(self, response):
+        error_message = response.get('status', {}).get('error')
+        if error_message:
+            raise ExtractorError('Mirrativ says: %s' % error_message, expected=True)
+
+
+class MirrativIE(MirrativBaseIE):
+    IE_NAME = 'mirrativ'
+    _VALID_URL = r'https?://(?:www.)?mirrativ\.com/live/(?P<id>[^/?#&]+)'
+    LIVE_API_URL = 'https://www.mirrativ.com/api/live/live?live_id=%s'
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage('https://www.mirrativ.com/live/%s' % video_id, video_id)
+        live_response = self._download_json(self.LIVE_API_URL % video_id, video_id)
+        self.assert_error(live_response)
+
+        hls_url = None
+        is_live = False
+        if live_response.get('archive_url_hls'):
+            hls_url = live_response['archive_url_hls']
+        elif live_response.get('streaming_url_hls'):
+            hls_url = live_response['streaming_url_hls']
+            is_live = True
+        else:
+            raise ExtractorError('Live has ended, and has no archive saved', expected=True)
+
+        formats = self._extract_m3u8_formats(
+            hls_url, video_id,
+            ext='mp4', entry_protocol='m3u8_native',
+            m3u8_id='hls', live=is_live)
+
+        title = self._og_search_title(webpage, default=None) or self._search_regex(
+            r'<title>\s*(.+?) - Mirrativ\s*</title>', webpage) or live_response.get('title')
+        description = live_response.get('description')
+        thumbnail = live_response.get('image_url')
+
+        owner = live_response.get('owner', {})
+        uploader = owner.get('name')
+        uploader_id = owner.get('user_id')
+
+        return {
+            'id': video_id,
+            'title': title,
+            'is_live': is_live,
+            'description': description,
+            'formats': formats,
+            'thumbnail': thumbnail,
+            'uploader': uploader,
+            'uploader_id': uploader_id,
+        }
+
+
+class MirrativUserIE(MirrativBaseIE):
+    IE_NAME = 'mirrativ:user'
+    _VALID_URL = r'https?://(?:www.)?mirrativ\.com/user/(?P<id>\d+)'
+    LIVE_HISTORY_API_URL = 'https://www.mirrativ.com/api/live/live_history?user_id=%s&page=%d'
+    USER_INFO_API_URL = 'https://www.mirrativ.com/api/user/profile?user_id=%s'
+
+    def _real_extract(self, url):
+        user_id = self._match_id(url)
+        user_info = self._download_json(
+            self.USER_INFO_API_URL % user_id, user_id,
+            note='Downloading user info', fatal=False)
+        self.assert_error(user_info)
+
+        if user_info:
+            uploader, description = user_info.get('name'), user_info.get('description')
+        else:
+            uploader, description = [None] * 2
+
+        entries = []
+        page = 1
+        while page is not None:
+            api_response = self._download_json(
+                self.LIVE_HISTORY_API_URL % (user_id, page), user_id,
+                note='Downloading page %d' % page)
+            self.assert_error(api_response)
+            lives = api_response.get('lives')
+            if not lives:
+                break
+            for live in lives:
+                live_id = live.get('live_id')
+                url = 'https://www.mirrativ.com/live/%s' % live_id
+                entries.append(self.url_result(url, 'Mirrativ', live_id, live.get('title')))
+            page = api_response.get('next_page')
+
+        return self.playlist_result(entries, user_id, uploader, description)
diff --git a/youtube_dl/extractor/motherless.py b/youtube_dl/extractor/motherless.py
index ef1e081f2..223c659c1 100644
--- a/youtube_dl/extractor/motherless.py
+++ b/youtube_dl/extractor/motherless.py
@@ -11,6 +11,7 @@ from ..utils import (
     orderedSet,
     str_to_int,
     unified_strdate,
+    determine_ext,
 )
 
 
@@ -92,14 +93,26 @@ class MotherlessIE(InfoExtractor):
         if '>The content you are trying to view is for friends only.' in webpage:
             raise ExtractorError('Video %s is for friends only' % video_id, expected=True)
 
+        html5_players = self._parse_html5_media_entries(url, webpage, video_id)
+        formats = None
+        for player in html5_players:
+            # this is needed because header might have <video> tag
+            if any(video_id in x['url'] for x in player['formats']):
+                formats = player['formats']
+        if not formats:
+            video_url = (self._html_search_regex(
+                (r'setup\(\{\s*["\']file["\']\s*:\s*(["\'])(?P<url>(?:(?!\1).)+)\1',
+                 r'fileurl\s*=\s*(["\'])(?P<url>(?:(?!\1).)+)\1'),
+                webpage, 'video URL', default=None, group='url')
+                or 'http://cdn4.videos.motherlessmedia.com/videos/%s.mp4?fs=opencloud' % video_id)
+            formats = [{
+                'url': video_url,
+                'ext': determine_ext(video_url),
+            }]
+
         title = self._html_search_regex(
             (r'(?s)<div[^>]+\bclass=["\']media-meta-title[^>]+>(.+?)</div>',
              r'id="view-upload-title">\s+([^<]+)<'), webpage, 'title')
-        video_url = (self._html_search_regex(
-            (r'setup\(\{\s*["\']file["\']\s*:\s*(["\'])(?P<url>(?:(?!\1).)+)\1',
-             r'fileurl\s*=\s*(["\'])(?P<url>(?:(?!\1).)+)\1'),
-            webpage, 'video URL', default=None, group='url')
-            or 'http://cdn4.videos.motherlessmedia.com/videos/%s.mp4?fs=opencloud' % video_id)
         age_limit = self._rta_search(webpage)
         view_count = str_to_int(self._html_search_regex(
             (r'>([\d,.]+)\s+Views<', r'<strong>Views</strong>\s+([^<]+)<'),
@@ -145,7 +158,7 @@ class MotherlessIE(InfoExtractor):
             'like_count': like_count,
             'comment_count': comment_count,
             'age_limit': age_limit,
-            'url': video_url,
+            'formats': formats,
         }
 
 
diff --git a/youtube_dl/extractor/nbc.py b/youtube_dl/extractor/nbc.py
index 0d77648c2..1644768eb 100644
--- a/youtube_dl/extractor/nbc.py
+++ b/youtube_dl/extractor/nbc.py
@@ -7,7 +7,9 @@ import re
 from .common import InfoExtractor
 from .theplatform import ThePlatformIE
 from .adobepass import AdobePassIE
-from ..compat import compat_urllib_parse_unquote
+from ..compat import (
+    compat_urllib_parse,
+)
 from ..utils import (
     int_or_none,
     parse_duration,
@@ -85,7 +87,7 @@ class NBCIE(AdobePassIE):
 
     def _real_extract(self, url):
         permalink, video_id = re.match(self._VALID_URL, url).groups()
-        permalink = 'http' + compat_urllib_parse_unquote(permalink)
+        permalink = 'http' + compat_urllib_parse.unquote(permalink)
         video_data = self._download_json(
             'https://friendship.nbc.co/v2/graphql', video_id, query={
                 'query': '''query bonanzaPage(
@@ -388,9 +390,10 @@ class NBCNewsIE(ThePlatformIE):
         formats = []
         for va in video_data.get('videoAssets', []):
             public_url = va.get('publicUrl')
+            parsed_url = compat_urllib_parse.urlparse(public_url)
             if not public_url:
                 continue
-            if '://link.theplatform.com/' in public_url:
+            if parsed_url.hostname == 'link.theplatform.com':
                 public_url = update_url_query(public_url, {'format': 'redirect'})
             format_id = va.get('format')
             if format_id == 'M3U':
diff --git a/youtube_dl/extractor/niconico.py b/youtube_dl/extractor/niconico.py
index a85fc3d5c..5552183f7 100644
--- a/youtube_dl/extractor/niconico.py
+++ b/youtube_dl/extractor/niconico.py
@@ -3,16 +3,23 @@ from __future__ import unicode_literals
 
 import datetime
 import functools
-import json
 import math
+import re
+try:
+    import dateutil.parser
+    HAVE_DATEUTIL = True
+except (ImportError, SyntaxError):
+    # dateutil is optional
+    HAVE_DATEUTIL = False
 
 from .common import InfoExtractor
 from ..compat import (
+    compat_str,
     compat_parse_qs,
     compat_urllib_parse_urlparse,
+    compat_HTTPError,
 )
 from ..utils import (
-    determine_ext,
     dict_get,
     ExtractorError,
     float_or_none,
@@ -22,13 +29,25 @@ from ..utils import (
     parse_iso8601,
     remove_start,
     try_get,
+    unescapeHTML,
     unified_timestamp,
     urlencode_postdata,
-    xpath_text,
+    update_url_query,
+    time_millis,
 )
 
 
-class NiconicoIE(InfoExtractor):
+class NiconicoBaseIE(InfoExtractor):
+    _API_HEADERS = {
+        'X-Frontend-ID': '6',
+        'X-Frontend-Version': '0',
+        'X-Niconico-Language': 'en-us',
+        'Referer': 'https://www.nicovideo.jp/',
+        'Origin': 'https://www.nicovideo.jp',
+    }
+
+
+class NiconicoIE(NiconicoBaseIE):
     IE_NAME = 'niconico'
     IE_DESC = '„Éã„Ç≥„Éã„Ç≥ÂãïÁîª'
 
@@ -157,11 +176,37 @@ class NiconicoIE(InfoExtractor):
     }, {
         'url': 'http://sp.nicovideo.jp/watch/sm28964488?ss_pos=1&cp_in=wt_tg',
         'only_matching': True,
+    }, {
+        'note': 'a video that is only served as an ENCRYPTED HLS.',
+        'url': 'https://www.nicovideo.jp/watch/so38016254',
+        'only_matching': True,
+    }, {
+        'url': 'nico:sm25182253',
+        'only_matching': True,
+    }, {
+        'url': 'niconico:sm25182253',
+        'only_matching': True,
+    }, {
+        'url': 'nicovideo:sm25182253',
+        'only_matching': True,
     }]
 
-    _VALID_URL = r'https?://(?:www\.|secure\.|sp\.)?nicovideo\.jp/watch/(?P<id>(?:[a-z]{2})?[0-9]+)'
+    _URL_BEFORE_ID_PART = r'(?:https?://(?:(?:www\.|secure\.|sp\.)?nicovideo\.jp/watch|nico\.ms)/|nico(?:nico|video)?:)'
+    _VALID_URL = r'%s(?P<id>(?P<alphabet>[a-z]{2})?[0-9]+)' % _URL_BEFORE_ID_PART
     _NETRC_MACHINE = 'niconico'
 
+    @classmethod
+    def suitable(cls, url):
+        m = cls._valid_url_re().match(url)
+        if not m:
+            return False
+        if m.group('alphabet') == 'lv':
+            # niconico:live should take place
+            return False
+        # The only case that 'id_alphabet' never matches is channel-belonging video (which usually starts with 'so'),
+        # but in this case NiconicoIE can handle it
+        return True
+
     def _real_initialize(self):
         self._login()
 
@@ -177,10 +222,17 @@ class NiconicoIE(InfoExtractor):
             'mail_tel': username,
             'password': password,
         }
+        self._request_webpage(
+            'https://account.nicovideo.jp/login', None,
+            note='Acquiring Login session')
         urlh = self._request_webpage(
-            'https://account.nicovideo.jp/api/v1/login', None,
+            'https://account.nicovideo.jp/login/redirector?show_button_twitter=1&site=niconico&show_button_facebook=1', None,
             note='Logging in', errnote='Unable to log in',
-            data=urlencode_postdata(login_form_strs))
+            data=urlencode_postdata(login_form_strs),
+            headers={
+                'Referer': 'https://account.nicovideo.jp/login',
+                'Content-Type': 'application/x-www-form-urlencoded',
+            })
         if urlh is False:
             login_ok = False
         else:
@@ -191,32 +243,84 @@ class NiconicoIE(InfoExtractor):
             self._downloader.report_warning('unable to log in: bad username or password')
         return login_ok
 
-    def _extract_format_for_quality(self, api_data, video_id, audio_quality, video_quality):
+    def _extract_format_for_quality(self, api_data, video_id, audio_quality, video_quality, dmc_protocol):
         def yesno(boolean):
             return 'yes' if boolean else 'no'
 
-        session_api_data = api_data['video']['dmcInfo']['session_api']
-        session_api_endpoint = session_api_data['urls'][0]
-
-        format_id = '-'.join(map(lambda s: remove_start(s['id'], 'archive_'), [video_quality, audio_quality]))
+        def extract_video_quality(video_quality):
+            try:
+                # Example: 480p | 0.9M
+                r = re.match(r'^.*\| ([0-9]*\.?[0-9]*[MK])', video_quality)
+                if r is None:
+                    # Maybe conditionally throw depending on the settings?
+                    return 0
+
+                vbr_with_unit = r.group(1)
+                unit = vbr_with_unit[-1]
+                video_bitrate = float(vbr_with_unit[:-1])
+
+                if unit == 'M':
+                    video_bitrate *= 1000000
+                elif unit == 'K':
+                    video_bitrate *= 1000
+
+                return video_bitrate
+            except BaseException:
+                # Should at least log or something here
+                return 0
+
+        session_api_data = api_data['media']['delivery']['movie']['session']
+
+        format_id = '-'.join(
+            [remove_start(s['id'], 'archive_') for s in (video_quality, audio_quality)] + [dmc_protocol])
+
+        extract_m3u8 = False
+        if dmc_protocol == 'http':
+            protocol = 'http'
+            protocol_parameters = {
+                'http_output_download_parameters': {
+                    'use_ssl': yesno(session_api_data['urls'][0]['isSsl']),
+                    'use_well_known_port': yesno(session_api_data['urls'][0]['isWellKnownPort']),
+                }
+            }
+        elif dmc_protocol == 'hls':
+            protocol = 'm3u8'
+            parsed_token = self._parse_json(session_api_data['token'], video_id)
+            encryption = try_get(api_data, lambda x: x['media']['delivery']['encryption'], dict)
+            protocol_parameters = {
+                'hls_parameters': {
+                    'segment_duration': 6000,
+                    'transfer_preset': '',
+                    'use_ssl': yesno(session_api_data['urls'][0]['isSsl']),
+                    'use_well_known_port': yesno(session_api_data['urls'][0]['isWellKnownPort']),
+                }
+            }
+            if 'hls_encryption' in parsed_token and encryption:
+                protocol_parameters['hls_parameters']['encryption'] = {
+                    parsed_token['hls_encryption']: {
+                        'encrypted_key': encryption['encryptedKey'],
+                        'key_uri': encryption['keyUri'],
+                    }
+                }
+            else:
+                protocol = 'm3u8_native'
+                extract_m3u8 = True
+        else:
+            return None
 
-        session_response = self._download_json(
-            session_api_endpoint['url'], video_id,
-            query={'_format': 'json'},
-            headers={'Content-Type': 'application/json'},
-            note='Downloading JSON metadata for %s' % format_id,
-            data=json.dumps({
+        if True:  # indent this for mergeability
+            dmc_data = {
                 'session': {
                     'client_info': {
-                        'player_id': session_api_data['player_id'],
+                        'player_id': session_api_data['playerId'],
                     },
                     'content_auth': {
-                        'auth_type': session_api_data['auth_types'][session_api_data['protocols'][0]],
-                        'content_key_timeout': session_api_data['content_key_timeout'],
+                        'auth_type': session_api_data['authTypes'][session_api_data['protocols'][0]],
+                        'content_key_timeout': session_api_data['contentKeyTimeout'],
                         'service_id': 'nicovideo',
-                        'service_user_id': session_api_data['service_user_id']
+                        'service_user_id': session_api_data['serviceUserId']
                     },
-                    'content_id': session_api_data['content_id'],
+                    'content_id': session_api_data['contentId'],
                     'content_src_id_sets': [{
                         'content_src_ids': [{
                             'src_id_to_mux': {
@@ -229,7 +333,7 @@ class NiconicoIE(InfoExtractor):
                     'content_uri': '',
                     'keep_method': {
                         'heartbeat': {
-                            'lifetime': session_api_data['heartbeat_lifetime']
+                            'lifetime': session_api_data['heartbeatLifetime']
                         }
                     },
                     'priority': session_api_data['priority'],
@@ -237,16 +341,11 @@ class NiconicoIE(InfoExtractor):
                         'name': 'http',
                         'parameters': {
                             'http_parameters': {
-                                'parameters': {
-                                    'http_output_download_parameters': {
-                                        'use_ssl': yesno(session_api_endpoint['is_ssl']),
-                                        'use_well_known_port': yesno(session_api_endpoint['is_well_known_port']),
-                                    }
-                                }
+                                'parameters': protocol_parameters
                             }
                         }
                     },
-                    'recipe_id': session_api_data['recipe_id'],
+                    'recipe_id': session_api_data['recipeId'],
                     'session_operation_auth': {
                         'session_operation_auth_by_signature': {
                             'signature': session_api_data['signature'],
@@ -255,18 +354,36 @@ class NiconicoIE(InfoExtractor):
                     },
                     'timing_constraint': 'unlimited'
                 }
-            }).encode())
+            }
 
-        resolution = video_quality.get('resolution', {})
+        resolution = video_quality['metadata'].get('resolution', {})
+        vid_quality = video_quality['metadata'].get('bitrate')
+        is_low = 'low' in video_quality['id']
 
         return {
-            'url': session_response['data']['session']['content_uri'],
+            'url': session_api_data['urls'][0]['url'],
             'format_id': format_id,
+            'format_note': 'DMC ' + video_quality['metadata']['label'] + ' ' + dmc_protocol.upper(),
             'ext': 'mp4',  # Session API are used in HTML5, which always serves mp4
-            'abr': float_or_none(audio_quality.get('bitrate'), 1000),
-            'vbr': float_or_none(video_quality.get('bitrate'), 1000),
+            'acodec': 'aac',
+            'vcodec': 'h264',  # As far as I'm aware DMC videos can only serve h264/aac combinations
+            'abr': float_or_none(audio_quality['metadata'].get('bitrate'), 1000),
+            # So this is kind of a hack; sometimes, the bitrate is incorrectly reported as 0kbs. If this is the case,
+            # extract it from the rest of the metadata we have available
+            'vbr': float_or_none(vid_quality if vid_quality > 0 else extract_video_quality(video_quality['metadata'].get('label')), 1000),
             'height': resolution.get('height'),
             'width': resolution.get('width'),
+            'quality': -2 if is_low else None,
+            'protocol': 'niconico_dmc',
+            'expected_protocol': protocol,
+            'session_api_data': session_api_data,
+            'dmc_data': dmc_data,
+            'video_id': video_id,
+            'extract_m3u8': extract_m3u8,
+            'http_headers': {
+                'Origin': 'https://www.nicovideo.jp',
+                'Referer': 'https://www.nicovideo.jp/watch/' + video_id,
+            }
         }
 
     def _real_extract(self, url):
@@ -275,95 +392,64 @@ class NiconicoIE(InfoExtractor):
         # Get video webpage. We are not actually interested in it for normal
         # cases, but need the cookies in order to be able to download the
         # info webpage
-        webpage, handle = self._download_webpage_handle(
-            'http://www.nicovideo.jp/watch/' + video_id, video_id)
-        if video_id.startswith('so'):
-            video_id = self._match_id(handle.geturl())
-
-        api_data = self._parse_json(self._html_search_regex(
-            'data-api-data="([^"]+)"', webpage,
-            'API data', default='{}'), video_id)
-
-        def _format_id_from_url(video_url):
-            return 'economy' if video_real_url.endswith('low') else 'normal'
-
         try:
-            video_real_url = api_data['video']['smileInfo']['url']
-        except KeyError:  # Flash videos
-            # Get flv info
-            flv_info_webpage = self._download_webpage(
-                'http://flapi.nicovideo.jp/api/getflv/' + video_id + '?as3=1',
-                video_id, 'Downloading flv info')
-
-            flv_info = compat_parse_qs(flv_info_webpage)
-            if 'url' not in flv_info:
-                if 'deleted' in flv_info:
-                    raise ExtractorError('The video has been deleted.',
-                                         expected=True)
-                elif 'closed' in flv_info:
-                    raise ExtractorError('Niconico videos now require logging in',
-                                         expected=True)
-                elif 'error' in flv_info:
-                    raise ExtractorError('%s reports error: %s' % (
-                        self.IE_NAME, flv_info['error'][0]), expected=True)
+            webpage, handle = self._download_webpage_handle(
+                'http://www.nicovideo.jp/watch/' + video_id, video_id)
+            if video_id.startswith('so'):
+                video_id = self._match_id(handle.geturl())
+
+            api_data = self._parse_json(self._html_search_regex(
+                'data-api-data="([^"]+)"', webpage,
+                'API data', default='{}'), video_id)
+        except ExtractorError as e:
+            try:
+                api_data = self._download_json(
+                    'https://www.nicovideo.jp/api/watch/v3/%s?_frontendId=6&_frontendVersion=0&actionTrackId=AAAAAAAAAA_%d' % (video_id, time_millis()), video_id,
+                    note='Downloading API JSON', errnote='Unable to fetch data')['data']
+            except (ExtractorError, KeyError):
+                if not isinstance(e.cause, compat_HTTPError):
+                    raise e
+                else:
+                    e = e.cause
+                webpage = e.read().decode('utf-8', 'replace')
+                error_msg = self._html_search_regex(
+                    r'(?s)<section\s+class="(?:(?:ErrorMessage|WatchExceptionPage-message)\s*)+">(.+?)</section>',
+                    webpage, 'error reason', group=1, default=None)
+                if not error_msg:
+                    raise e
                 else:
-                    raise ExtractorError('Unable to find video URL')
-
-            video_info_xml = self._download_xml(
-                'http://ext.nicovideo.jp/api/getthumbinfo/' + video_id,
-                video_id, note='Downloading video info page')
-
-            def get_video_info(items):
-                if not isinstance(items, list):
-                    items = [items]
-                for item in items:
-                    ret = xpath_text(video_info_xml, './/' + item)
-                    if ret:
-                        return ret
-
-            video_real_url = flv_info['url'][0]
-
-            extension = get_video_info('movie_type')
-            if not extension:
-                extension = determine_ext(video_real_url)
-
-            formats = [{
-                'url': video_real_url,
-                'ext': extension,
-                'format_id': _format_id_from_url(video_real_url),
-            }]
+                    error_msg = re.sub(r'\s+', ' ', error_msg)
+                    raise ExtractorError(error_msg, expected=True)
+
+        formats = []
+
+        def get_video_info(items):
+            return dict_get(api_data['video'], items)
+
+        # dmc_info = api_data['video'].get('dmcInfo')
+        quality_info = api_data['media']['delivery']['movie']
+        session_api_data = api_data['media']['delivery']['movie']['session']
+        if quality_info:  # "New" HTML5 videos
+            for audio_quality in quality_info['audios']:
+                for video_quality in quality_info['videos']:
+                    if not audio_quality['isAvailable'] or not video_quality['isAvailable']:
+                        continue
+                    for protocol in session_api_data['protocols']:
+                        fmt = self._extract_format_for_quality(api_data, video_id, audio_quality, video_quality, protocol)
+                        if fmt:
+                            formats.append(fmt)
+
+            self._sort_formats(formats)
         else:
-            formats = []
-
-            dmc_info = api_data['video'].get('dmcInfo')
-            if dmc_info:  # "New" HTML5 videos
-                quality_info = dmc_info['quality']
-                for audio_quality in quality_info['audios']:
-                    for video_quality in quality_info['videos']:
-                        if not audio_quality['available'] or not video_quality['available']:
-                            continue
-                        formats.append(self._extract_format_for_quality(
-                            api_data, video_id, audio_quality, video_quality))
-
-                self._sort_formats(formats)
-            else:  # "Old" HTML5 videos
-                formats = [{
-                    'url': video_real_url,
-                    'ext': 'mp4',
-                    'format_id': _format_id_from_url(video_real_url),
-                }]
-
-            def get_video_info(items):
-                return dict_get(api_data['video'], items)
+            raise ExtractorError('Failed to extract formats')
 
         # Start extracting information
-        title = get_video_info('title')
-        if not title:
-            title = self._og_search_title(webpage, default=None)
-        if not title:
-            title = self._html_search_regex(
+        title = (
+            get_video_info(['originalTitle', 'title'])
+            or self._og_search_title(webpage, default=None)
+            or self._html_search_regex(
                 r'<span[^>]+class="videoHeaderTitle"[^>]*>([^<]+)</span>',
-                webpage, 'video title')
+                webpage, 'video title'))
 
         watch_api_data_string = self._html_search_regex(
             r'<div[^>]+id="watchAPIDataContainer"[^>]+>([^<]+)</div>',
@@ -372,14 +458,33 @@ class NiconicoIE(InfoExtractor):
         video_detail = watch_api_data.get('videoDetail', {})
 
         thumbnail = (
-            get_video_info(['thumbnail_url', 'thumbnailURL'])
-            or self._html_search_meta('image', webpage, 'thumbnail', default=None)
-            or video_detail.get('thumbnail'))
+            self._html_search_regex(r'<meta property="og:image" content="([^"]+)">', webpage, 'thumbnail data', default=None)
+            or api_data['video'].get('largeThumbnailURL')
+            or api_data['video'].get('thumbnailURL')
+            or get_video_info(['largeThumbnailURL', 'thumbnail_url', 'thumbnailURL'])
+            or self._html_search_meta('image', webpage, 'thumbnail', default=None))
+
+        match = self._html_search_meta('datePublished', webpage, 'date published', default=None)
+        if match:
+            timestamp = parse_iso8601(match.replace('+', ':00+'))
+        else:
+            date = api_data['video']['registeredAt']
+            # FIXME see animelover1984/youtube-dl
+            if HAVE_DATEUTIL:
+                timestamp = math.floor(dateutil.parser.parse(date).timestamp())
+            else:
+                timestamp = None
+
+        view_count = int_or_none(api_data['video']['count'].get('view'))
 
-        description = get_video_info('description')
+        description = (
+            api_data['video'].get('description')
+            # this cannot be checked before the JSON API check as on community videos the description is simply "community"
+            or get_video_info('description'))
 
-        timestamp = (parse_iso8601(get_video_info('first_retrieve'))
-                     or unified_timestamp(get_video_info('postedDateTime')))
+        if not timestamp:
+            timestamp = (parse_iso8601(get_video_info('first_retrieve'))
+                         or unified_timestamp(get_video_info('postedDateTime')))
         if not timestamp:
             match = self._html_search_meta('datePublished', webpage, 'date published', default=None)
             if match:
@@ -389,18 +494,9 @@ class NiconicoIE(InfoExtractor):
                 video_detail['postedAt'].replace('/', '-'),
                 delimiter=' ', timezone=datetime.timedelta(hours=9))
 
-        view_count = int_or_none(get_video_info(['view_counter', 'viewCount']))
-        if not view_count:
-            match = self._html_search_regex(
-                r'>Views: <strong[^>]*>([^<]+)</strong>',
-                webpage, 'view count', default=None)
-            if match:
-                view_count = int_or_none(match.replace(',', ''))
-        view_count = view_count or video_detail.get('viewCount')
-
-        comment_count = (int_or_none(get_video_info('comment_num'))
-                         or video_detail.get('commentCount')
-                         or try_get(api_data, lambda x: x['thread']['commentCount']))
+        comment_count = (
+            api_data['video']['count'].get('comment')
+            or try_get(api_data, lambda x: x['thread']['commentCount']))
         if not comment_count:
             match = self._html_search_regex(
                 r'>Comments: <strong[^>]*>([^<]+)</strong>',
@@ -408,10 +504,10 @@ class NiconicoIE(InfoExtractor):
             if match:
                 comment_count = int_or_none(match.replace(',', ''))
 
-        duration = (parse_duration(
-            get_video_info('length')
-            or self._html_search_meta(
-                'video:duration', webpage, 'video duration', default=None))
+        duration = (
+            parse_duration(
+                get_video_info('length')
+                or self._html_search_meta('video:duration', webpage, 'video duration', default=None))
             or video_detail.get('length')
             or get_video_info('duration'))
 
@@ -419,10 +515,24 @@ class NiconicoIE(InfoExtractor):
 
         # Note: cannot use api_data.get('owner', {}) because owner may be set to "null"
         # in the JSON, which will cause None to be returned instead of {}.
-        owner = try_get(api_data, lambda x: x.get('owner'), dict) or {}
+        owner = try_get(api_data, lambda x: x['owner'], dict) or {}
         uploader_id = get_video_info(['ch_id', 'user_id']) or owner.get('id')
         uploader = get_video_info(['ch_name', 'user_nickname']) or owner.get('nickname')
 
+        tags = api_data['video'].get('tags') or []
+        genre = get_video_info('genre')
+
+        tracking_id = try_get(api_data, lambda x: x['media']['delivery']['trackingId'], compat_str)
+        if tracking_id:
+            tracking_url = update_url_query('https://nvapi.nicovideo.jp/v1/2ab0cbaa/watch', {'t': tracking_id})
+            # TODO: may need to simulate https://public.api.nicovideo.jp/v1/user/actions/video/watch-events.json?__retry=0 ?
+            watch_request_response = self._download_json(
+                tracking_url, video_id,
+                note='Acquiring permission for downloading video', fatal=False,
+                headers=self._API_HEADERS)
+            if try_get(watch_request_response, lambda x: x['meta']['status'], int) != 200:
+                self.report_warning('Failed to acquire permission for playing video. The video may not download.')
+
         return {
             'id': video_id,
             'title': title,
@@ -433,36 +543,30 @@ class NiconicoIE(InfoExtractor):
             'timestamp': timestamp,
             'uploader_id': uploader_id,
             'view_count': view_count,
+            'tags': tags,
+            'genre': genre,
             'comment_count': comment_count,
             'duration': duration,
             'webpage_url': webpage_url,
         }
 
 
-class NiconicoPlaylistIE(InfoExtractor):
-    _VALID_URL = r'https?://(?:www\.)?nicovideo\.jp/(?:user/\d+/)?mylist/(?P<id>\d+)'
-
-    _TESTS = [{
-        'url': 'http://www.nicovideo.jp/mylist/27411728',
-        'info_dict': {
-            'id': '27411728',
-            'title': 'AKB48„ÅÆ„Ç™„Éº„É´„Éä„Ç§„Éà„Éã„ÉÉ„Éù„É≥',
-            'description': 'md5:d89694c5ded4b6c693dea2db6e41aa08',
-            'uploader': '„ÅÆ„Å£„Åè',
-            'uploader_id': '805442',
-        },
-        'playlist_mincount': 225,
-    }, {
-        'url': 'https://www.nicovideo.jp/user/805442/mylist/27411728',
-        'only_matching': True,
-    }]
+class NiconicoPlaylistBaseIE(NiconicoBaseIE):
     _PAGE_SIZE = 100
 
+    _API_HEADERS = {
+        'X-Frontend-ID': '6',
+        'X-Frontend-Version': '0',
+        'X-Niconico-Language': 'en-us'
+    }
+
     def _call_api(self, list_id, resource, query):
-        return self._download_json(
-            'https://nvapi.nicovideo.jp/v2/mylists/' + list_id, list_id,
-            'Downloading %s JSON metatdata' % resource, query=query,
-            headers={'X-Frontend-Id': 6})['data']['mylist']
+        "Implement this in child class"
+        pass
+
+    @staticmethod
+    def _get_video_item(video):
+        return video.get('video')
 
     def _parse_owner(self, item):
         owner = item.get('owner') or {}
@@ -479,8 +583,9 @@ class NiconicoPlaylistIE(InfoExtractor):
             'page': page,
             'pageSize': self._PAGE_SIZE,
         })['items']
-        for item in items:
-            video = item.get('video') or {}
+        for video in items:
+            # this is needed to support both mylist and user
+            video = self._get_video_item(video)
             video_id = video.get('id')
             if not video_id:
                 continue
@@ -500,6 +605,32 @@ class NiconicoPlaylistIE(InfoExtractor):
             info.update(self._parse_owner(video))
             yield info
 
+
+class NiconicoPlaylistIE(NiconicoPlaylistBaseIE):
+    IE_NAME = 'niconico:playlist'
+    _VALID_URL = r'https?://(?:(?:www\.|sp\.)?nicovideo\.jp|nico\.ms)/(?:user/\d+/)?(?:my/)?mylist/(?P<id>\d+)'
+
+    _TESTS = [{
+        'url': 'http://www.nicovideo.jp/mylist/27411728',
+        'info_dict': {
+            'id': '27411728',
+            'title': 'AKB48„ÅÆ„Ç™„Éº„É´„Éä„Ç§„Éà„Éã„ÉÉ„Éù„É≥',
+            'description': 'md5:d89694c5ded4b6c693dea2db6e41aa08',
+            'uploader': '„ÅÆ„Å£„Åè',
+            'uploader_id': '805442',
+        },
+        'playlist_mincount': 225,
+    }, {
+        'url': 'https://www.nicovideo.jp/user/805442/mylist/27411728',
+        'only_matching': True,
+    }]
+
+    def _call_api(self, list_id, resource, query):
+        return self._download_json(
+            'https://nvapi.nicovideo.jp/v2/mylists/' + list_id, list_id,
+            'Downloading %s' % resource, query=query,
+            headers=self._API_HEADERS)['data']['mylist']
+
     def _real_extract(self, url):
         list_id = self._match_id(url)
         mylist = self._call_api(list_id, 'list', {
@@ -513,3 +644,149 @@ class NiconicoPlaylistIE(InfoExtractor):
             entries, list_id, mylist.get('name'), mylist.get('description'))
         result.update(self._parse_owner(mylist))
         return result
+
+
+class NiconicoUserIE(NiconicoPlaylistBaseIE):
+    IE_NAME = 'niconico:user'
+    _VALID_URL = r'https?://(?:(?:www\.|sp\.)?nicovideo\.jp|nico\.ms)/user/(?P<id>\d+)'
+
+    _TESTS = [{
+        'url': 'https://www.nicovideo.jp/user/17988631',
+        'info_dict': {
+            'id': '17988631',
+            'title': 'USAGE',
+        },
+        'playlist_mincount': 37,  # as of 2021/01/13
+    }, {
+        'url': 'https://www.nicovideo.jp/user/1050860/video',
+        'info_dict': {
+            'id': '1050860',
+            'title': 'Ëä±„Åü„Çì„Å®„Åã„É¶„É™„Ç´„Å®„Åã‚úø',
+        },
+        'playlist_mincount': 165,  # as of 2021/04/04
+    }, {
+        'url': 'https://www.nicovideo.jp/user/805442/',
+        'only_matching': True,
+    }, {
+        'url': 'https://nico.ms/user/805442/',
+        'only_matching': True,
+    }]
+
+    @classmethod
+    def suitable(cls, url):
+        return super(NiconicoUserIE, cls).suitable(url) and not NiconicoPlaylistIE.suitable(url)
+
+    def _call_api(self, list_id, resource, query):
+        return self._download_json(
+            'https://nvapi.nicovideo.jp/v1/users/%s/videos' % list_id, list_id,
+            'Downloading %s' % resource, query=query,
+            headers=self._API_HEADERS)['data']
+
+    @staticmethod
+    def _get_video_item(video):
+        return video
+
+    def _real_extract(self, url):
+        list_id = self._match_id(url)
+
+        user_webpage = self._download_webpage('https://www.nicovideo.jp/user/%s' % list_id, list_id)
+        user_info = self._search_regex(r'<div id="js-initial-userpage-data" .+? data-initial-data="(.+)?"', user_webpage, 'user info', default={})
+        user_info = unescapeHTML(user_info)
+        user_info = self._parse_json(user_info, list_id)
+        user_info = try_get(user_info, lambda x: x['userDetails']['userDetails']['user'], dict) or {}
+
+        mylist = self._call_api(list_id, 'list', {
+            'pageSize': 1,
+        })
+        entries = InAdvancePagedList(
+            functools.partial(self._fetch_page, list_id),
+            math.ceil(mylist['totalCount'] / self._PAGE_SIZE),
+            self._PAGE_SIZE)
+        result = self.playlist_result(
+            entries, list_id, user_info.get('nickname'), user_info.get('strippedDescription'))
+        result.update(self._parse_owner(mylist))
+        return result
+
+
+# cannot use NiconicoPlaylistBaseIE because /series/ has different structure than others
+class NiconicoSeriesIE(NiconicoBaseIE):
+    IE_NAME = 'niconico:series'
+    _VALID_URL = r'https?://(?:(?:www\.|sp\.)?nicovideo\.jp|nico\.ms)/series/(?P<id>\d+)'
+
+    _TESTS = [{
+        'url': 'https://www.nicovideo.jp/series/110226',
+        'info_dict': {
+            'id': '110226',
+            'title': '„ÅîÁ´ãÊ¥æ„Ç°ÔºÅ„ÅÆ„Ç∑„É™„Éº„Ç∫',
+        },
+        'playlist_mincount': 10,  # as of 2021/03/17
+    }, {
+        'url': 'https://www.nicovideo.jp/series/12312/',
+        'info_dict': {
+            'id': '12312',
+            'title': '„Éê„Éà„É´„Çπ„Éî„É™„ÉÉ„ÉÑ„ÄÄ„ÅäÂãß„ÇÅ„Ç´„Éº„ÉâÁ¥π‰ªã(Ë™øÊï¥‰∏≠)',
+        },
+        'playlist_mincount': 97,  # as of 2021/03/17
+    }, {
+        'url': 'https://nico.ms/series/203559',
+        'only_matching': True,
+    }]
+
+    def _real_extract(self, url):
+        list_id = self._match_id(url)
+        webpage = self._download_webpage('https://www.nicovideo.jp/series/%s' % list_id, list_id)
+
+        title = self._search_regex(
+            (r'<title>„Äå(.+)ÔºàÂÖ®',
+             r'<div class="TwitterShareButton"\s+data-text="(.+)\s+https:'),
+            webpage, 'title', fatal=False)
+        if title:
+            title = unescapeHTML(title)
+        playlist = []
+        for match in re.finditer(r'<a href="/watch/([a-z0-9]+)" data-href="/watch/\1', webpage):
+            playlist.append(self.url_result('https://www.nicovideo.jp/watch/%s' % match.group(1), video_id=match.group(1)))
+        return self.playlist_result(playlist, list_id, title)
+
+
+class NiconicoLiveIE(NiconicoBaseIE):
+    IE_NAME = 'niconico:live'
+    IE_DESC = '„Éã„Ç≥„Éã„Ç≥ÁîüÊîæÈÄÅ'
+    _VALID_URL = r'(?:https?://(?:sp\.)?live2?\.nicovideo\.jp/(?:watch|gate)/|nico(?:nico|video)?:)(?P<id>lv\d+)'
+    _FEATURE_DEPENDENCY = ('websocket', )
+
+    # sort qualities in this order to trick youtube-dl to download highest quality as default
+    _KNOWN_QUALITIES = ('abr', 'super_low', 'low', 'normal', 'high', 'super_high')
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage('https://live2.nicovideo.jp/watch/%s' % video_id, video_id)
+
+        embedded_data = self._search_regex(r'<script\s+id="embedded-data"\s*data-props="(.+?)"', webpage, 'embedded data')
+        embedded_data = unescapeHTML(embedded_data)
+        embedded_data = self._parse_json(embedded_data, video_id)
+
+        ws_url = embedded_data['site']['relive']['webSocketUrl']
+        if not ws_url:
+            raise ExtractorError('the live hasn\'t started yet or already ended', expected=True)
+
+        title = try_get(
+            None,
+            (lambda x: embedded_data['program']['title'],
+             lambda x: self._html_search_meta(('og:title', 'twitter:title'), webpage, 'live title', fatal=False)),
+            compat_str)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'formats': [{
+                'url': ws_url,
+                'protocol': 'niconico_live',
+                'format_id': '%s' % quality,
+                'video_id': video_id,
+                'cookies': str(self._get_cookies('https://live2.nicovideo.jp/')).replace('Set-Cookie: ', ''),
+                'ext': 'mp4',
+                'is_live': True,
+                'live_quality': quality,
+            } for quality in self._KNOWN_QUALITIES],
+            'is_live': True,
+        }
diff --git a/youtube_dl/extractor/niconico_smile.py b/youtube_dl/extractor/niconico_smile.py
new file mode 100644
index 000000000..d18ad9949
--- /dev/null
+++ b/youtube_dl/extractor/niconico_smile.py
@@ -0,0 +1,81 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+from .niconico import NiconicoIE
+from ..utils import (
+    lowercase_escape,
+    try_get,
+)
+from ..compat import (
+    compat_parse_qs,
+    compat_urllib_parse,
+    compat_str,
+)
+
+
+class NiconicoSmileIE(InfoExtractor):
+    IE_NAME = 'niconico:smile'
+    _VALID_URL = r'(?:https?://www\.nicozon\.net/downloader\.html\?video_id=|nicozon:(?:%s)?)(?P<id>(?:[a-z]{2})?[0-9]+)' % NiconicoIE._URL_BEFORE_ID_PART
+    IE10_USERAGENT = 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)'
+    _WORKING = False  # This will never be removed
+    _FEATURE_DEPENDENCY = ('yaml', )
+
+    def _real_extract(self, url):
+        # NOTE: If you're interested in SWF file and decompiled ActionScript, contact me
+        # NOTE: SWF is ActionScript 3.0, which Ruffle cannot load it
+        import yaml
+
+        video_id = self._match_id(url)
+        _headers = {'User-Agent': self.IE10_USERAGENT}
+
+        niconico_thumb_watch_js = self._download_webpage(
+            'http://ext.nicovideo.jp/thumb_watch/%s?thumb_mode=html&redirect=1' % video_id, video_id, headers=_headers,
+            note='Fetching niconico old player')
+
+        # data set for Nicovideo.MiniPlayer is mostly valid as YAML, so it can be parsed as below
+        niconico_video_data = yaml.safe_load(self._search_regex(r'(?s)Nicovideo\.Video\(({.+})\);', niconico_thumb_watch_js, 'niconico info data', group=1).replace('\t', ''))
+        niconico_player_data = yaml.safe_load(self._search_regex(r'(?s)video,\s*({.+}),\s*\'', niconico_thumb_watch_js, 'niconico info data', group=1).replace('\t', ''))
+
+        tap_url = 'http://ext.nicovideo.jp/thumb_watch?as3=1&v=' + video_id + '&k=' + niconico_player_data.get('thumbPlayKey', 'undefined')
+        thumb_watch_data = self._download_webpage(tap_url, video_id, note='Tapping thumb_watch URL', headers=_headers, fatal=False)
+        twd_parsed = compat_parse_qs(thumb_watch_data)
+
+        video_url = twd_parsed['url'][0]
+        if video_url.endswith('low'):
+            video_url = video_url[0:-3]
+
+        url_parsed = compat_urllib_parse.urlparse(video_url)
+
+        video_tag = try_get(
+            compat_parse_qs(url_parsed.query),
+            (lambda x: x['v'][0],
+             lambda x: x['m'][0],),
+            compat_str)
+
+        _headers['Referer'] = compat_urllib_parse.urlunparse(url_parsed._replace(
+            path='/', params='', query='', fragment=''))
+
+        formats = [{
+            'format_id': 'flv',
+            'url': compat_urllib_parse.urlunparse(url_parsed._replace(query='m=%s' % video_tag)),
+            'ext': 'flv',
+            'http_headers': _headers,
+        }, {
+            'format_id': 'economy',
+            'url': compat_urllib_parse.urlunparse(url_parsed._replace(query='v=%slow' % video_tag)),
+            'ext': 'mp4',
+            'http_headers': _headers,
+        }, {
+            'format_id': 'high',
+            'url': compat_urllib_parse.urlunparse(url_parsed._replace(query='v=%s' % video_tag)),
+            'ext': 'mp4',
+            'http_headers': _headers,
+        }]
+
+        return {
+            'id': video_id,
+            'title': lowercase_escape(niconico_video_data.get('title')),
+            'description': lowercase_escape(niconico_video_data.get('description')),
+            'formats': formats,
+        }
diff --git a/youtube_dl/extractor/nowness.py b/youtube_dl/extractor/nowness.py
index f26dafb8f..3d52397f5 100644
--- a/youtube_dl/extractor/nowness.py
+++ b/youtube_dl/extractor/nowness.py
@@ -6,7 +6,10 @@ from .brightcove import (
     BrightcoveNewIE,
 )
 from .common import InfoExtractor
-from ..compat import compat_str
+from ..compat import (
+    compat_str,
+    compat_urllib_parse,
+)
 from ..utils import (
     ExtractorError,
     sanitized_Request,
@@ -43,10 +46,11 @@ class NownessBaseIE(InfoExtractor):
 
     def _api_request(self, url, request_path):
         display_id = self._match_id(url)
+        parsed_url = compat_urllib_parse.urlparse(url)
         request = sanitized_Request(
             'http://api.nowness.com/api/' + request_path % display_id,
             headers={
-                'X-Nowness-Language': 'zh-cn' if 'cn.nowness.com' in url else 'en-us',
+                'X-Nowness-Language': 'zh-cn' if parsed_url.hostname == 'cn.nowness.com' else 'en-us',
             })
         return display_id, self._download_json(request, display_id)
 
diff --git a/youtube_dl/extractor/openrec.py b/youtube_dl/extractor/openrec.py
new file mode 100644
index 000000000..2c742f35d
--- /dev/null
+++ b/youtube_dl/extractor/openrec.py
@@ -0,0 +1,99 @@
+# coding: utf-8
+from __future__ import unicode_literals, with_statement
+
+from .common import InfoExtractor
+from ..utils import unified_strdate
+
+
+class OpenRecBaseIE(InfoExtractor):
+    pass
+
+
+class OpenRecIE(OpenRecBaseIE):
+    IE_NAME = 'openrec'
+    _VALID_URL = r'https?://(?:www\.)?openrec\.tv/live/(?P<id>[^/]+)'
+    _TESTS = [{
+        'url': 'https://www.openrec.tv/live/2p8v31qe4zy',
+        'only_matching': True,
+    }]
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage('https://www.openrec.tv/live/%s' % video_id, video_id)
+
+        window_stores = self._parse_json(
+            self._search_regex(r'(?m)window\.stores\s*=\s*(\{.+?\});$', webpage, 'window.stores'), video_id)
+        movie_store = window_stores['moviePageStore']['movieStore']
+
+        title = movie_store['title']
+        description = movie_store['introduction']
+        thumbnail = movie_store['thumbnailUrl']
+        uploader = movie_store['channel']['name']
+        uploader_id = movie_store['channel']['id']
+        upload_date = unified_strdate(movie_store['createdAt'])
+
+        m3u8_playlists = movie_store['media']
+        formats = []
+        for (name, m3u8_url) in m3u8_playlists.items():
+            if not m3u8_url:
+                continue
+            fmt_list = self._extract_m3u8_formats(
+                m3u8_url, video_id, ext='mp4', entry_protocol='m3u8',
+                m3u8_id='hls-%s' % name, live=True)
+            formats.extend(fmt_list)
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'description': description,
+            'thumbnail': thumbnail,
+            'formats': formats,
+            'uploader': uploader,
+            'uploader_id': uploader_id,
+            'upload_date': upload_date,
+            'is_live': True,
+        }
+
+
+class OpenRecCaptureIE(OpenRecBaseIE):
+    IE_NAME = 'openrec:capture'
+    _VALID_URL = r'https?://(?:www\.)?openrec\.tv/capture/(?P<id>[^/]+)'
+    _TESTS = [{
+        'url': 'https://www.openrec.tv/capture/l9nk2x4gn14',
+        'only_matching': True,
+    }]
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage('https://www.openrec.tv/capture/%s' % video_id, video_id)
+
+        window_stores = self._parse_json(
+            self._search_regex(r'(?m)window\.stores\s*=\s*(\{.+?\});$', webpage, 'window.stores'), video_id)
+        movie_store = window_stores['moviePageStore']['movieStore']
+        capture_data = window_stores['capturePlayPageStore']['capture']
+        channel_info = window_stores['capturePlayPageStore']['movie']['channel']
+
+        title = capture_data['title']
+        thumbnail = movie_store['thumbnailUrl']
+        uploader = channel_info['name']
+        uploader_id = channel_info['id']
+        upload_date = unified_strdate(capture_data['createdAt'])
+
+        m3u8_url = capture_data['source']
+        formats = self._extract_m3u8_formats(
+            m3u8_url, video_id, ext='mp4', entry_protocol='m3u8',
+            m3u8_id='hls')
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'thumbnail': thumbnail,
+            'formats': formats,
+            'uploader': uploader,
+            'uploader_id': uploader_id,
+            'upload_date': upload_date,
+        }
diff --git a/youtube_dl/extractor/peertube.py b/youtube_dl/extractor/peertube.py
deleted file mode 100644
index d9b13adc2..000000000
--- a/youtube_dl/extractor/peertube.py
+++ /dev/null
@@ -1,628 +0,0 @@
-# coding: utf-8
-from __future__ import unicode_literals
-
-import re
-
-from .common import InfoExtractor
-from ..compat import compat_str
-from ..utils import (
-    int_or_none,
-    parse_resolution,
-    str_or_none,
-    try_get,
-    unified_timestamp,
-    url_or_none,
-    urljoin,
-)
-
-
-class PeerTubeIE(InfoExtractor):
-    _INSTANCES_RE = r'''(?:
-                            # Taken from https://instances.joinpeertube.org/instances
-                            peertube\.rainbowswingers\.net|
-                            tube\.stanisic\.nl|
-                            peer\.suiri\.us|
-                            medias\.libox\.fr|
-                            videomensoif\.ynh\.fr|
-                            peertube\.travelpandas\.eu|
-                            peertube\.rachetjay\.fr|
-                            peertube\.montecsys\.fr|
-                            tube\.eskuero\.me|
-                            peer\.tube|
-                            peertube\.umeahackerspace\.se|
-                            tube\.nx-pod\.de|
-                            video\.monsieurbidouille\.fr|
-                            tube\.openalgeria\.org|
-                            vid\.lelux\.fi|
-                            video\.anormallostpod\.ovh|
-                            tube\.crapaud-fou\.org|
-                            peertube\.stemy\.me|
-                            lostpod\.space|
-                            exode\.me|
-                            peertube\.snargol\.com|
-                            vis\.ion\.ovh|
-                            videosdulib\.re|
-                            v\.mbius\.io|
-                            videos\.judrey\.eu|
-                            peertube\.osureplayviewer\.xyz|
-                            peertube\.mathieufamily\.ovh|
-                            www\.videos-libr\.es|
-                            fightforinfo\.com|
-                            peertube\.fediverse\.ru|
-                            peertube\.oiseauroch\.fr|
-                            video\.nesven\.eu|
-                            v\.bearvideo\.win|
-                            video\.qoto\.org|
-                            justporn\.cc|
-                            video\.vny\.fr|
-                            peervideo\.club|
-                            tube\.taker\.fr|
-                            peertube\.chantierlibre\.org|
-                            tube\.ipfixe\.info|
-                            tube\.kicou\.info|
-                            tube\.dodsorf\.as|
-                            videobit\.cc|
-                            video\.yukari\.moe|
-                            videos\.elbinario\.net|
-                            hkvideo\.live|
-                            pt\.tux\.tf|
-                            www\.hkvideo\.live|
-                            FIGHTFORINFO\.com|
-                            pt\.765racing\.com|
-                            peertube\.gnumeria\.eu\.org|
-                            nordenmedia\.com|
-                            peertube\.co\.uk|
-                            tube\.darfweb\.eu|
-                            tube\.kalah-france\.org|
-                            0ch\.in|
-                            vod\.mochi\.academy|
-                            film\.node9\.org|
-                            peertube\.hatthieves\.es|
-                            video\.fitchfamily\.org|
-                            peertube\.ddns\.net|
-                            video\.ifuncle\.kr|
-                            video\.fdlibre\.eu|
-                            tube\.22decembre\.eu|
-                            peertube\.harmoniescreatives\.com|
-                            tube\.fabrigli\.fr|
-                            video\.thedwyers\.co|
-                            video\.bruitbruit\.com|
-                            peertube\.foxfam\.club|
-                            peer\.philoxweb\.be|
-                            videos\.bugs\.social|
-                            peertube\.malbert\.xyz|
-                            peertube\.bilange\.ca|
-                            libretube\.net|
-                            diytelevision\.com|
-                            peertube\.fedilab\.app|
-                            libre\.video|
-                            video\.mstddntfdn\.online|
-                            us\.tv|
-                            peertube\.sl-network\.fr|
-                            peertube\.dynlinux\.io|
-                            peertube\.david\.durieux\.family|
-                            peertube\.linuxrocks\.online|
-                            peerwatch\.xyz|
-                            v\.kretschmann\.social|
-                            tube\.otter\.sh|
-                            yt\.is\.nota\.live|
-                            tube\.dragonpsi\.xyz|
-                            peertube\.boneheadmedia\.com|
-                            videos\.funkwhale\.audio|
-                            watch\.44con\.com|
-                            peertube\.gcaillaut\.fr|
-                            peertube\.icu|
-                            pony\.tube|
-                            spacepub\.space|
-                            tube\.stbr\.io|
-                            v\.mom-gay\.faith|
-                            tube\.port0\.xyz|
-                            peertube\.simounet\.net|
-                            play\.jergefelt\.se|
-                            peertube\.zeteo\.me|
-                            tube\.danq\.me|
-                            peertube\.kerenon\.com|
-                            tube\.fab-l3\.org|
-                            tube\.calculate\.social|
-                            peertube\.mckillop\.org|
-                            tube\.netzspielplatz\.de|
-                            vod\.ksite\.de|
-                            peertube\.laas\.fr|
-                            tube\.govital\.net|
-                            peertube\.stephenson\.cc|
-                            bistule\.nohost\.me|
-                            peertube\.kajalinifi\.de|
-                            video\.ploud\.jp|
-                            video\.omniatv\.com|
-                            peertube\.ffs2play\.fr|
-                            peertube\.leboulaire\.ovh|
-                            peertube\.tronic-studio\.com|
-                            peertube\.public\.cat|
-                            peertube\.metalbanana\.net|
-                            video\.1000i100\.fr|
-                            peertube\.alter-nativ-voll\.de|
-                            tube\.pasa\.tf|
-                            tube\.worldofhauru\.xyz|
-                            pt\.kamp\.site|
-                            peertube\.teleassist\.fr|
-                            videos\.mleduc\.xyz|
-                            conf\.tube|
-                            media\.privacyinternational\.org|
-                            pt\.forty-two\.nl|
-                            video\.halle-leaks\.de|
-                            video\.grosskopfgames\.de|
-                            peertube\.schaeferit\.de|
-                            peertube\.jackbot\.fr|
-                            tube\.extinctionrebellion\.fr|
-                            peertube\.f-si\.org|
-                            video\.subak\.ovh|
-                            videos\.koweb\.fr|
-                            peertube\.zergy\.net|
-                            peertube\.roflcopter\.fr|
-                            peertube\.floss-marketing-school\.com|
-                            vloggers\.social|
-                            peertube\.iriseden\.eu|
-                            videos\.ubuntu-paris\.org|
-                            peertube\.mastodon\.host|
-                            armstube\.com|
-                            peertube\.s2s\.video|
-                            peertube\.lol|
-                            tube\.open-plug\.eu|
-                            open\.tube|
-                            peertube\.ch|
-                            peertube\.normandie-libre\.fr|
-                            peertube\.slat\.org|
-                            video\.lacaveatonton\.ovh|
-                            peertube\.uno|
-                            peertube\.servebeer\.com|
-                            peertube\.fedi\.quebec|
-                            tube\.h3z\.jp|
-                            tube\.plus200\.com|
-                            peertube\.eric\.ovh|
-                            tube\.metadocs\.cc|
-                            tube\.unmondemeilleur\.eu|
-                            gouttedeau\.space|
-                            video\.antirep\.net|
-                            nrop\.cant\.at|
-                            tube\.ksl-bmx\.de|
-                            tube\.plaf\.fr|
-                            tube\.tchncs\.de|
-                            video\.devinberg\.com|
-                            hitchtube\.fr|
-                            peertube\.kosebamse\.com|
-                            yunopeertube\.myddns\.me|
-                            peertube\.varney\.fr|
-                            peertube\.anon-kenkai\.com|
-                            tube\.maiti\.info|
-                            tubee\.fr|
-                            videos\.dinofly\.com|
-                            toobnix\.org|
-                            videotape\.me|
-                            voca\.tube|
-                            video\.heromuster\.com|
-                            video\.lemediatv\.fr|
-                            video\.up\.edu\.ph|
-                            balafon\.video|
-                            video\.ivel\.fr|
-                            thickrips\.cloud|
-                            pt\.laurentkruger\.fr|
-                            video\.monarch-pass\.net|
-                            peertube\.artica\.center|
-                            video\.alternanet\.fr|
-                            indymotion\.fr|
-                            fanvid\.stopthatimp\.net|
-                            video\.farci\.org|
-                            v\.lesterpig\.com|
-                            video\.okaris\.de|
-                            tube\.pawelko\.net|
-                            peertube\.mablr\.org|
-                            tube\.fede\.re|
-                            pytu\.be|
-                            evertron\.tv|
-                            devtube\.dev-wiki\.de|
-                            raptube\.antipub\.org|
-                            video\.selea\.se|
-                            peertube\.mygaia\.org|
-                            video\.oh14\.de|
-                            peertube\.livingutopia\.org|
-                            peertube\.the-penguin\.de|
-                            tube\.thechangebook\.org|
-                            tube\.anjara\.eu|
-                            pt\.pube\.tk|
-                            video\.samedi\.pm|
-                            mplayer\.demouliere\.eu|
-                            widemus\.de|
-                            peertube\.me|
-                            peertube\.zapashcanon\.fr|
-                            video\.latavernedejohnjohn\.fr|
-                            peertube\.pcservice46\.fr|
-                            peertube\.mazzonetto\.eu|
-                            video\.irem\.univ-paris-diderot\.fr|
-                            video\.livecchi\.cloud|
-                            alttube\.fr|
-                            video\.coop\.tools|
-                            video\.cabane-libre\.org|
-                            peertube\.openstreetmap\.fr|
-                            videos\.alolise\.org|
-                            irrsinn\.video|
-                            video\.antopie\.org|
-                            scitech\.video|
-                            tube2\.nemsia\.org|
-                            video\.amic37\.fr|
-                            peertube\.freeforge\.eu|
-                            video\.arbitrarion\.com|
-                            video\.datsemultimedia\.com|
-                            stoptrackingus\.tv|
-                            peertube\.ricostrongxxx\.com|
-                            docker\.videos\.lecygnenoir\.info|
-                            peertube\.togart\.de|
-                            tube\.postblue\.info|
-                            videos\.domainepublic\.net|
-                            peertube\.cyber-tribal\.com|
-                            video\.gresille\.org|
-                            peertube\.dsmouse\.net|
-                            cinema\.yunohost\.support|
-                            tube\.theocevaer\.fr|
-                            repro\.video|
-                            tube\.4aem\.com|
-                            quaziinc\.com|
-                            peertube\.metawurst\.space|
-                            videos\.wakapo\.com|
-                            video\.ploud\.fr|
-                            video\.freeradical\.zone|
-                            tube\.valinor\.fr|
-                            refuznik\.video|
-                            pt\.kircheneuenburg\.de|
-                            peertube\.asrun\.eu|
-                            peertube\.lagob\.fr|
-                            videos\.side-ways\.net|
-                            91video\.online|
-                            video\.valme\.io|
-                            video\.taboulisme\.com|
-                            videos-libr\.es|
-                            tv\.mooh\.fr|
-                            nuage\.acostey\.fr|
-                            video\.monsieur-a\.fr|
-                            peertube\.librelois\.fr|
-                            videos\.pair2jeux\.tube|
-                            videos\.pueseso\.club|
-                            peer\.mathdacloud\.ovh|
-                            media\.assassinate-you\.net|
-                            vidcommons\.org|
-                            ptube\.rousset\.nom\.fr|
-                            tube\.cyano\.at|
-                            videos\.squat\.net|
-                            video\.iphodase\.fr|
-                            peertube\.makotoworkshop\.org|
-                            peertube\.serveur\.slv-valbonne\.fr|
-                            vault\.mle\.party|
-                            hostyour\.tv|
-                            videos\.hack2g2\.fr|
-                            libre\.tube|
-                            pire\.artisanlogiciel\.net|
-                            videos\.numerique-en-commun\.fr|
-                            video\.netsyms\.com|
-                            video\.die-partei\.social|
-                            video\.writeas\.org|
-                            peertube\.swarm\.solvingmaz\.es|
-                            tube\.pericoloso\.ovh|
-                            watching\.cypherpunk\.observer|
-                            videos\.adhocmusic\.com|
-                            tube\.rfc1149\.net|
-                            peertube\.librelabucm\.org|
-                            videos\.numericoop\.fr|
-                            peertube\.koehn\.com|
-                            peertube\.anarchmusicall\.net|
-                            tube\.kampftoast\.de|
-                            vid\.y-y\.li|
-                            peertube\.xtenz\.xyz|
-                            diode\.zone|
-                            tube\.egf\.mn|
-                            peertube\.nomagic\.uk|
-                            visionon\.tv|
-                            videos\.koumoul\.com|
-                            video\.rastapuls\.com|
-                            video\.mantlepro\.com|
-                            video\.deadsuperhero\.com|
-                            peertube\.musicstudio\.pro|
-                            peertube\.we-keys\.fr|
-                            artitube\.artifaille\.fr|
-                            peertube\.ethernia\.net|
-                            tube\.midov\.pl|
-                            peertube\.fr|
-                            watch\.snoot\.tube|
-                            peertube\.donnadieu\.fr|
-                            argos\.aquilenet\.fr|
-                            tube\.nemsia\.org|
-                            tube\.bruniau\.net|
-                            videos\.darckoune\.moe|
-                            tube\.traydent\.info|
-                            dev\.videos\.lecygnenoir\.info|
-                            peertube\.nayya\.org|
-                            peertube\.live|
-                            peertube\.mofgao\.space|
-                            video\.lequerrec\.eu|
-                            peertube\.amicale\.net|
-                            aperi\.tube|
-                            tube\.ac-lyon\.fr|
-                            video\.lw1\.at|
-                            www\.yiny\.org|
-                            videos\.pofilo\.fr|
-                            tube\.lou\.lt|
-                            choob\.h\.etbus\.ch|
-                            tube\.hoga\.fr|
-                            peertube\.heberge\.fr|
-                            video\.obermui\.de|
-                            videos\.cloudfrancois\.fr|
-                            betamax\.video|
-                            video\.typica\.us|
-                            tube\.piweb\.be|
-                            video\.blender\.org|
-                            peertube\.cat|
-                            tube\.kdy\.ch|
-                            pe\.ertu\.be|
-                            peertube\.social|
-                            videos\.lescommuns\.org|
-                            tv\.datamol\.org|
-                            videonaute\.fr|
-                            dialup\.express|
-                            peertube\.nogafa\.org|
-                            megatube\.lilomoino\.fr|
-                            peertube\.tamanoir\.foucry\.net|
-                            peertube\.devosi\.org|
-                            peertube\.1312\.media|
-                            tube\.bootlicker\.party|
-                            skeptikon\.fr|
-                            video\.blueline\.mg|
-                            tube\.homecomputing\.fr|
-                            tube\.ouahpiti\.info|
-                            video\.tedomum\.net|
-                            video\.g3l\.org|
-                            fontube\.fr|
-                            peertube\.gaialabs\.ch|
-                            tube\.kher\.nl|
-                            peertube\.qtg\.fr|
-                            video\.migennes\.net|
-                            tube\.p2p\.legal|
-                            troll\.tv|
-                            videos\.iut-orsay\.fr|
-                            peertube\.solidev\.net|
-                            videos\.cemea\.org|
-                            video\.passageenseine\.fr|
-                            videos\.festivalparminous\.org|
-                            peertube\.touhoppai\.moe|
-                            sikke\.fi|
-                            peer\.hostux\.social|
-                            share\.tube|
-                            peertube\.walkingmountains\.fr|
-                            videos\.benpro\.fr|
-                            peertube\.parleur\.net|
-                            peertube\.heraut\.eu|
-                            tube\.aquilenet\.fr|
-                            peertube\.gegeweb\.eu|
-                            framatube\.org|
-                            thinkerview\.video|
-                            tube\.conferences-gesticulees\.net|
-                            peertube\.datagueule\.tv|
-                            video\.lqdn\.fr|
-                            tube\.mochi\.academy|
-                            media\.zat\.im|
-                            video\.colibris-outilslibres\.org|
-                            tube\.svnet\.fr|
-                            peertube\.video|
-                            peertube3\.cpy\.re|
-                            peertube2\.cpy\.re|
-                            videos\.tcit\.fr|
-                            peertube\.cpy\.re|
-                            canard\.tube
-                        )'''
-    _UUID_RE = r'[\da-fA-F]{8}-[\da-fA-F]{4}-[\da-fA-F]{4}-[\da-fA-F]{4}-[\da-fA-F]{12}'
-    _API_BASE = 'https://%s/api/v1/videos/%s/%s'
-    _VALID_URL = r'''(?x)
-                    (?:
-                        peertube:(?P<host>[^:]+):|
-                        https?://(?P<host_2>%s)/(?:videos/(?:watch|embed)|api/v\d/videos)/
-                    )
-                    (?P<id>%s)
-                    ''' % (_INSTANCES_RE, _UUID_RE)
-    _TESTS = [{
-        'url': 'https://framatube.org/videos/watch/9c9de5e8-0a1e-484a-b099-e80766180a6d',
-        'md5': '9bed8c0137913e17b86334e5885aacff',
-        'info_dict': {
-            'id': '9c9de5e8-0a1e-484a-b099-e80766180a6d',
-            'ext': 'mp4',
-            'title': 'What is PeerTube?',
-            'description': 'md5:3fefb8dde2b189186ce0719fda6f7b10',
-            'thumbnail': r're:https?://.*\.(?:jpg|png)',
-            'timestamp': 1538391166,
-            'upload_date': '20181001',
-            'uploader': 'Framasoft',
-            'uploader_id': '3',
-            'uploader_url': 'https://framatube.org/accounts/framasoft',
-            'channel': 'Les vid√©os de Framasoft',
-            'channel_id': '2',
-            'channel_url': 'https://framatube.org/video-channels/bf54d359-cfad-4935-9d45-9d6be93f63e8',
-            'language': 'en',
-            'license': 'Attribution - Share Alike',
-            'duration': 113,
-            'view_count': int,
-            'like_count': int,
-            'dislike_count': int,
-            'tags': ['framasoft', 'peertube'],
-            'categories': ['Science & Technology'],
-        }
-    }, {
-        # Issue #26002
-        'url': 'peertube:spacepub.space:d8943b2d-8280-497b-85ec-bc282ec2afdc',
-        'info_dict': {
-            'id': 'd8943b2d-8280-497b-85ec-bc282ec2afdc',
-            'ext': 'mp4',
-            'title': 'Dot matrix printer shell demo',
-            'uploader_id': '3',
-            'timestamp': 1587401293,
-            'upload_date': '20200420',
-            'uploader': 'Drew DeVault',
-        }
-    }, {
-        'url': 'https://peertube.tamanoir.foucry.net/videos/watch/0b04f13d-1e18-4f1d-814e-4979aa7c9c44',
-        'only_matching': True,
-    }, {
-        # nsfw
-        'url': 'https://tube.22decembre.eu/videos/watch/9bb88cd3-9959-46d9-9ab9-33d2bb704c39',
-        'only_matching': True,
-    }, {
-        'url': 'https://tube.22decembre.eu/videos/embed/fed67262-6edb-4d1c-833b-daa9085c71d7',
-        'only_matching': True,
-    }, {
-        'url': 'https://tube.openalgeria.org/api/v1/videos/c1875674-97d0-4c94-a058-3f7e64c962e8',
-        'only_matching': True,
-    }, {
-        'url': 'peertube:video.blender.org:b37a5b9f-e6b5-415c-b700-04a5cd6ec205',
-        'only_matching': True,
-    }]
-
-    @staticmethod
-    def _extract_peertube_url(webpage, source_url):
-        mobj = re.match(
-            r'https?://(?P<host>[^/]+)/videos/(?:watch|embed)/(?P<id>%s)'
-            % PeerTubeIE._UUID_RE, source_url)
-        if mobj and any(p in webpage for p in (
-                '<title>PeerTube<',
-                'There will be other non JS-based clients to access PeerTube',
-                '>We are sorry but it seems that PeerTube is not compatible with your web browser.<')):
-            return 'peertube:%s:%s' % mobj.group('host', 'id')
-
-    @staticmethod
-    def _extract_urls(webpage, source_url):
-        entries = re.findall(
-            r'''(?x)<iframe[^>]+\bsrc=["\'](?P<url>(?:https?:)?//%s/videos/embed/%s)'''
-            % (PeerTubeIE._INSTANCES_RE, PeerTubeIE._UUID_RE), webpage)
-        if not entries:
-            peertube_url = PeerTubeIE._extract_peertube_url(webpage, source_url)
-            if peertube_url:
-                entries = [peertube_url]
-        return entries
-
-    def _call_api(self, host, video_id, path, note=None, errnote=None, fatal=True):
-        return self._download_json(
-            self._API_BASE % (host, video_id, path), video_id,
-            note=note, errnote=errnote, fatal=fatal)
-
-    def _get_subtitles(self, host, video_id):
-        captions = self._call_api(
-            host, video_id, 'captions', note='Downloading captions JSON',
-            fatal=False)
-        if not isinstance(captions, dict):
-            return
-        data = captions.get('data')
-        if not isinstance(data, list):
-            return
-        subtitles = {}
-        for e in data:
-            language_id = try_get(e, lambda x: x['language']['id'], compat_str)
-            caption_url = urljoin('https://%s' % host, e.get('captionPath'))
-            if not caption_url:
-                continue
-            subtitles.setdefault(language_id or 'en', []).append({
-                'url': caption_url,
-            })
-        return subtitles
-
-    def _real_extract(self, url):
-        mobj = re.match(self._VALID_URL, url)
-        host = mobj.group('host') or mobj.group('host_2')
-        video_id = mobj.group('id')
-
-        video = self._call_api(
-            host, video_id, '', note='Downloading video JSON')
-
-        title = video['name']
-
-        formats = []
-        files = video.get('files') or []
-        for playlist in (video.get('streamingPlaylists') or []):
-            if not isinstance(playlist, dict):
-                continue
-            playlist_files = playlist.get('files')
-            if not (playlist_files and isinstance(playlist_files, list)):
-                continue
-            files.extend(playlist_files)
-        for file_ in files:
-            if not isinstance(file_, dict):
-                continue
-            file_url = url_or_none(file_.get('fileUrl'))
-            if not file_url:
-                continue
-            file_size = int_or_none(file_.get('size'))
-            format_id = try_get(
-                file_, lambda x: x['resolution']['label'], compat_str)
-            f = parse_resolution(format_id)
-            f.update({
-                'url': file_url,
-                'format_id': format_id,
-                'filesize': file_size,
-            })
-            if format_id == '0p':
-                f['vcodec'] = 'none'
-            else:
-                f['fps'] = int_or_none(file_.get('fps'))
-            formats.append(f)
-        self._sort_formats(formats)
-
-        full_description = self._call_api(
-            host, video_id, 'description', note='Downloading description JSON',
-            fatal=False)
-
-        description = None
-        if isinstance(full_description, dict):
-            description = str_or_none(full_description.get('description'))
-        if not description:
-            description = video.get('description')
-
-        subtitles = self.extract_subtitles(host, video_id)
-
-        def data(section, field, type_):
-            return try_get(video, lambda x: x[section][field], type_)
-
-        def account_data(field, type_):
-            return data('account', field, type_)
-
-        def channel_data(field, type_):
-            return data('channel', field, type_)
-
-        category = data('category', 'label', compat_str)
-        categories = [category] if category else None
-
-        nsfw = video.get('nsfw')
-        if nsfw is bool:
-            age_limit = 18 if nsfw else 0
-        else:
-            age_limit = None
-
-        webpage_url = 'https://%s/videos/watch/%s' % (host, video_id)
-
-        return {
-            'id': video_id,
-            'title': title,
-            'description': description,
-            'thumbnail': urljoin(webpage_url, video.get('thumbnailPath')),
-            'timestamp': unified_timestamp(video.get('publishedAt')),
-            'uploader': account_data('displayName', compat_str),
-            'uploader_id': str_or_none(account_data('id', int)),
-            'uploader_url': url_or_none(account_data('url', compat_str)),
-            'channel': channel_data('displayName', compat_str),
-            'channel_id': str_or_none(channel_data('id', int)),
-            'channel_url': url_or_none(channel_data('url', compat_str)),
-            'language': data('language', 'id', compat_str),
-            'license': data('licence', 'label', compat_str),
-            'duration': int_or_none(video.get('duration')),
-            'view_count': int_or_none(video.get('views')),
-            'like_count': int_or_none(video.get('likes')),
-            'dislike_count': int_or_none(video.get('dislikes')),
-            'age_limit': age_limit,
-            'tags': try_get(video, lambda x: x['tags'], list),
-            'categories': categories,
-            'formats': formats,
-            'subtitles': subtitles,
-            'webpage_url': webpage_url,
-        }
diff --git a/youtube_dl/extractor/peertube/__init__.py b/youtube_dl/extractor/peertube/__init__.py
new file mode 100644
index 000000000..91e82fbb8
--- /dev/null
+++ b/youtube_dl/extractor/peertube/__init__.py
@@ -0,0 +1,5 @@
+from __future__ import unicode_literals
+
+from .peertube import PeerTubeIE
+
+__all__ = ['PeerTubeIE']
diff --git a/youtube_dl/extractor/peertube/instances.py b/youtube_dl/extractor/peertube/instances.py
new file mode 100644
index 000000000..d59e72e0b
--- /dev/null
+++ b/youtube_dl/extractor/peertube/instances.py
@@ -0,0 +1,1018 @@
+# coding: utf-8
+# AUTOMATICALLY GENERATED FILE. DO NOT EDIT.
+# Generated by ./devscripts/make_peertube_instance_list.py
+from __future__ import unicode_literals
+
+instances = {
+    # list of instances here
+    "40two.tube",
+    "a.metube.ch",
+    "advtv.ml",
+    "algorithmic.tv",
+    "alimulama.com",
+    "alttube.fr",
+    "aperi.tube",
+    "archive.vidicon.org",
+    "ark.germanium.top",
+    "armstube.com",
+    "artefac-paris.tv",
+    "artitube.artifaille.fr",
+    "based.directory",
+    "battlepenguin.video",
+    "beertube.epgn.ch",
+    "befree.nohost.me",
+    "bestgore.fun",
+    "betamax.video",
+    "bitcointv.com",
+    "bittube.video",
+    "blockvideo.live",
+    "canard.tube",
+    "cattube.org",
+    "cinema.yunohost.support",
+    "clap.nerv-project.eu",
+    "climatejustice.video",
+    "comic.bot.nu",
+    "conf.tube",
+    "conspiracydistillery.com",
+    "courier.no",
+    "daschauher.aksel.rocks",
+    "dev.quadplay.tv",
+    "devtube.dev-wiki.de",
+    "dialup.express",
+    "digitalcourage.video",
+    "diode.zone",
+    "dislive.me",
+    "docker.videos.lecygnenoir.info",
+    "dollarvigilante.tv",
+    "dreiecksnebel.alex-detsch.de",
+    "eduvid.org",
+    "evangelisch.video",
+    "evertron.tv",
+    "exo.tube",
+    "exode.me",
+    "fair.tube",
+    "famichiki.tube",
+    "fanvid.stopthatimp.net",
+    "fediverse.tv",
+    "fightforinfo.com",
+    "film.k-prod.fr",
+    "film.node9.org",
+    "flim.txmn.tk",
+    "fontube.fr",
+    "fotogramas.politicaconciencia.org",
+    "framatube.org",
+    "ftsi.ru",
+    "gary.vger.cloud",
+    "gegenstimme.tv",
+    "germanium.inter21.net",
+    "gitmo.tv",
+    "gorf.tube",
+    "graeber.video",
+    "greatview.video",
+    "greenlife.li",
+    "grypstube.uni-greifswald.de",
+    "guggenberger.website",
+    "gw2.ffoh.de",
+    "hacc.media",
+    "haytv.blueline.mg",
+    "highvoltage.tv",
+    "hiphop.horse",
+    "hitchtube.fr",
+    "hpstube.fr",
+    "htp.live",
+    "ias-peertube.iosb.fraunhofer.de",
+    "ignazbearth.ch",
+    "indymotion.fr",
+    "irrsinn.video",
+    "jafningi.medienverwaltung.org",
+    "jrgnsn.video",
+    "juggling.digital",
+    "jupiter.tube",
+    "justtelly.com",
+    "kenfm.inter21.net",
+    "kenfm.quadplay.tv",
+    "kino.lewactwo.pl",
+    "kino.schuerz.at",
+    "kinowolnosc.pl",
+    "kirche.peertube-host.de",
+    "kodcast.com",
+    "kolektiva.media",
+    "kumi.tube",
+    "kuninglangsat.com",
+    "lastbreach.tv",
+    "lepetitmayennais.fr.nf",
+    "lexx.impa.me",
+    "lfbu.nl",
+    "libre.tube",
+    "libre.video",
+    "libremedia.video",
+    "live.69.mu",
+    "live.libratoi.org",
+    "live.nanao.moe",
+    "livegram.net",
+    "lolitube.freedomchan.moe",
+    "lostpod.space",
+    "lucarne.balsamine.be",
+    "maindreieck-tv.de",
+    "mani.tube",
+    "manicphase.me",
+    "media.assassinate-you.net",
+    "media.gzevd.de",
+    "media.inno3.cricket",
+    "media.kaitaia.life",
+    "media.krashboyz.org",
+    "media.privacyinternational.org",
+    "media.skewed.de",
+    "media.undeadnetwork.de",
+    "media.zat.im",
+    "medias.pingbase.net",
+    "megatube.lilomoino.fr",
+    "melsungen.peertube-host.de",
+    "merci-la-police.fr",
+    "meta-tube.de",
+    "midi-les-animes.moe",
+    "mirametube.fr",
+    "mojotube.net",
+    "monplaisirtube.ddns.net",
+    "motibus.hexe.net",
+    "mountaintown.video",
+    "mplayer.demouliere.eu",
+    "music.noho.st",
+    "my.bunny.cafe",
+    "myfreetube.de",
+    "mymusicportal.musicallstudio.online",
+    "mytube.kn-cloud.de",
+    "mytube.madzel.de",
+    "nadajemy.com",
+    "nanawel-peertube.dyndns.org",
+    "noagendatube.com",
+    "notretube.asselma.eu",
+    "nuage.acostey.fr",
+    "open.tube",
+    "p.eertu.be",
+    "p.lu",
+    "p0.pm",
+    "p2ptv.ru",
+    "pace.rip",
+    "peer.azurs.fr",
+    "peer.hostux.social",
+    "peer.philoxweb.be",
+    "peer.tube",
+    "peerkids.com",
+    "peertube.020.pl",
+    "peertube.1312.media",
+    "peertube.1984.cz",
+    "peertube.acab.io",
+    "peertube.alpharius.io",
+    "peertube.am-networks.fr",
+    "peertube.amicale.net",
+    "peertube.anduin.net",
+    "peertube.anon-kenkai.com",
+    "peertube.anzui.dev",
+    "peertube.artica.center",
+    "peertube.asrun.eu",
+    "peertube.atilla.org",
+    "peertube.aventer.biz",
+    "peertube.b38.rural-it.org",
+    "peertube.be",
+    "peertube.bgzashtita.es",
+    "peertube.bilange.ca",
+    "peertube.bittube.tv",
+    "peertube.boba.best",
+    "peertube.br0.fr",
+    "peertube.bubbletea.dev",
+    "peertube.bubuit.net",
+    "peertube.cats-home.net",
+    "peertube.cayra.info",
+    "peertube.cf",
+    "peertube.ch",
+    "peertube.chantierlibre.org",
+    "peertube.chemnitz.freifunk.net",
+    "peertube.chevro.fr",
+    "peertube.chtisurel.net",
+    "peertube.cipherbliss.com",
+    "peertube.cloud.sans.pub",
+    "peertube.co.uk",
+    "peertube.cpge-brizeux.fr",
+    "peertube.cpy.re",
+    "peertube.ctseuro.com",
+    "peertube.cyber-privacy.online",
+    "peertube.cythin.com",
+    "peertube.datagueule.tv",
+    "peertube.davidpeach.co.uk",
+    "peertube.davigge.com",
+    "peertube.dc.pini.fr",
+    "peertube.debian.social",
+    "peertube.demonix.fr",
+    "peertube.designersethiques.org",
+    "peertube.desmu.fr",
+    "peertube.devloprog.org",
+    "peertube.devol.it",
+    "peertube.donnadieu.fr",
+    "peertube.dsmouse.net",
+    "peertube.dynlinux.io",
+    "peertube.education-forum.com",
+    "peertube.esadhar.net",
+    "peertube.ethibox.fr",
+    "peertube.eu.org",
+    "peertube.european-pirates.eu",
+    "peertube.euskarabildua.eus",
+    "peertube.f-si.org",
+    "peertube.fedi.quebec",
+    "peertube.ffs2play.fr",
+    "peertube.fidonet.io",
+    "peertube.floss-marketing-school.com",
+    "peertube.fomin.site",
+    "peertube.forsud.be",
+    "peertube.fr",
+    "peertube.francoispelletier.org",
+    "peertube.freeforge.eu",
+    "peertube.freenet.ru",
+    "peertube.fshm.in",
+    "peertube.functional.cafe",
+    "peertube.gaialabs.ch",
+    "peertube.gardeludwig.fr",
+    "peertube.gargantia.fr",
+    "peertube.gcfamily.fr",
+    "peertube.gegeweb.eu",
+    "peertube.genma.fr",
+    "peertube.get-racing.de",
+    "peertube.gidikroon.eu",
+    "peertube.gruezishop.ch",
+    "peertube.habets.house",
+    "peertube.hackerfraternity.org",
+    "peertube.hairylarry.rocks",
+    "peertube.heraut.eu",
+    "peertube.hhhammer.de",
+    "peertube.hosnet.fr",
+    "peertube.ichigo.everydayimshuflin.com",
+    "peertube.ignifi.me",
+    "peertube.inapurna.org",
+    "peertube.informaction.info",
+    "peertube.interhop.org",
+    "peertube.iriseden.eu",
+    "peertube.iselfhost.com",
+    "peertube.it",
+    "peertube.jackbot.fr",
+    "peertube.joffreyverd.fr",
+    "peertube.kathryl.fr",
+    "peertube.ketchup.noho.st",
+    "peertube.kleph.eu",
+    "peertube.kodcast.com",
+    "peertube.koehn.com",
+    "peertube.kosebamse.com",
+    "peertube.kx.studio",
+    "peertube.la-famille-muller.fr",
+    "peertube.laas.fr",
+    "peertube.lagob.fr",
+    "peertube.lagvoid.com",
+    "peertube.lavallee.tech",
+    "peertube.le5emeaxe.fr",
+    "peertube.lestutosdeprocessus.fr",
+    "peertube.lhc.net.br",
+    "peertube.librelabucm.org",
+    "peertube.librenet.co.za",
+    "peertube.linuxrocks.online",
+    "peertube.livingutopia.org",
+    "peertube.logilab.fr",
+    "peertube.louisematic.site",
+    "peertube.luckow.org",
+    "peertube.luga.at",
+    "peertube.lyceeconnecte.fr",
+    "peertube.makotoworkshop.org",
+    "peertube.manalejandro.com",
+    "peertube.marud.fr",
+    "peertube.maxweiss.io",
+    "peertube.me",
+    "peertube.monadnock.ca",
+    "peertube.monlycee.net",
+    "peertube.musicstudio.pro",
+    "peertube.mxinfo.fr",
+    "peertube.mygaia.org",
+    "peertube.nayya.org",
+    "peertube.nebelcloud.de",
+    "peertube.netzbegruenung.de",
+    "peertube.newsocial.tech",
+    "peertube.nicolastissot.fr",
+    "peertube.nixi.icu",
+    "peertube.nocturlab.fr",
+    "peertube.nodja.com",
+    "peertube.nogafa.org",
+    "peertube.nomagic.uk",
+    "peertube.normandie-libre.fr",
+    "peertube.noussommes.org",
+    "peertube.nz",
+    "peertube.offerman.com",
+    "peertube.okko.io",
+    "peertube.opencloud.lu",
+    "peertube.openstreetmap.fr",
+    "peertube.opentoken.de",
+    "peertube.paladines.ec",
+    "peertube.paladyn.org",
+    "peertube.parleur.net",
+    "peertube.patapouf.xyz",
+    "peertube.pcservice46.fr",
+    "peertube.peacememori.es",
+    "peertube.pi2.dev",
+    "peertube.pl",
+    "peertube.pontostroy.gq",
+    "peertube.portaesgnos.org",
+    "peertube.public.cat",
+    "peertube.qtg.fr",
+    "peertube.r2.enst.fr",
+    "peertube.r5c3.fr",
+    "peertube.rachetjay.fr",
+    "peertube.rainbowswingers.net",
+    "peertube.red",
+    "peertube.rijpersweg.nl",
+    "peertube.robonomics.network",
+    "peertube.roflcopter.fr",
+    "peertube.runfox.tk",
+    "peertube.s2s.video",
+    "peertube.satoshishop.de",
+    "peertube.scic-tetris.org",
+    "peertube.se",
+    "peertube.securitymadein.lu",
+    "peertube.semipvt.com",
+    "peertube.semweb.pro",
+    "peertube.servebeer.com",
+    "peertube.serveur.slv-valbonne.fr",
+    "peertube.simounet.net",
+    "peertube.skinet.org",
+    "peertube.sl-network.fr",
+    "peertube.slat.org",
+    "peertube.snargol.com",
+    "peertube.social",
+    "peertube.social.my-wan.de",
+    "peertube.solidev.net",
+    "peertube.stefofficiel.me",
+    "peertube.stemy.me",
+    "peertube.stephenson.cc",
+    "peertube.stream",
+    "peertube.su",
+    "peertube.sunknudsen.com",
+    "peertube.swarm.solvingmaz.es",
+    "peertube.swrs.net",
+    "peertube.takeko.cyou",
+    "peertube.tangentfox.com",
+    "peertube.taxinachtegel.de",
+    "peertube.techandmemes.dynip.online",
+    "peertube.terranout.mine.nu",
+    "peertube.the-penguin.de",
+    "peertube.thebigowo.xyz",
+    "peertube.ti-fr.com",
+    "peertube.tiennot.net",
+    "peertube.togart.de",
+    "peertube.toldi.eu",
+    "peertube.touhoppai.moe",
+    "peertube.travelpandas.eu",
+    "peertube.travnewmatic.com",
+    "peertube.trippelm.tv",
+    "peertube.tspu.edu.ru",
+    "peertube.tux.ovh",
+    "peertube.tv",
+    "peertube.tweb.tv",
+    "peertube.ukpats.io",
+    "peertube.umeahackerspace.se",
+    "peertube.underworld.fr",
+    "peertube.uno",
+    "peertube.ventresmous.fr",
+    "peertube.viviers-fibre.net",
+    "peertube.wapnitsky.com",
+    "peertube.we-keys.fr",
+    "peertube.westring.digital",
+    "peertube.wivodaim.net",
+    "peertube.xhrpb.com",
+    "peertube.xwiki.com",
+    "peertube.zapashcanon.fr",
+    "peertube.zergy.net",
+    "peertube.zoz-serv.duckdns.org",
+    "peertube.zoz-serv.org",
+    "peertube1.zeteo.me",
+    "peertube2.cpy.re",
+    "peertube3.cpy.re",
+    "peervideo.club",
+    "peervideo.ru",
+    "perceptiontravel.tv",
+    "periscope.numenaute.org",
+    "perron-tube.de",
+    "pertur.be",
+    "petitlutinartube.fr",
+    "petube.zone",
+    "pierre.tube",
+    "piraten.space",
+    "pire.artisanlogiciel.net",
+    "play.mepiu.it",
+    "player.ojamajo.moe",
+    "plextube.nl",
+    "polskijutub.mkljczk.pl",
+    "pony.tube",
+    "ppstube.portageps.org",
+    "pt.apathy.top",
+    "pt.diaspodon.fr",
+    "pt.earne.link",
+    "pt.gordons.gen.nz",
+    "pt.kircheneuenburg.de",
+    "pt.laurentkruger.fr",
+    "pt.neko.bar",
+    "pt.pube.tk",
+    "pt.vulpes.one",
+    "ptmir1.inter21.net",
+    "ptmir2.inter21.net",
+    "ptmir3.inter21.net",
+    "ptmir4.inter21.net",
+    "ptmir5.inter21.net",
+    "ptube.horsentiers.fr",
+    "ptube.rousset.nom.fr",
+    "ptube.xmanifesto.club",
+    "quantube.win",
+    "queermotion.org",
+    "raptube.antipub.org",
+    "re-wizja.re-medium.com",
+    "rebeltube.de",
+    "refuznik.video",
+    "regarder.sans.pub",
+    "renewtube.org",
+    "replay.jres.org",
+    "repro.video",
+    "runtube.re",
+    "ruraletv.ovh",
+    "rxb.pl",
+    "s2.veezee.tube",
+    "scifitube.myddns.me",
+    "scitech.video",
+    "sdmtube.fr",
+    "serv1.wiki-tube.de",
+    "serv3.wiki-tube.de",
+    "share.tube",
+    "shtv.live",
+    "siber62.com",
+    "sickstream.net",
+    "sk.peertube.roederstein.de",
+    "skeptikon.fr",
+    "sleepy.tube",
+    "spacepub.space",
+    "spectra.video",
+    "stoptrackingus.tv",
+    "stream.dragon-fly.club",
+    "stream.elven.pw",
+    "stream.okin.cloud",
+    "streamsource.video",
+    "studios.racer159.com",
+    "tcode.kenfm.de",
+    "teloche.l-internet.fr",
+    "testtube.florimond.eu",
+    "tgi.hosted.spacebear.ee",
+    "thaitube.in.th",
+    "the.jokertv.eu",
+    "theater.ethernia.net",
+    "thecool.tube",
+    "thinkerview.video",
+    "tilvids.com",
+    "toobnix.org",
+    "totse.tube",
+    "trailblazer.tube",
+    "troll.tv",
+    "truetube.media",
+    "tuba.lhub.pl",
+    "tube-aix-marseille.beta.education.fr",
+    "tube-amiens.beta.education.fr",
+    "tube-besancon.beta.education.fr",
+    "tube-bordeaux.beta.education.fr",
+    "tube-clermont-ferrand.beta.education.fr",
+    "tube-corse.beta.education.fr",
+    "tube-creteil.beta.education.fr",
+    "tube-dijon.beta.education.fr",
+    "tube-education.beta.education.fr",
+    "tube-grenoble.beta.education.fr",
+    "tube-lille.beta.education.fr",
+    "tube-limoges.beta.education.fr",
+    "tube-montpellier.beta.education.fr",
+    "tube-nachrichten.com",
+    "tube-nancy.beta.education.fr",
+    "tube-nantes.beta.education.fr",
+    "tube-nice.beta.education.fr",
+    "tube-normandie.beta.education.fr",
+    "tube-orleans-tours.beta.education.fr",
+    "tube-outremer.beta.education.fr",
+    "tube-paris.beta.education.fr",
+    "tube-poitiers.beta.education.fr",
+    "tube-reims.beta.education.fr",
+    "tube-strasbourg.beta.education.fr",
+    "tube-toulouse.beta.education.fr",
+    "tube-versailles.beta.education.fr",
+    "tube.1001solutions.net",
+    "tube.22decembre.eu",
+    "tube.4aem.com",
+    "tube.4f9e1738.ignorelist.com",
+    "tube.9minuti.it",
+    "tube.abolivier.bzh",
+    "tube.ac-amiens.fr",
+    "tube.ac-lyon.fr",
+    "tube.aerztefueraufklaerung.de",
+    "tube.afix.space",
+    "tube.alexx.ml",
+    "tube.amic37.fr",
+    "tube.angst-frei.ch",
+    "tube.anjara.eu",
+    "tube.anufrij.de",
+    "tube.aquilenet.fr",
+    "tube.arkhalabs.io",
+    "tube.as211696.net",
+    "tube.awkward.company",
+    "tube.azbyka.ru",
+    "tube.baab.de",
+    "tube.bachaner.fr",
+    "tube.benzo.online",
+    "tube.bmesh.org",
+    "tube.bruniau.net",
+    "tube.calculate.social",
+    "tube.chaoszone.tv",
+    "tube.chatelet.ovh",
+    "tube.chibatsu.net",
+    "tube.cloud-libre.eu",
+    "tube.cms.garden",
+    "tube.conferences-gesticulees.net",
+    "tube.crapaud-fou.org",
+    "tube.cryptography.dog",
+    "tube.cyano.at",
+    "tube.danq.me",
+    "tube.darknight-coffee.org",
+    "tube.demo.activitypub.eu",
+    "tube.dev.lhub.pl",
+    "tube.dirkomatik.de",
+    "tube.distrilab.fr",
+    "tube.dragonpsi.xyz",
+    "tube.dsocialize.net",
+    "tube.dzek.ru",
+    "tube.extinctionrebellion.fr",
+    "tube.fab-l3.org",
+    "tube.fait.ch",
+    "tube.fdn.fr",
+    "tube.fede.re",
+    "tube.fediportal.eu.org",
+    "tube.florimond.eu",
+    "tube.foxarmy.ml",
+    "tube.foxden.party",
+    "tube.frischesicht.de",
+    "tube.g1zm0.de",
+    "tube.gnous.eu",
+    "tube.grap.coop",
+    "tube.graz.social",
+    "tube.grin.hu",
+    "tube.hackerscop.org",
+    "tube.hoga.fr",
+    "tube.homecomputing.fr",
+    "tube.hordearii.fr",
+    "tube.interhacker.space",
+    "tube.jeena.net",
+    "tube.kagouille.fr",
+    "tube.kai-stuht.com",
+    "tube.kdy.ch",
+    "tube.kenfm.de",
+    "tube.kher.nl",
+    "tube.kicou.info",
+    "tube.kotur.org",
+    "tube.ksl-bmx.de",
+    "tube.lacaveatonton.ovh",
+    "tube.linc.systems",
+    "tube.linkse.media",
+    "tube.lokad.com",
+    "tube.lucie-philou.com",
+    "tube.maiti.info",
+    "tube.maliweb.at",
+    "tube.melonbread.xyz",
+    "tube.midov.pl",
+    "tube.miegl.cz",
+    "tube.motuhake.xyz",
+    "tube.mrbesen.de",
+    "tube.nah.re",
+    "tube.nchoco.net",
+    "tube.nemsia.org",
+    "tube.netzspielplatz.de",
+    "tube.nocturlab.fr",
+    "tube.novg.net",
+    "tube.nowtech.io",
+    "tube.nox-rhea.org",
+    "tube.nuagelibre.fr",
+    "tube.nx-pod.de",
+    "tube.nx12.net",
+    "tube.octaplex.net",
+    "tube.odat.xyz",
+    "tube.oisux.org",
+    "tube.okcinfo.news",
+    "tube.opportunis.me",
+    "tube.org.il",
+    "tube.ortion.xyz",
+    "tube.others.social",
+    "tube.otter.sh",
+    "tube.ouahpiti.info",
+    "tube.p2p.legal",
+    "tube.picasoft.net",
+    "tube.piweb.be",
+    "tube.plaf.fr",
+    "tube.plomlompom.com",
+    "tube.pmj.rocks",
+    "tube.poittevin.fr",
+    "tube.port0.xyz",
+    "tube.portes-imaginaire.org",
+    "tube.postblue.info",
+    "tube.privacytools.io",
+    "tube.probono.media",
+    "tube.querdenken-711.de",
+    "tube.radiomercure.fr",
+    "tube.radtke.pw",
+    "tube.rebellion.global",
+    "tube.rfc1149.net",
+    "tube.rhythms-of-resistance.org",
+    "tube.rita.moe",
+    "tube.rsi.cnr.it",
+    "tube.saik0.com",
+    "tube.saumon.io",
+    "tube.schleuss.online",
+    "tube.schule.social",
+    "tube.seditio.fr",
+    "tube.shanti.cafe",
+    "tube.shela.nu",
+    "tube.skrep.in",
+    "tube.sp4ke.com",
+    "tube.systest.eu",
+    "tube.taker.fr",
+    "tube.tappret.fr",
+    "tube.tardis.world",
+    "tube.tchncs.de",
+    "tube.tepewu.dev",
+    "tube.thechangebook.org",
+    "tube.toldi.eu",
+    "tube.traydent.info",
+    "tube.troopers.agency",
+    "tube.tty.nu",
+    "tube.tuxfriend.fr",
+    "tube.undernet.uy",
+    "tube.valinor.fr",
+    "tube.vigilian-consulting.nl",
+    "tube.villejuif.fr",
+    "tube.vraphim.com",
+    "tube.wehost.lgbt",
+    "tube.wolfe.casa",
+    "tube.worldofhauru.xyz",
+    "tube.xd0.de",
+    "tube.xy-space.de",
+    "tube.yapbreak.fr",
+    "tube.zee.li",
+    "tube.zveronline.ru",
+    "tube.zzz.blue",
+    "tube1.it.tuwien.ac.at",
+    "tubedu.org",
+    "tubes.jodh.us",
+    "tuktube.com",
+    "tuner.rayn.bo",
+    "tututu.tube",
+    "tuvideo.encanarias.info",
+    "tv.adn.life",
+    "tv.bitma.st",
+    "tv.catalpafestival.fr",
+    "tv.datamol.org",
+    "tv.diafri.com",
+    "tv.generallyrubbish.net.au",
+    "tv.mattchristiansenmedia.com",
+    "tv.mooh.fr",
+    "tv.netwhood.online",
+    "tv.neue.city",
+    "tv.piejacker.net",
+    "tv.pirateradio.social",
+    "tv.poa.st",
+    "tv.santsenques.cat",
+    "tv1.cocu.cc",
+    "tv2.cocu.cc",
+    "tvox.ru",
+    "unfilter.tube",
+    "us.tv",
+    "v.basspistol.org",
+    "v.kretschmann.social",
+    "v.lastorder.xyz",
+    "v.lesterpig.com",
+    "v.lor.sh",
+    "v.mbius.io",
+    "v.mkp.ca",
+    "v.pfaff.dev",
+    "v.phreedom.club",
+    "v.sil.sh",
+    "v.szy.io",
+    "v.wolfskaempf.de",
+    "vault.mle.party",
+    "vdp.veezee.tube",
+    "veezee.tube",
+    "vid.addyadds.us",
+    "vid.ancreport.com",
+    "vid.fai.su",
+    "vid.garwood.io",
+    "vid.lelux.fi",
+    "vid.lubar.me",
+    "vid.ncrypt.at",
+    "vid.qorg11.net",
+    "vid.rajeshtaylor.com",
+    "vid.samtripoli.com",
+    "vid.werefox.dev",
+    "vid.wildeboer.net",
+    "vid.wizards.zone",
+    "vid.y-y.li",
+    "vidcommons.org",
+    "video-cave.de",
+    "video.076.ne.jp",
+    "video.1146.nohost.me",
+    "video.ados.accoord.fr",
+    "video.altertek.org",
+    "video.anartist.org",
+    "video.antopie.org",
+    "video.apps.thedoodleproject.net",
+    "video.asgardius.company",
+    "video.autizmo.xyz",
+    "video.balsillie.net",
+    "video.barbed.fr",
+    "video.barcelo.cf",
+    "video.bards.online",
+    "video.benetou.fr",
+    "video.binarydad.com",
+    "video.blast-info.fr",
+    "video.blender.org",
+    "video.blueline.mg",
+    "video.boobalar.net",
+    "video.cabane-libre.org",
+    "video.catgirl.biz",
+    "video.chromatique.xyz",
+    "video.cigliola.com",
+    "video.cm-en-transition.fr",
+    "video.cnt.social",
+    "video.coales.co",
+    "video.codefor.de",
+    "video.codingfield.com",
+    "video.colibris-outilslibres.org",
+    "video.comptoir.net",
+    "video.comune.trento.it",
+    "video.coop.tools",
+    "video.cpn.so",
+    "video.csc49.fr",
+    "video.cybre.town",
+    "video.demokratischer-sommer.de",
+    "video.die-partei.social",
+    "video.discord-insoumis.fr",
+    "video.dresden.network",
+    "video.ecole-89.com",
+    "video.elgrillolibertario.org",
+    "video.emergeheart.info",
+    "video.eradicatinglove.xyz",
+    "video.exodus-privacy.eu.org",
+    "video.farci.org",
+    "video.fbxl.net",
+    "video.fdlibre.eu",
+    "video.fhtagn.org",
+    "video.fitchfamily.org",
+    "video.freie-linke.de",
+    "video.g3l.org",
+    "video.gcfam.net",
+    "video.greenmycity.eu",
+    "video.gresille.org",
+    "video.guerredeclasse.fr",
+    "video.hackers.town",
+    "video.hainry.fr",
+    "video.hardlimit.com",
+    "video.hdys.band",
+    "video.hooli.co",
+    "video.hylianux.com",
+    "video.iamalbanian.com",
+    "video.igem.org",
+    "video.ihatebeinga.live",
+    "video.internet-czas-dzialac.pl",
+    "video.iphodase.fr",
+    "video.irem.univ-paris-diderot.fr",
+    "video.islameye.com",
+    "video.ivancan.com",
+    "video.kicik.fr",
+    "video.kuba-orlik.name",
+    "video.kyushojitsu.ca",
+    "video.latavernedejohnjohn.fr",
+    "video.lavolte.net",
+    "video.lemediatv.fr",
+    "video.lespoesiesdheloise.fr",
+    "video.liberta.vip",
+    "video.liege.bike",
+    "video.linc.systems",
+    "video.linux.it",
+    "video.linuxtrent.it",
+    "video.livecchi.cloud",
+    "video.lokal.social",
+    "video.lono.space",
+    "video.lqdn.fr",
+    "video.lunasqu.ee",
+    "video.lundi.am",
+    "video.lw1.at",
+    "video.magicknetwork.com",
+    "video.mantlepro.com",
+    "video.marcorennmaus.de",
+    "video.mass-trespass.uk",
+    "video.maxsons.org",
+    "video.migennes.net",
+    "video.mindsforge.com",
+    "video.monarch-pass.net",
+    "video.monedalliure.org",
+    "video.monsieurbidouille.fr",
+    "video.morgiver.net",
+    "video.motoreitaliacarlonegri.it",
+    "video.mstddntfdn.online",
+    "video.mugoreve.fr",
+    "video.mundodesconocido.com",
+    "video.mustangmedic.tv",
+    "video.mycrowd.ca",
+    "video.neoziggurat.com",
+    "video.nesakko.xyz",
+    "video.nesven.eu",
+    "video.netsyms.com",
+    "video.nobodyhasthe.biz",
+    "video.nogafam.es",
+    "video.nondroit.zone",
+    "video.nyentrik.com",
+    "video.odayacres.farm",
+    "video.oh14.de",
+    "video.okaris.de",
+    "video.omniatv.com",
+    "video.p1ng0ut.social",
+    "video.p3x.de",
+    "video.passageenseine.fr",
+    "video.pcf.fr",
+    "video.ploud.fr",
+    "video.ploud.jp",
+    "video.pony.gallery",
+    "video.potate.space",
+    "video.pourpenser.pro",
+    "video.pure-isp.eu",
+    "video.qoto.org",
+    "video.rastapuls.com",
+    "video.resolutions.it",
+    "video.rhizome.org",
+    "video.rubdos.be",
+    "video.scoopgang.social",
+    "video.screamer.wiki",
+    "video.sdm-tools.net",
+    "video.selea.se",
+    "video.selogic.fr",
+    "video.sftblw.moe",
+    "video.shitposter.club",
+    "video.skyn3t.in",
+    "video.soi.ch",
+    "video.stuartbrand.co.uk",
+    "video.subak.ovh",
+    "video.taboulisme.com",
+    "video.tedomum.net",
+    "video.thierrypizza.cf",
+    "video.thinkof.name",
+    "video.toby3d.me",
+    "video.toot.pt",
+    "video.travisshears.xyz",
+    "video.triplea.fr",
+    "video.turbo.chat",
+    "video.typica.us",
+    "video.up.edu.ph",
+    "video.ustim.ru",
+    "video.vaku.org.ua",
+    "video.valme.io",
+    "video.vanderwarker.photos",
+    "video.veloma.org",
+    "video.violoncello.ch",
+    "video.vny.fr",
+    "video.wakkeren.nl",
+    "video.wilkie.how",
+    "video.writeas.org",
+    "video.wsf2021.info",
+    "video.yanoagenda.com",
+    "video.yukari.moe",
+    "videomensoif.ynh.fr",
+    "videonaute.fr",
+    "videos-libr.es",
+    "videos-passages.huma-num.fr",
+    "videos.3d-wolf.com",
+    "videos.aadtp.be",
+    "videos.ac-nancy-metz.fr",
+    "videos.adhocmusic.com",
+    "videos.ahp-numerique.fr",
+    "videos.alexandrebadalo.pt",
+    "videos.alolise.org",
+    "videos.antoine.winninger.me",
+    "videos.archigny.net",
+    "videos.benjaminbrady.ie",
+    "videos.benpro.fr",
+    "videos.buceoluegoexisto.com",
+    "videos.capas.se",
+    "videos.cemea.org",
+    "videos.city-of-glass.net",
+    "videos.cloudron.io",
+    "videos.codingotaku.com",
+    "videos.coletivos.org",
+    "videos.danksquad.org",
+    "videos.denshi.live",
+    "videos.domainepublic.net",
+    "videos.eptv.fr",
+    "videos.fap.team",
+    "videos.festivalparminous.org",
+    "videos.fromouter.space",
+    "videos.fsci.in",
+    "videos.funkwhale.audio",
+    "videos.funny.cl",
+    "videos.gerdemann.me",
+    "videos.globenet.org",
+    "videos.govanify.com",
+    "videos.gunfreezone.net",
+    "videos.hack2g2.fr",
+    "videos.hauspie.fr",
+    "videos.iut-orsay.fr",
+    "videos.john-livingston.fr",
+    "videos.jordanwarne.xyz",
+    "videos.judrey.eu",
+    "videos.koumoul.com",
+    "videos.koweb.fr",
+    "videos.laliguepaysdelaloire.org",
+    "videos.lavoixdessansvoix.org",
+    "videos.lemouvementassociatif-pdl.org",
+    "videos.lescommuns.org",
+    "videos.leslionsfloorball.fr",
+    "videos.lucero.top",
+    "videos.lukesmith.xyz",
+    "videos.martinezperspective.org",
+    "videos.martyn.berlin",
+    "videos.masonbee.nz",
+    "videos.mastodont.cat",
+    "videos.mattwilson.org",
+    "videos.monstro1.com",
+    "videos.mzhub.net",
+    "videos.npa-marseille.org",
+    "videos.pair2jeux.tube",
+    "videos.pofilo.fr",
+    "videos.pueseso.club",
+    "videos.pzelawski.xyz",
+    "videos.quinlan.cloud",
+    "videos.rampin.org",
+    "videos.realnephestate.xyz",
+    "videos.scanlines.xyz",
+    "videos.sibear.fr",
+    "videos.side-ways.net",
+    "videos.squat.net",
+    "videos.stadtfabrikanten.org",
+    "videos.tankernn.eu",
+    "videos.tcit.fr",
+    "videos.testimonia.org",
+    "videos.thisishowidontdisappear.com",
+    "videos.tormentasolar.win",
+    "videos.traumaheilung.net",
+    "videos.trom.tf",
+    "videos.ubuntu-paris.org",
+    "videos.upr.fr",
+    "videos.wakkerewereld.nu",
+    "videos.weblib.re",
+    "videos.yesil.club",
+    "videosdulib.re",
+    "videotape.me",
+    "vids.jaguarmundi.com",
+    "vids.roshless.me",
+    "vidz.dou.bet",
+    "visionon.tv",
+    "vod.ksite.de",
+    "vod.lumikko.dev",
+    "vs.uniter.network",
+    "vulgarisation-informatique.fr",
+    "wago.tube",
+    "watch.44con.com",
+    "watch.breadtube.tv",
+    "watch.deranalyst.ch",
+    "watch.ignorance.eu",
+    "watch.krazy.party",
+    "watch.legalloli.net",
+    "watch.libertaria.space",
+    "watch.rt4mn.org",
+    "watch.snoot.tube",
+    "watch.tubelab.video",
+    "webtv.vandoeuvre.net",
+    "whitepositive.media",
+    "widemus.de",
+    "wiki-tube.de",
+    "wikileaks.video",
+    "wiwi.video",
+    "worldofvids.com",
+    "wwtube.net",
+    "www.captain-german.com",
+    "www.nadajemy.com",
+    "www.quertube.de",
+    "www.rezwatch.tv",
+    "www.shijiaojingjing.com",
+    "www.videos-libr.es",
+    "www.wiki-tube.de",
+    "www.yiny.org",
+    "www.youtorrent.tv",
+    "www4.mir.inter21.net",
+    "xxivproduction.video",
+    "xxx.noho.st",
+    "xxxporn.co.uk",
+    "yt.is.nota.live",
+    "yunopeertube.myddns.me",
+}
+
+__all__ = ['instances']
diff --git a/youtube_dl/extractor/peertube/peertube.py b/youtube_dl/extractor/peertube/peertube.py
new file mode 100644
index 000000000..8c8f8e6e5
--- /dev/null
+++ b/youtube_dl/extractor/peertube/peertube.py
@@ -0,0 +1,284 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .instances import instances
+from ..common import InfoExtractor
+from ...compat import compat_str
+from ...utils import (
+    int_or_none,
+    parse_resolution,
+    str_or_none,
+    try_get,
+    unified_timestamp,
+    url_or_none,
+    urljoin,
+    ExtractorError,
+    preferredencoding,
+)
+
+
+known_valid_instances = set()
+
+
+class PeerTubeIE(InfoExtractor):
+    _UUID_RE = r'[\da-fA-F]{8}-[\da-fA-F]{4}-[\da-fA-F]{4}-[\da-fA-F]{4}-[\da-fA-F]{12}'
+    _API_BASE = 'https://%s/api/v1/videos/%s/%s'
+    _VALID_URL = r'''(?x)
+                    (?:
+                        (?P<prefix>peertube:)(?P<host>[^:]+):|
+                        https?://(?P<host_2>[^/]+)/(?:videos/(?:watch|embed)|api/v\d/videos)/
+                    )
+                    (?P<id>%s)
+                    ''' % _UUID_RE
+    _TESTS = [{
+        'url': 'https://framatube.org/videos/watch/9c9de5e8-0a1e-484a-b099-e80766180a6d',
+        'md5': '9bed8c0137913e17b86334e5885aacff',
+        'info_dict': {
+            'id': '9c9de5e8-0a1e-484a-b099-e80766180a6d',
+            'ext': 'mp4',
+            'title': 'What is PeerTube?',
+            'description': 'md5:3fefb8dde2b189186ce0719fda6f7b10',
+            'thumbnail': r're:https?://.*\.(?:jpg|png)',
+            'timestamp': 1538391166,
+            'upload_date': '20181001',
+            'uploader': 'Framasoft',
+            'uploader_id': '3',
+            'uploader_url': 'https://framatube.org/accounts/framasoft',
+            'channel': 'Les vid√©os de Framasoft',
+            'channel_id': '2',
+            'channel_url': 'https://framatube.org/video-channels/bf54d359-cfad-4935-9d45-9d6be93f63e8',
+            'language': 'en',
+            'license': 'Attribution - Share Alike',
+            'duration': 113,
+            'view_count': int,
+            'like_count': int,
+            'dislike_count': int,
+            'tags': ['framasoft', 'peertube'],
+            'categories': ['Science & Technology'],
+        }
+    }, {
+        # Issue #26002
+        'url': 'peertube:spacepub.space:d8943b2d-8280-497b-85ec-bc282ec2afdc',
+        'info_dict': {
+            'id': 'd8943b2d-8280-497b-85ec-bc282ec2afdc',
+            'ext': 'mp4',
+            'title': 'Dot matrix printer shell demo',
+            'uploader_id': '3',
+            'timestamp': 1587401293,
+            'upload_date': '20200420',
+            'uploader': 'Drew DeVault',
+        }
+    }, {
+        # nsfw
+        'url': 'https://tube.22decembre.eu/videos/watch/9bb88cd3-9959-46d9-9ab9-33d2bb704c39',
+        'only_matching': True,
+    }, {
+        'url': 'https://tube.22decembre.eu/videos/embed/fed67262-6edb-4d1c-833b-daa9085c71d7',
+        'only_matching': True,
+    }, {
+        'url': 'peertube:video.blender.org:b37a5b9f-e6b5-415c-b700-04a5cd6ec205',
+        'only_matching': True,
+    }]
+
+    @staticmethod
+    def _extract_peertube_url(webpage, source_url):
+        mobj = re.match(
+            r'https?://(?P<host>[^/]+?)/videos/(?:watch|embed)/(?P<id>%s)'
+            % PeerTubeIE._UUID_RE, source_url)
+        if mobj and any(p in webpage for p in (
+                '<title>PeerTube<',
+                'There will be other non JS-based clients to access PeerTube',
+                '>We are sorry but it seems that PeerTube is not compatible with your web browser.<')):
+            return 'peertube:%s:%s' % mobj.group('host', 'id')
+
+    @staticmethod
+    def _extract_urls(webpage, source_url):
+        entries = re.findall(
+            r'''(?x)<iframe[^>]+\bsrc=["\'](?P<url>(?:https?:)?//[^/]+?/videos/embed/%s)'''
+            % PeerTubeIE._UUID_RE, webpage)
+        if not entries:
+            peertube_url = PeerTubeIE._extract_peertube_url(webpage, source_url)
+            if peertube_url:
+                entries = [peertube_url]
+        return entries
+
+    @classmethod
+    def suitable(cls, url):
+        mobj = re.match(cls._VALID_URL, url)
+        if not mobj:
+            return False
+        prefix = mobj.group('prefix')
+        hostname = mobj.group('host') or mobj.group('host_2')
+        return cls._test_peertube_instance(None, hostname, True, prefix)
+
+    @staticmethod
+    def _test_peertube_instance(ie, hostname, skip, prefix):
+        hostname = hostname.encode('idna')
+        if not isinstance(hostname, compat_str):
+            hostname = hostname.decode(preferredencoding())
+
+        if hostname in instances:
+            return True
+        if hostname in known_valid_instances:
+            return True
+
+        # HELP: more cases needed
+        # if hostname in ['medium.com', 'lbry.tv']:
+        #     return False
+
+        # continue anyway if "peertube:" is used
+        if prefix:
+            return True
+        # without --check-peertube-instance,
+        #   skip further instance check
+        if skip:
+            return False
+
+        ie.report_warning('Testing if %s is a PeerTube instance because it is not listed in joinpeertube.org.' % hostname)
+
+        try:
+            # try /api/v1/config
+            api_request_config = ie._download_json(
+                'https://%s/api/v1/config' % hostname, hostname,
+                note='Testing PeerTube API /api/v1/config')
+            if not api_request_config.get('instance', {}).get('name'):
+                return False
+
+            # try /api/v1/videos
+            api_request_videos = ie._download_json(
+                'https://%s/api/v1/videos' % hostname, hostname,
+                note='Testing PeerTube API /api/v1/videos')
+            if not isinstance(api_request_videos.get('data'), (tuple, list)):
+                return False
+        except (IOError, ExtractorError):
+            return False
+
+        # this is probably peertube instance
+        known_valid_instances.add(hostname)
+        return True
+
+    def _call_api(self, host, video_id, path, note=None, errnote=None, fatal=True):
+        return self._download_json(
+            self._API_BASE % (host, video_id, path), video_id,
+            note=note, errnote=errnote, fatal=fatal)
+
+    def _get_subtitles(self, host, video_id):
+        captions = self._call_api(
+            host, video_id, 'captions', note='Downloading captions JSON',
+            fatal=False)
+        if not isinstance(captions, dict):
+            return
+        data = captions.get('data')
+        if not isinstance(data, list):
+            return
+        subtitles = {}
+        for e in data:
+            language_id = try_get(e, lambda x: x['language']['id'], compat_str)
+            caption_url = urljoin('https://%s' % host, e.get('captionPath'))
+            if not caption_url:
+                continue
+            subtitles.setdefault(language_id or 'en', []).append({
+                'url': caption_url,
+            })
+        return subtitles
+
+    def _real_extract(self, url):
+        mobj = re.match(self._VALID_URL, url)
+        host = mobj.group('host') or mobj.group('host_2')
+        video_id = mobj.group('id')
+
+        video = self._call_api(
+            host, video_id, '', note='Downloading video JSON')
+
+        title = video['name']
+
+        formats = []
+        files = video.get('files') or []
+        for playlist in (video.get('streamingPlaylists') or []):
+            if not isinstance(playlist, dict):
+                continue
+            playlist_files = playlist.get('files')
+            if not (playlist_files and isinstance(playlist_files, list)):
+                continue
+            files.extend(playlist_files)
+        for file_ in files:
+            if not isinstance(file_, dict):
+                continue
+            file_url = url_or_none(file_.get('fileUrl'))
+            if not file_url:
+                continue
+            file_size = int_or_none(file_.get('size'))
+            format_id = try_get(
+                file_, lambda x: x['resolution']['label'], compat_str)
+            f = parse_resolution(format_id)
+            f.update({
+                'url': file_url,
+                'format_id': format_id,
+                'filesize': file_size,
+            })
+            if format_id == '0p':
+                f['vcodec'] = 'none'
+            else:
+                f['fps'] = int_or_none(file_.get('fps'))
+            formats.append(f)
+        self._sort_formats(formats)
+
+        full_description = self._call_api(
+            host, video_id, 'description', note='Downloading description JSON',
+            fatal=False)
+
+        description = None
+        if isinstance(full_description, dict):
+            description = str_or_none(full_description.get('description'))
+        if not description:
+            description = video.get('description')
+
+        subtitles = self.extract_subtitles(host, video_id)
+
+        def data(section, field, type_):
+            return try_get(video, lambda x: x[section][field], type_)
+
+        def account_data(field, type_):
+            return data('account', field, type_)
+
+        def channel_data(field, type_):
+            return data('channel', field, type_)
+
+        category = data('category', 'label', compat_str)
+        categories = [category] if category else None
+
+        nsfw = video.get('nsfw')
+        if nsfw is bool:
+            age_limit = 18 if nsfw else 0
+        else:
+            age_limit = None
+
+        webpage_url = 'https://%s/videos/watch/%s' % (host, video_id)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'description': description,
+            'thumbnail': urljoin(webpage_url, video.get('thumbnailPath')),
+            'timestamp': unified_timestamp(video.get('publishedAt')),
+            'uploader': account_data('displayName', compat_str),
+            'uploader_id': str_or_none(account_data('id', int)),
+            'uploader_url': url_or_none(account_data('url', compat_str)),
+            'channel': channel_data('displayName', compat_str),
+            'channel_id': str_or_none(channel_data('id', int)),
+            'channel_url': url_or_none(channel_data('url', compat_str)),
+            'language': data('language', 'id', compat_str),
+            'license': data('licence', 'label', compat_str),
+            'duration': int_or_none(video.get('duration')),
+            'view_count': int_or_none(video.get('views')),
+            'like_count': int_or_none(video.get('likes')),
+            'dislike_count': int_or_none(video.get('dislikes')),
+            'age_limit': age_limit,
+            'tags': try_get(video, lambda x: x['tags'], list),
+            'categories': categories,
+            'formats': formats,
+            'subtitles': subtitles,
+            'webpage_url': webpage_url,
+        }
diff --git a/youtube_dl/extractor/pixivsketch.py b/youtube_dl/extractor/pixivsketch.py
new file mode 100644
index 000000000..219a600de
--- /dev/null
+++ b/youtube_dl/extractor/pixivsketch.py
@@ -0,0 +1,111 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+from ..utils import (
+    try_get,
+    ExtractorError,
+)
+from ..compat import (
+    compat_str,
+)
+
+
+class PixivSketchBaseIE(InfoExtractor):
+    pass
+
+
+class PixivSketchIE(PixivSketchBaseIE):
+    IE_NAME = 'pixiv:sketch'
+    # https://sketch.pixiv.net/@kotaru_taruto/lives/3404565243464976376
+    _VALID_URL = r'https?://sketch\.pixiv\.net/@(?P<uploader_id>[a-zA-Z0-9_-]+)/lives/(?P<id>\d+)/?'
+    _TEST = {}
+    API_JSON_URL = 'https://sketch.pixiv.net/api/lives/%s.json'
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        uploader_id_url = compat_str(self._VALID_URL_RE.match(url).group('uploader_id'))
+        data = self._download_json(self.API_JSON_URL % video_id, video_id, headers={
+            'Referer': url,
+            'X-Requested-With': url,
+        })['data']
+
+        if not data or not data['is_broadcasting']:
+            raise ExtractorError('This live is offline. Use https://sketch.pixiv.net/%s for ongoing live.' % uploader_id_url, expected=True)
+
+        formats = self._extract_m3u8_formats(
+            data['owner']['hls_movie']['url'], video_id, ext='mp4',
+            entry_protocol='m3u8_native', m3u8_id='hls')
+        self._sort_formats(formats)
+
+        title = data['name']
+        uploader = try_get(data, (
+            lambda x: x['user']['name'],
+            lambda x: x['owner']['user']['name'],
+        ), None)
+        uploader_id = try_get(data, (
+            lambda x: x['user']['unique_name'],
+            lambda x: x['owner']['user']['unique_name'],
+        ), None) or uploader_id_url
+        uploader_id_numeric = try_get(data, (
+            lambda x: compat_str(x['user']['id']),
+            lambda x: compat_str(x['owner']['user']['id']),
+        ), None)
+        uploader_pixiv_id = try_get(data, (
+            lambda x: compat_str(x['user']['pixiv_user_id']),
+            lambda x: compat_str(x['owner']['user']['pixiv_user_id']),
+        ), None)
+        if data['is_r18']:
+            age_limit = 18
+        elif data['is_r15']:
+            age_limit = 15
+        else:
+            age_limit = 0
+
+        return {
+            'formats': formats,
+            'id': video_id,
+            'title': title,
+            'uploader': uploader,
+            'uploader_id': uploader_id,
+            'uploader_id_numeric': uploader_id_numeric,
+            'uploader_pixiv_id': uploader_pixiv_id,
+            'age_limit': age_limit,
+            'is_live': True
+            # 'raw': data,
+        }
+
+
+class PixivSketchUserIE(PixivSketchBaseIE):
+    IE_NAME = 'pixiv:sketch:user'
+    # https://sketch.pixiv.net/@kotaru_taruto
+    _VALID_URL = r'https?://sketch\.pixiv\.net/@(?P<id>[a-zA-Z0-9_-]+)/?'
+    API_JSON_URL = 'https://sketch.pixiv.net/api/lives/users/@%s.json'
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        data = self._download_json(self.API_JSON_URL % video_id, video_id, headers={
+            'Referer': url,
+            'X-Requested-With': url,
+        })['data']
+
+        if not data:
+            try:
+                self._download_json('https://sketch.pixiv.net/api/users/current.json', video_id, headers={
+                    'Referer': url,
+                    'X-Requested-With': url,
+                }, note='Investigating reason of download failure')['data']
+            except ExtractorError as cause:
+                if cause.cause.code == 401:
+                    # without login, it throws 401
+                    raise ExtractorError('Please log in, or use live URL like https://sketch.pixiv.net/@%s/1234567890' % video_id,
+                                         expected=True, cause=cause.cause)
+                else:
+                    raise
+            else:
+                raise ExtractorError('This user is offline', expected=True)
+        if not data['is_broadcasting']:
+            raise ExtractorError('This user is offline', expected=True)
+
+        live_id = data['id']
+        return self.url_result('https://sketch.pixiv.net/@%s/lives/%s' % (video_id, live_id))
diff --git a/youtube_dl/extractor/pornhub.py b/youtube_dl/extractor/pornhub.py
index 031454600..aade5337b 100644
--- a/youtube_dl/extractor/pornhub.py
+++ b/youtube_dl/extractor/pornhub.py
@@ -727,7 +727,7 @@ class PornHubPagedVideoListIE(PornHubPagedPlaylistBaseIE):
     @classmethod
     def suitable(cls, url):
         return (False
-                if PornHubIE.suitable(url) or PornHubUserIE.suitable(url) or PornHubUserVideosUploadIE.suitable(url)
+                if PornHubIE.suitable(url) or PornHubUserIE.suitable(url) or PornHubUserVideosUploadIE.suitable(url) or PornHubUserLiveIE.suitable(url)
                 else super(PornHubPagedVideoListIE, cls).suitable(url))
 
 
@@ -743,3 +743,75 @@ class PornHubUserVideosUploadIE(PornHubPagedPlaylistBaseIE):
         'url': 'https://www.pornhub.com/model/zoe_ph/videos/upload',
         'only_matching': True,
     }]
+
+
+class PornHubUserLiveIE(PornHubBaseIE):
+    _VALID_URL = r'(?P<url>https?://(?:[^/]+\.)?(?P<host>pornhub(?:premium)?\.(?:com|net|org))/live/(?P<id>[^/?#&]+))(?:[?#&]|/(?!videos)|$)'
+    _TESTS = [{
+        'url': 'https://www.pornhub.com/live/sadiejaymes?track=6001',
+        'only_matching': True,
+    }, {
+        'url': 'https://www.pornhub.com/live/sadiejaymes',
+        'only_matching': True,
+    }, {
+        'url': 'https://www.pornhub.com/live/adorablenyan?track=6001',
+        'only_matching': True,
+    }, {
+        'url': 'https://www.pornhub.com/live/adorablenyan',
+        'only_matching': True,
+    }]
+
+    SANITIZED_URL = 'https://www.pornhub.com/live/%s'
+    LIVE_API_URL = 'https://manifest-server.naiadsystems.com/live/s:%s.json?last=load&format=mp4-hls'
+
+    def _real_extract(self, url):
+        user_id = self._match_id(url)
+
+        webpage = self._download_webpage(self.SANITIZED_URL % user_id, user_id)
+        title = self._search_regex(r'<title>(.+?) : Online .+?</title>', webpage, 'title')
+
+        live_response = self._download_json(self.LIVE_API_URL % user_id, user_id)
+        response_formats = live_response.get('formats')
+
+        hls_manifest = response_formats.get('mp4-hls', {}).get('manifest')
+        if hls_manifest:
+            formats = self._extract_m3u8_formats(
+                hls_manifest, user_id,
+                ext='mp4', entry_protocol='m3u8_native',
+                m3u8_id='hls', live=True)
+        else:
+            formats = []
+
+        rtmp_section = response_formats.get('mp4-rtmp')
+        rtmp_vcodec = rtmp_section.get('videoCodec')
+        rtmp_acodec = rtmp_section.get('audioCodec')
+
+        for fmt in rtmp_section.get('encodings'):
+            rtmp_url = fmt.get('location')
+            video_width = fmt.get('videoWidth')
+            video_height = fmt.get('videoHeight')
+            vbr = fmt.get('videoKbps')
+            abr = fmt.get('audioKbps')
+            formats.append({
+                'format_id': 'rtmp-%s' % video_width,
+                'protocol': 'rtmp',
+                'ext': 'mp4',
+                'url': rtmp_url,
+                'width': video_width,
+                'height': video_height,
+                'vcodec': rtmp_vcodec,
+                'acodec': rtmp_acodec,
+                'vbr': vbr,
+                'abr': abr,
+                # 'preference': -1,
+            })
+
+        self._sort_formats(formats)
+        return {
+            'id': user_id,
+            'uploader': user_id,
+            'uploader_id': user_id,
+            'title': title,
+            'formats': formats,
+            'is_live': True,
+        }
diff --git a/youtube_dl/extractor/qawebsites.py b/youtube_dl/extractor/qawebsites.py
new file mode 100644
index 000000000..dde5970e3
--- /dev/null
+++ b/youtube_dl/extractor/qawebsites.py
@@ -0,0 +1,169 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import ExtractorError, clean_html, unescapeHTML
+
+from .youtube import (
+    YoutubeIE,
+    YoutubePlaylistIE,
+    YoutubeTabIE,
+)
+from .twitter import TwitterIE
+from .instagram import InstagramIE
+
+
+class QAWebsitesBaseIE(InfoExtractor):
+    # extractors to be searched against
+    _SEARCH_IE = (InstagramIE, YoutubeIE, YoutubePlaylistIE, YoutubeTabIE, TwitterIE)
+
+    def _extract_text(self, url):
+        " Implement this in subclasses "
+        return ('question', 'answer')
+
+    def _real_extract(self, url):
+        question_id = self._match_id(url)
+        question_text, answer_text = self._extract_text(url)
+
+        if not question_text and not answer_text:
+            raise ExtractorError('Nothing is present')
+        if not question_text:
+            self.report_warning('Cannot extract question text')
+        if not answer_text:
+            self.report_warning('Cannot extract answer text')
+
+        matches = set(
+            x.group(0)
+            for message in (question_text, answer_text)
+            for ie in self._SEARCH_IE
+            for x in re.finditer(self.remove_anchors(ie._VALID_URL), message))
+        if not matches:
+            raise ExtractorError('There is no video URLs here', expected=True)
+
+        return self.playlist_result([self.url_result(x) for x in matches], question_id)
+
+    @staticmethod
+    def remove_anchors(regex):
+        for rr in (r'^\s*(\(\?[a-z]+\))?\s*\^', r'()\$\s*$'):
+            regex = re.sub(rr, r'\g<1>', regex)
+        return regex
+
+
+class PeingIE(QAWebsitesBaseIE):
+    IE_NAME = 'peing'
+    _VALID_URL = r'https?://(?:www\.)?peing\.net/(?:(?:en|ja|zh-CN|zh-TW|ko|dt|th)/)?q/(?P<id>[a-f0-9-]{36})'
+
+    _TEST = {
+        'url': 'https://peing.net/ja/q/483f91de-f607-4384-ba5a-28ca22a3aa67',
+        'info_dict': {},
+        'add_ie': YoutubeIE.ie_key(),
+    }
+
+    def _extract_text(self, url):
+        question_id = self._match_id(url)
+        webpage = self._download_webpage(url, question_id)
+
+        question_text = self._html_search_meta(
+            ('twitter:image:alt', 'og:image:alt'),
+            webpage) or self._search_regex(
+            (r'<img\s+alt="(.+?)"\s*class="question-eye-catch"\s*onerror="',
+             r'data-item-body=\'(.+?)\'',
+             r'<title>(.+?)\s*\|'),
+            webpage, 'question text', fatal=False) or ''
+
+        answer_text = self._html_search_meta(
+            'description', webpage) or self._search_regex(
+            r"<p\s+class=(['\"])text\1>(.+?)</p>", webpage,
+            'answer text', fatal=False, group=2) or ''
+
+        return question_text, answer_text
+
+
+class AskfmIE(QAWebsitesBaseIE):
+    IE_NAME = 'ask.fm'
+    _VALID_URL = r'https?://ask\.fm/(?P<uploader>[^/]+)/answers/(?P<id>\d+)'
+
+    def _extract_text(self, url):
+        question_id = self._match_id(url)
+        webpage = self._download_webpage(url, question_id)
+
+        question_text = self._html_search_meta(
+            ('og:title'), webpage) or self._search_regex(
+            (r'<h1 class="medium">(.+?)</h1>',
+             r'data-item-body=\'(.+?)\'',
+             r'<title>(.+?)\s*\|'),
+            webpage, 'question text', fatal=False)
+        if not question_text:
+            params = self._search_regex(r'data-params="(.+?)"', webpage, 'question text', fatal=False)
+            params = unescapeHTML(params)
+            params = self._parse_json(params, question_id, fatal=False)
+            if params:
+                question_text = params['question[question_text]']
+        if not question_text:
+            question_text = ''
+
+        answer_text = self._search_regex(
+            r'<div class="streamItem_content">(.+?)</div>', webpage,
+            'answer text', fatal=False) or self._html_search_meta(
+            ('og:description', 'description'), webpage) or ''
+
+        return question_text, answer_text
+
+
+class MarshmallowQAIE(QAWebsitesBaseIE):
+    IE_NAME = 'marshmallow-qa'
+    _VALID_URL = r'https?://(?:www\.)?marshmallow-qa\.com/messages/(?P<id>[a-f0-9-]{36})'
+
+    _TEST = {
+        'url': 'https://marshmallow-qa.com/messages/513fb153-8e9b-4173-bea9-2210339dd81e',
+        'only_matching': True,
+    }
+
+    def _extract_text(self, url):
+        question_id = self._match_id(url)
+        webpage = self._download_webpage(url, question_id)
+
+        question_text = self._search_regex(
+            (r'<meta property="(?:og|twitter):title" content="(.+?)\s*\|\s*[^"]+?"\s*/>',
+             r'<div data-target="obscene-word\.content">(.+?)</div>',
+             r'<title>\s*(.+?)\s*\|'),
+            webpage, 'question text', fatal=False) or ''
+
+        answer_text = self._search_regex(
+            r'<div class="answer-content pre-wrap text-dark" data-target="obscene-word\.content">(.+?)</div>',
+            webpage, 'answer text', fatal=False) or ''
+
+        return question_text, answer_text
+
+
+class MottohometeIE(QAWebsitesBaseIE):
+    IE_NAME = 'mottohomete'
+    _VALID_URL = r'https?://(?:www\.)?mottohomete\.net/letters/(?P<id>[a-f0-9-]{36})'
+
+    _TEST = {
+        'url': 'https://www.mottohomete.net/letters/cbfc80f6-61b4-44c1-9772-b2de9702f1ce',
+        'only_matching': True,
+    }
+
+    def _extract_text(self, url):
+        question_id = self._match_id(url)
+        webpage = self._download_webpage(url, question_id)
+
+        question_text = self._html_search_meta(
+            ('twitter:title', 'og:title'),
+            webpage)
+        if not question_text:
+            question_text = self._search_regex(
+                r"(?s)<div class='panel-heading'>\s*<div class='cwrap'><p>(.+?)</p>",
+                webpage, 'question text', fatal=False)
+            question_text = clean_html(question_text)
+        if not question_text:
+            question_text = ''
+
+        answer_text = self._search_regex(
+            r"(?s)<div class='panel-body'>\s*<div class='cwrap'><p>(.+?)</p>",
+            webpage, 'answer text', fatal=False) or ''
+
+        return question_text, answer_text
diff --git a/youtube_dl/extractor/radiko.py b/youtube_dl/extractor/radiko.py
new file mode 100644
index 000000000..ee6cab711
--- /dev/null
+++ b/youtube_dl/extractor/radiko.py
@@ -0,0 +1,165 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+import base64
+
+from .common import InfoExtractor
+from ..utils import (
+    ExtractorError,
+    update_url_query,
+    clean_html,
+    unified_timestamp,
+    time_millis,
+)
+from ..compat import compat_urllib_parse
+
+
+class RadikoIE(InfoExtractor):
+    _VALID_URL = r'https?://(?:www.)?radiko\.jp/#!/ts/(?P<station>[A-Z-]+)/(?P<id>\d+)'
+    _FULL_KEY = None
+    _AUTH_CACHE = ()
+
+    _TESTS = [{
+        'url': 'https://radiko.jp/#!/ts/QRR/20210425101300',
+        'only_matching': True,
+    }, {
+        'url': 'https://radiko.jp/#!/ts/JOAK-FM/20210509090000',
+        'only_matching': True,
+    }]
+
+    def _real_extract(self, url):
+        m = self._valid_url_re().match(url)
+        station = m.group('station')
+        video_id = m.group('id')
+        vid_int = unified_timestamp(video_id, False)
+
+        auth_token, area_id = self._auth_client()
+
+        station_program = self._download_xml(
+            'https://radiko.jp/v3/program/station/weekly/%s.xml' % station, video_id,
+            note='Downloading radio program for %s station' % station)
+
+        prog = None
+        for p in station_program.findall('.//prog'):
+            ft_str, to_str = p.attrib['ft'], p.attrib['to']
+            ft = unified_timestamp(ft_str, False)
+            to = unified_timestamp(to_str, False)
+            if ft <= vid_int and vid_int < to:
+                prog = p
+                break
+        if not prog:
+            raise ExtractorError('Cannot identify radio program to download!')
+        assert ft, to
+
+        title = prog.find('title').text
+        description = clean_html(prog.find('desc').text)
+        program_description = clean_html(prog.find('info').text)
+
+        m3u8_playlist_data = self._download_xml(
+            'https://radiko.jp/v3/station/stream/pc_html5/%s.xml' % station, video_id,
+            note='Downloading m3u8 information')
+        m3u8_urls = m3u8_playlist_data.findall('.//url')
+
+        formats = []
+        found = set()
+        for url_tag in m3u8_urls:
+            pcu = url_tag.find('playlist_create_url')
+            url_attrib = url_tag.attrib
+            playlist_url = update_url_query(pcu.text, {
+                'station_id': station,
+                'start_at': ft_str,  # begin time of the radio
+                'ft': ft_str,  # same as start_id
+                'end_at': to_str,  # end time of the radio
+                'to': to_str,  # same as end_at
+                'seek': video_id,
+                'l': '15',
+                'lsid': '77d0678df93a1034659c14d6fc89f018',
+                'type': 'b',
+            })
+            if playlist_url in found:
+                continue
+            else:
+                found.add(playlist_url)
+
+            time_to_skip = vid_int - ft
+            try:
+                subformats = self._extract_m3u8_formats(
+                    playlist_url, video_id, ext='mp4', entry_protocol='m3u8',
+                    live=True, fatal=False, m3u8_id=None,
+                    headers={
+                        'X-Radiko-AreaId': area_id,
+                        'X-Radiko-AuthToken': auth_token,
+                    })
+                for sf in subformats:
+                    sf['format_id'] = compat_urllib_parse.urlparse(sf['url']).netloc
+                    if re.match(r'^[cf]-radiko\.smartstream\.ne\.jp$', sf['format_id']):
+                        sf['preference'] = -100  # they probably goes to different station
+                    if url_attrib['timefree'] == '1' and time_to_skip:
+                        # sf['format_note'] = 'timefree'
+                        sf['input_params'] = ['-ss', '%d' % time_to_skip]
+                formats.extend(subformats)
+            except ExtractorError:
+                pass
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'description': description,
+            'program_description': program_description,
+            'formats': formats,
+            'is_live': True,
+        }
+
+    def _auth_client(self):
+        if self._AUTH_CACHE:
+            return self._AUTH_CACHE
+
+        auth1_handle = self._download_webpage_handle(
+            'https://radiko.jp/v2/api/auth1', None, 'Authenticating (1)',
+            headers={
+                'x-radiko-app': 'pc_html5',
+                'x-radiko-app-version': '0.0.1',
+                'x-radiko-device': 'pc',
+                'x-radiko-user': 'dummy_user',
+            })[1]  # response body is completely useless
+        auth1_header = auth1_handle.info()
+
+        auth_token = auth1_header['X-Radiko-AuthToken']
+        kl = int(auth1_header['X-Radiko-KeyLength'])
+        ko = int(auth1_header['X-Radiko-KeyOffset'])
+        raw_partial_key = self._extract_full_key()[ko:ko + kl]
+        partial_key = base64.b64encode(raw_partial_key).decode()
+
+        area_id = self._download_webpage(
+            'https://radiko.jp/v2/api/auth2', None, 'Authenticating (2)',
+            headers={
+                'x-radiko-device': 'pc',
+                'x-radiko-user': 'dummy_user',
+                'x-radiko-authtoken': auth_token,
+                'x-radiko-partialkey': partial_key,
+            }).split(',')[0]
+
+        self._AUTH_CACHE = (auth_token, area_id)
+        return self._AUTH_CACHE
+
+    def _extract_full_key(self):
+        if self._FULL_KEY:
+            return self._FULL_KEY
+
+        jscode = self._download_webpage(
+            'https://radiko.jp/apps/js/playerCommon.js?_=%d' % time_millis(), None,
+            note='Downloading player js code')
+        full_key = self._search_regex(
+            (r"RadikoJSPlayer\([^,]*,\s*(['\"])pc_html5\1,\s*(['\"])(?P<fullkey>[0-9a-f]+)\2,\s*{"),
+            jscode, 'full key', fatal=False, group='fullkey')
+
+        if full_key:
+            full_key = full_key.encode()
+        else:  # use full key ever known
+            full_key = b'bcd151073c03b352e1ef2fd66c32209da9ca0afa'
+
+        self._FULL_KEY = full_key
+        return full_key
diff --git a/youtube_dl/extractor/reddit.py b/youtube_dl/extractor/reddit.py
index 222fa0172..e2be1d534 100644
--- a/youtube_dl/extractor/reddit.py
+++ b/youtube_dl/extractor/reddit.py
@@ -11,6 +11,9 @@ from ..utils import (
     unescapeHTML,
     url_or_none,
 )
+from ..compat import (
+    compat_urllib_parse,
+)
 
 
 class RedditIE(InfoExtractor):
@@ -107,9 +110,10 @@ class RedditRIE(InfoExtractor):
             url + '/.json', video_id)[0]['data']['children'][0]['data']
 
         video_url = data['url']
+        parsed_video_url = compat_urllib_parse.urlparse(video_url)
 
         # Avoid recursing into the same reddit URL
-        if 'reddit.com/' in video_url and '/%s/' % video_id in video_url:
+        if parsed_video_url.hostname.endswith('reddit.com') and '/%s/' % video_id in video_url:
             raise ExtractorError('No media found', expected=True)
 
         over_18 = data.get('over_18')
diff --git a/youtube_dl/extractor/sharevideos.py b/youtube_dl/extractor/sharevideos.py
new file mode 100644
index 000000000..41fbb018a
--- /dev/null
+++ b/youtube_dl/extractor/sharevideos.py
@@ -0,0 +1,65 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from ..utils import KNOWN_EXTENSIONS, determine_ext, mimetype2ext, try_get
+from .common import InfoExtractor
+
+
+class ShareVideosIE(InfoExtractor):
+    IE_NAME = 'sharevideos'
+    _VALID_URL = r'https?://(?:embed\.)?share-videos\.se/auto/(?:embed|video)/(?P<id>\d+)\?uid=(?P<uid>\d+)'
+    TEST = {
+        'url': 'http://share-videos.se/auto/video/83645793?uid=13',
+        'md5': 'b68d276de422ab07ee1d49388103f457',
+        'info_dict': {
+            'id': '83645793',
+            'title': 'Lock up and get excited',
+            'ext': 'mp4'
+        },
+        'skip': 'TODO: fix nested playlists processing in tests',
+    }
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        uid = self._VALID_URL_RE.match(url).group('uid')
+        webpage = self._download_webpage('https://embed.share-videos.se/auto/embed/%s?uid=%s' % (video_id, uid), video_id)
+
+        title = self._html_extract_title(webpage, 'video title', default=None)
+        if not title:
+            video_webpage = self._download_webpage(
+                'https://share-videos.se/auto/video/%s?uid=%s' % (video_id, uid),
+                video_id)
+            title = self._html_extract_title(video_webpage, 'video title', default=None)
+        if not title:
+            tags = self._download_json(
+                'https://search.share-videos.se/json/movie_tag?svid=%s&site=sv' % video_id,
+                video_id, note='Giving it a better name', fatal=None)
+            if tags and isinstance(tags, (list, tuple)):
+                title = ' '.join(tags)
+        if not title:
+            self.report_warning('There is no title candidate for this video', video_id)
+            title = 'untitled'
+
+        def extract_mp4_url(x):
+            src = self._search_regex(r'random_file\.push\("(.+)"\);', webpage, 'video url')
+            src_type = self._search_regex(r'player.src\({type: \'(.+);\',', webpage, 'video type', fatal=False, default='video/mp4')
+            ext = determine_ext(src).lower()
+            return {
+                'formats': [
+                    {
+                        'url': src,
+                        'ext': (mimetype2ext(src_type)
+                                or ext if ext in KNOWN_EXTENSIONS else 'mp4'),
+                    }
+                ]
+            }
+        entry = try_get(webpage, (
+            lambda x: self._parse_html5_media_entries(url, webpage, video_id, m3u8_id='hls')[0],
+            extract_mp4_url,
+        ), dict)
+        self._sort_formats(entry['formats'])
+        entry.update({
+            'id': video_id,
+            'title': title,
+        })
+        return entry
diff --git a/youtube_dl/extractor/sohu.py b/youtube_dl/extractor/sohu.py
index a62ed84f1..af6e99558 100644
--- a/youtube_dl/extractor/sohu.py
+++ b/youtube_dl/extractor/sohu.py
@@ -6,7 +6,7 @@ import re
 from .common import InfoExtractor
 from ..compat import (
     compat_str,
-    compat_urllib_parse_urlencode,
+    compat_urllib_parse,
 )
 from ..utils import (
     ExtractorError,
@@ -138,11 +138,12 @@ class SohuIE(InfoExtractor):
                 clips_url = data['clipsURL']
                 su = data['su']
 
-                video_url = 'newflv.sohu.ccgslb.net'
+                video_url = 'http://newflv.sohu.ccgslb.net'
+                parsed_video_url = compat_urllib_parse.urlparse(video_url)
                 cdnId = None
                 retries = 0
 
-                while 'newflv.sohu.ccgslb.net' in video_url:
+                while 'newflv.sohu.ccgslb.net' in parsed_video_url.hostname:
                     params = {
                         'prot': 9,
                         'file': clips_url[i],
@@ -160,10 +161,11 @@ class SohuIE(InfoExtractor):
                     if retries > 0:
                         download_note += ' (retry #%d)' % retries
                     part_info = self._parse_json(self._download_webpage(
-                        'http://%s/?%s' % (allot, compat_urllib_parse_urlencode(params)),
+                        'http://%s/?%s' % (allot, compat_urllib_parse.urlencode(params)),
                         video_id, download_note), video_id)
 
                     video_url = part_info['url']
+                    parsed_video_url = compat_urllib_parse.urlparse(video_url)
                     cdnId = part_info.get('nid')
 
                     retries += 1
diff --git a/youtube_dl/extractor/southpark.py b/youtube_dl/extractor/southpark.py
index 0774da06e..9aedaa04a 100644
--- a/youtube_dl/extractor/southpark.py
+++ b/youtube_dl/extractor/southpark.py
@@ -56,40 +56,26 @@ class SouthParkEsIE(SouthParkIE):
 
 class SouthParkDeIE(SouthParkIE):
     IE_NAME = 'southpark.de'
-    _VALID_URL = r'https?://(?:www\.)?(?P<url>southpark\.de/(?:clips|alle-episoden|collections)/(?P<id>.+?)(\?|#|$))'
-    _FEED_URL = 'http://www.southpark.de/feeds/video-player/mrss/'
+    _VALID_URL = r'https?://(?:www\.)?(?P<url>southpark\.de/(?:(en/(videoclip|collections|episodes))|(videoclip|collections|folgen))/(?P<id>(?P<unique_id>.+?)/.+?)(?:\?|#|$))'
+    # _FEED_URL = 'http://feeds.mtvnservices.com/od/feed/intl-mrss-player-feed'
 
     _TESTS = [{
-        'url': 'http://www.southpark.de/clips/uygssh/the-government-wont-respect-my-privacy#tab=featured',
-        'info_dict': {
-            'id': '85487c96-b3b9-4e39-9127-ad88583d9bf2',
-            'ext': 'mp4',
-            'title': 'South Park|The Government Won\'t Respect My Privacy',
-            'description': 'Cartman explains the benefits of "Shitter" to Stan, Kyle and Craig.',
-            'timestamp': 1380160800,
-            'upload_date': '20130926',
-        },
-    }, {
-        # non-ASCII characters in initial URL
-        'url': 'http://www.southpark.de/alle-episoden/s18e09-hashtag-aufw√§rmen',
-        'info_dict': {
-            'title': 'Hashtag ‚ÄûAufw√§rmen‚Äú',
-            'description': 'Kyle will mit seinem kleinen Bruder Ike Videospiele spielen. Als der nicht mehr mit ihm spielen will, hat Kyle Angst, dass er die Kids von heute nicht mehr versteht.',
-        },
-        'playlist_count': 3,
+        'url': 'https://www.southpark.de/videoclip/rsribv/south-park-rueckzug-zum-gummibonbon-wald',
+        'only_matching': True,
     }, {
-        # non-ASCII characters in redirect URL
-        'url': 'http://www.southpark.de/alle-episoden/s18e09',
-        'info_dict': {
-            'title': 'Hashtag ‚ÄûAufw√§rmen‚Äú',
-            'description': 'Kyle will mit seinem kleinen Bruder Ike Videospiele spielen. Als der nicht mehr mit ihm spielen will, hat Kyle Angst, dass er die Kids von heute nicht mehr versteht.',
-        },
-        'playlist_count': 3,
+        'url': 'https://www.southpark.de/folgen/jiru42/south-park-verkabelung-staffel-23-ep-9',
+        'only_matching': True,
     }, {
-        'url': 'http://www.southpark.de/collections/2476/superhero-showdown/1',
+        'url': 'https://www.southpark.de/collections/zzno5a/south-park-good-eats/7q26gp',
         'only_matching': True,
     }]
 
+    def _get_feed_url(self, uri, url=None):
+        video_id = self._id_from_uri(uri)
+        config = self._download_json(
+            'http://media.mtvnservices.com/pmt/e1/access/index.html?uri=%s&configtype=edge&ref=%s' % (uri, url), video_id)
+        return self._remove_template_parameter(config['feedWithQueryParams'])
+
 
 class SouthParkNlIE(SouthParkIE):
     IE_NAME = 'southpark.nl'
diff --git a/youtube_dl/extractor/tktube.py b/youtube_dl/extractor/tktube.py
new file mode 100644
index 000000000..e338c683e
--- /dev/null
+++ b/youtube_dl/extractor/tktube.py
@@ -0,0 +1,72 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+import time
+
+from .common import InfoExtractor
+from ..utils import determine_ext, int_or_none
+
+
+class TktubeIE(InfoExtractor):
+    IE_NAME = 'tktube'
+    _VALID_URL = r'https?://(?:www\.)?tktube\.com/videos/(?P<id>\d+/[^/?#&]+)'
+    _WORKING = False
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        url = 'https://www.tktube.com/videos/%s/' % video_id
+        webpage = self._download_webpage(url, video_id)
+
+        title = self._search_regex(
+            (r'(?m)<div class="headline">\s*<h1>(.+?)</h1>\s*</div>',
+             r'<title>(.+?)</title>',
+             r'<meta property="og:title" content="(.+?)"',), webpage, 'title')
+        description = self._og_search_description(webpage)
+
+        mobj = re.search(r'<a href="https://www\.tktube\.com/members/(\d+)/">\s*(\S+?)\s*</a>', webpage)
+        if mobj:
+            uploader_id, uploader = mobj.groups()
+        else:
+            self.report_warning('Failed to extract uploader info')
+            uploader_id, uploader = None, None
+
+        # self._download_webpage('https://www.tktube.com/player/kt_player.js?v=5.2.0', video_id)
+        # self._download_webpage('https://www.tktube.com/static/js/main.min.js?v=7.2', video_id)
+
+        data_dict = {g.group(1): g.group(2) for g in re.finditer(r"(\S+?):\s*'(.+?)'", webpage)}
+        formats = []
+        for k, v in data_dict.items():
+            if k.startswith('video_'):
+                if '.mp4' not in v:
+                    continue
+                # remove first 'function/0/' and add some params
+                video_url = '%s?rnd=%d' % (v[11:], int(time.time() * 1000))
+                format_id = data_dict['%s_text' % k]
+                width, height = None, int_or_none(format_id[:-1])
+                if height:
+                    width = height // 16 * 9
+                formats.append({
+                    'format_id': format_id,
+                    'url': video_url,
+                    'protocol': 'http',
+                    'ext': determine_ext(video_url),
+                    'width': width,
+                    'height': height,
+                    'http_headers': {
+                        'Referer': url,
+                        'Accept': '*/*',
+                        'Accept-Encoding': 'identity;q=1, *;q=0',
+                    },
+                })
+
+        self._sort_formats(formats)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'uploader': uploader,
+            'uploader_id': uploader_id,
+            'description': description,
+            'formats': formats,
+        }
diff --git a/youtube_dl/extractor/tokyomotion.py b/youtube_dl/extractor/tokyomotion.py
new file mode 100644
index 000000000..949196833
--- /dev/null
+++ b/youtube_dl/extractor/tokyomotion.py
@@ -0,0 +1,151 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import (
+    sanitized_Request,
+    ExtractorError,
+    try_get,
+)
+from ..compat import (
+    compat_str,
+)
+
+try:
+    from urllib import quote
+except ImportError:
+    from urllib.parse import quote
+
+
+class TokyoMotionBaseIE(InfoExtractor):
+    def _download_page(self, url, video_id, note=None):
+        # This fails
+        # return self._download_webpage(url, video_id)
+        # Use ones in generic extractor
+        request = sanitized_Request(url)
+        request.add_header('Accept-Encoding', '*')
+        full_response = self._request_webpage(request, video_id, note=note)
+        return self._webpage_read_content(full_response, url, video_id)
+
+    @staticmethod
+    def _int_id(url):
+        m = TokyoMotionIE._valid_url_re().match(url)
+        return int(compat_str(m.group('id')))
+
+    @staticmethod
+    def _extract_video_urls(variant, webpage):
+        return ('https://www.%smotion.net%s' % (variant, quote(frg.group()))
+                for frg in re.finditer(r'/video/(?P<id>\d+)/[^#?&"\']+', webpage))
+
+    def _do_paging(self, variant, user_id):
+        index = 1
+        all_matches = []
+        while True:
+            newurl = self.USER_VIDEOS_FULL_URL % (variant, user_id, index)
+            webpage = self._download_page(newurl, user_id, note='Downloading page %d' % index)
+            all_matches.extend(self._extract_video_urls(variant, webpage))
+            index = index + 1
+            if ('videos?page=%d"' % index) not in webpage and ('&page=%d"' % index) not in webpage:
+                break
+        return (self.url_result(url) for url in sorted(set(all_matches), key=self._int_id))
+
+
+class TokyoMotionPlaylistBaseIE(TokyoMotionBaseIE):
+    def _real_extract(self, url):
+        user_id = self._match_id(url)
+        variant = self._VALID_URL_RE.match(url).group('variant')
+        matches = self._do_paging(variant, user_id)
+        return self.playlist_result(matches, user_id, self.TITLE % user_id)
+
+
+class TokyoMotionIE(TokyoMotionBaseIE):
+    IE_NAME = 'tokyomotion'
+    _VALID_URL = r'https?://(?:www\.)?(?P<variant>tokyo|osaka)motion\.net/video/(?P<id>\d+)(?P<excess>/.+)?'
+    _TEST = {
+        'url': 'https://www.tokyomotion.net/video/915034/%E9%80%86%E3%81%95',
+        'info_dict': {
+            'id': '915034',
+            'ext': 'mp4',
+            'title': 'ÈÄÜ„Åï',
+        }
+    }
+
+    def _real_extract(self, url):
+        url = url.split('?')[0].split('#')[0]  # sanitize URL
+
+        mobj = self._valid_url_re().match(url)
+        assert mobj
+        video_id = mobj.group('id')
+        variant = mobj.group('variant')
+        if not mobj.group('excess'):
+            # fix URL silently
+            url = url.split('#')[0]
+            if not url.endswith('/'):
+                url += '/'
+            url += 'a'
+
+        webpage = self._download_page(url, video_id)
+
+        title = self._og_search_title(webpage, default=None)
+
+        entry = try_get(
+            webpage,
+            lambda x: self._parse_html5_media_entries(url, x, video_id, m3u8_id='hls')[0],
+            dict)
+        if not entry:
+            raise ExtractorError('Private video', expected=True)
+
+        for fmt in entry['formats']:
+            fmt['external_downloader'] = 'ffmpeg'
+
+        self._sort_formats(entry['formats'], id_preference_dict={'hd': 1, 'sd': -1})
+        entry.update({
+            'id': video_id,
+            'title': title,
+            'age_limit': 18,
+            'series': 'TokyoMotion' if variant == 'tokyo' else 'OsakaMotion',
+        })
+        return entry
+
+
+class TokyoMotionUserIE(TokyoMotionPlaylistBaseIE):
+    IE_NAME = 'tokyomotion:user'
+    _VALID_URL = r'https?://(?:www\.)?(?P<variant>tokyo|osaka)motion\.net/user/(?P<id>[^/]+)(?:/videos)?'
+    _TEST = {}
+    USER_VIDEOS_FULL_URL = 'https://www.%smotion.net/user/%s/videos?page=%d'
+    TITLE = 'Uploads from %s'
+
+
+class TokyoMotionUserFavsIE(TokyoMotionPlaylistBaseIE):
+    IE_NAME = 'tokyomotion:user:favs'
+    _VALID_URL = r'https?://(?:www\.)?(?P<variant>tokyo|osaka)motion\.net/user/(?P<id>[^/]+)/favorite/videos'
+    _TEST = {}
+    USER_VIDEOS_FULL_URL = 'https://www.%smotion.net/user/%s/favorite/videos?page=%d'
+    TITLE = 'Favorites from %s'
+
+
+class TokyoMotionSearchesIE(TokyoMotionPlaylistBaseIE):
+    IE_NAME = 'tokyomotion:searches'
+    _VALID_URL = r'https?://(?:www\.)?(?P<variant>tokyo|osaka)motion\.net/search\?search_query=(?P<id>[^/&]+)(?:&search_type=videos)?(?:&page=\d+)?'
+    _TEST = {}
+    USER_VIDEOS_FULL_URL = 'https://www.%smotion.net/search?search_query=%s&search_type=videos&page=%d'
+    TITLE = 'Search results for %s'
+
+
+class TokyoMotionScannerIE(TokyoMotionBaseIE):
+    IE_DESC = False  # Do not list
+    IE_NAME = 'tokyomotion:scanner'
+    _VALID_URL = r'tmscan:https?://(?:www\.)?(?P<variant>tokyo|osaka)motion\.net/(?P<id>.*)'
+    _TEST = {}
+
+    def _real_extract(self, url):
+        mobj = self._valid_url_re().match(url)
+        assert mobj
+        user_id = mobj.group('id')
+        variant = mobj.group('variant')
+        webpage = self._download_page(url[7:], user_id)
+        matches = self._extract_video_urls(variant, webpage)
+        playlist = (self.url_result(url) for url in sorted(set(matches), key=self._int_id))
+        return self.playlist_result(playlist, user_id, 'Scanned results for %s' % url)
diff --git a/youtube_dl/extractor/twitcasting.py b/youtube_dl/extractor/twitcasting.py
index 6596eef9f..1f40adf56 100644
--- a/youtube_dl/extractor/twitcasting.py
+++ b/youtube_dl/extractor/twitcasting.py
@@ -1,10 +1,12 @@
 # coding: utf-8
 from __future__ import unicode_literals
 
+import itertools
 import re
 
 from .common import InfoExtractor
 from ..utils import (
+    ExtractorError,
     clean_html,
     float_or_none,
     get_element_by_class,
@@ -13,11 +15,18 @@ from ..utils import (
     str_to_int,
     unified_timestamp,
     urlencode_postdata,
+    try_get,
+    urljoin,
 )
+from ..compat import compat_str
 
 
-class TwitCastingIE(InfoExtractor):
-    _VALID_URL = r'https?://(?:[^/]+\.)?twitcasting\.tv/(?P<uploader_id>[^/]+)/movie/(?P<id>\d+)'
+class TwitCastingBaseIE(InfoExtractor):
+    pass
+
+
+class TwitCastingIE(TwitCastingBaseIE):
+    _VALID_URL = r'https?://(?:[^/]+\.)?twitcasting\.tv/(?P<uploader_id>[^/]+)/(?:movie|twplayer)/(?P<id>\d+)'
     _TESTS = [{
         'url': 'https://twitcasting.tv/ivetesangalo/movie/2357609',
         'md5': '745243cad58c4681dc752490f7540d7f',
@@ -65,25 +74,57 @@ class TwitCastingIE(InfoExtractor):
             request_data = urlencode_postdata({
                 'password': video_password,
             })
-        webpage = self._download_webpage(url, video_id, data=request_data)
-
-        title = clean_html(get_element_by_id(
-            'movietitle', webpage)) or self._html_search_meta(
-            ['og:title', 'twitter:title'], webpage, fatal=True)
-
-        video_js_data = {}
-        m3u8_url = self._search_regex(
-            r'data-movie-url=(["\'])(?P<url>(?:(?!\1).)+)\1',
-            webpage, 'm3u8 url', group='url', default=None)
-        if not m3u8_url:
-            video_js_data = self._parse_json(self._search_regex(
-                r"data-movie-playlist='(\[[^']+\])'",
-                webpage, 'movie playlist'), video_id)[0]
-            m3u8_url = video_js_data['source']['url']
-
-        # use `m3u8` entry_protocol until EXT-X-MAP is properly supported by `m3u8_native` entry_protocol
-        formats = self._extract_m3u8_formats(
-            m3u8_url, video_id, 'mp4', m3u8_id='hls')
+        webpage = self._download_webpage(
+            url, video_id, data=request_data,
+            headers={'Origin': 'https://twitcasting.tv'})
+
+        title = try_get(
+            webpage,
+            (lambda x: self._html_search_meta(['og:title', 'twitter:title'], x, fatal=False)),
+            compat_str)
+        if not title:
+            raise ExtractorError('Failed to extract title')
+
+        video_js_data = try_get(
+            webpage,
+            lambda x: self._parse_json(self._search_regex(
+                r"data-movie-playlist='([^']+?)'",
+                x, 'movie playlist', default=None), video_id)["2"][0], dict) or {}
+        is_live = 'data-status="online"' in webpage
+        m3u8_url = try_get(
+            webpage,
+            (lambda x: self._search_regex(
+                r'data-movie-url=(["\'])(?P<url>(?:(?!\1).)+)\1',
+                x, 'm3u8 url', group='url', default=None),
+             lambda x: video_js_data['source']['url'],
+             lambda x: 'https://twitcasting.tv/%s/metastream.m3u8' % uploader_id
+                if is_live else None),
+            compat_str)
+
+        if is_live:
+            # use `m3u8` entry_protocol until EXT-X-MAP is properly supported by `m3u8_native` entry_protocol
+            formats = self._extract_m3u8_formats(
+                m3u8_url, video_id, 'mp4', m3u8_id='hls',
+                headers={
+                    'Accept': '*/*',
+                    'Origin': 'https://twitcasting.tv',
+                    'Referer': 'https://twitcasting.tv/',
+                })
+            self._sort_formats(formats)
+        else:
+            # This reduces the download of m3u8 playlist (2 -> 1)
+            formats = [{
+                'url': m3u8_url,
+                'format_id': 'hls',
+                'ext': 'mp4',
+                'protocol': 'm3u8',
+                'http_headers': {
+                    'Accept': '*/*',
+                    'Origin': 'https://twitcasting.tv',
+                    'Referer': 'https://twitcasting.tv/',
+                },
+                'input_params': ['-re'],
+            }]
 
         thumbnail = video_js_data.get('thumbnailUrl') or self._og_search_thumbnail(webpage)
         description = clean_html(get_element_by_id(
@@ -104,8 +145,62 @@ class TwitCastingIE(InfoExtractor):
             'description': description,
             'thumbnail': thumbnail,
             'timestamp': timestamp,
+            'uploader': uploader_id,
             'uploader_id': uploader_id,
             'duration': duration,
             'view_count': view_count,
             'formats': formats,
+            'is_live': True,
         }
+
+
+class TwitCastingUserIE(TwitCastingBaseIE):
+    _VALID_URL = r'https?://(?:[^/]+\.)?twitcasting\.tv/(?P<id>[^/]+)(?:/([a-zA-Z-_][^/]*)?)*$'
+    _TESTS = [{
+        'url': 'https://twitcasting.tv/ivetesangalo',
+        'only_matching': True,
+    }, {
+        'url': 'https://twitcasting.tv/mttbernardini/',
+        'only_matching': True,
+    }, {
+        'url': 'https://twitcasting.tv/noriyukicas',
+        'only_matching': True,
+    }, {
+        'url': 'https://twitcasting.tv/lockedlesmi/',
+        'only_matching': True,
+    }]
+
+    def _real_extract(self, url):
+        uploader_id = self._match_id(url)
+        webpage = self._download_webpage(url, uploader_id, note='Looking for current live')
+
+        current_live = self._search_regex(
+            (r'data-type="movie" data-id="(\d+)">',
+             r'tw-sound-flag-open-link" data-id="(\d+)" style=',),
+            webpage, 'current live ID', default=None)
+        if current_live:
+            self._downloader.report_warning('Redirecting to current live on user %s' % uploader_id)
+            return self.url_result('https://twitcasting.tv/%s/movie/%s' % (uploader_id, current_live))
+
+        next_url = 'https://twitcasting.tv/%s/show/' % uploader_id
+        urls = []
+        for index in itertools.count(1):
+            webpage = self._download_webpage(next_url, uploader_id, note='Downloading page %d' % index)
+            for match in re.finditer(r'''(?isx)
+                <a\s+class="tw-movie-thumbnail"\s*href="(?P<url>/[^/]+/movie/\d+)"\s*>
+                .+?</a>
+            ''', webpage):
+                if not re.search(r'<span\s+class="tw-movie-thumbnail-badge"\s*data-status="recorded">', match.group(0)):
+                    continue
+                video_full_url = urljoin(url, match.group('url'))
+                urls.append(self.url_result(video_full_url))
+
+            next_url = self._search_regex(
+                r'<a href="(/%s/show/%d-\d+)">%d</a>' % (re.escape(uploader_id), index, index + 1),
+                webpage, 'next url', default=None)
+            if next_url:
+                next_url = urljoin(url, next_url)
+            else:
+                break
+
+        return self.playlist_result(urls, uploader_id, 'Live archive from %s' % uploader_id)
diff --git a/youtube_dl/extractor/twitter.py b/youtube_dl/extractor/twitter.py
index cfa7a7326..d8d62e28a 100644
--- a/youtube_dl/extractor/twitter.py
+++ b/youtube_dl/extractor/twitter.py
@@ -32,7 +32,9 @@ from .periscope import (
 class TwitterBaseIE(InfoExtractor):
     _API_BASE = 'https://api.twitter.com/1.1/'
     _BASE_REGEX = r'https?://(?:(?:www|m(?:obile)?)\.)?twitter\.com/'
-    _GUEST_TOKEN = None
+    _API_REQUEST_HEADERS = {
+        'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAPYXBAAAAAAACLXUNDekMxqa8h%2F40K4moUkGsoc%3DTYfbDKbT3jJPCEVnMYqilB28NHfOPqkca3qaAxGfsyKCs0wRbw',
+    }
 
     def _extract_variant_formats(self, variant, video_id):
         variant_url = variant.get('url')
@@ -80,18 +82,15 @@ class TwitterBaseIE(InfoExtractor):
             })
 
     def _call_api(self, path, video_id, query={}):
-        headers = {
-            'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAPYXBAAAAAAACLXUNDekMxqa8h%2F40K4moUkGsoc%3DTYfbDKbT3jJPCEVnMYqilB28NHfOPqkca3qaAxGfsyKCs0wRbw',
-        }
-        if not self._GUEST_TOKEN:
-            self._GUEST_TOKEN = self._download_json(
+        if not self._API_REQUEST_HEADERS.get('x-guest-token'):
+            self._API_REQUEST_HEADERS['x-guest-token'] = self._download_json(
                 self._API_BASE + 'guest/activate.json', video_id,
                 'Downloading guest token', data=b'',
-                headers=headers)['guest_token']
-        headers['x-guest-token'] = self._GUEST_TOKEN
+                headers=self._API_REQUEST_HEADERS)['guest_token']
         try:
             return self._download_json(
-                self._API_BASE + path, video_id, headers=headers, query=query)
+                self._API_BASE + path, video_id, headers=self._API_REQUEST_HEADERS,
+                query=query)
         except ExtractorError as e:
             if isinstance(e.cause, compat_HTTPError) and e.cause.code == 403:
                 raise ExtractorError(self._parse_json(
@@ -427,6 +426,10 @@ class TwitterIE(TwitterBaseIE):
         # poll4choice_video card
         'url': 'https://twitter.com/SouthamptonFC/status/1347577658079641604',
         'only_matching': True,
+    }, {
+        # summary_large_image card
+        'url': 'https://twitter.com/muramurasa/status/1351970239769001984',
+        'only_matching': True,
     }]
 
     def _real_extract(self, url):
@@ -529,7 +532,7 @@ class TwitterIE(TwitterBaseIE):
                         'url': get_binding_value('broadcast_url'),
                         'ie_key': TwitterBroadcastIE.ie_key(),
                     })
-                elif card_name == 'summary':
+                elif re.match(r'^summary(_.+)?$', card_name):
                     info.update({
                         '_type': 'url',
                         'url': get_binding_value('card_url'),
diff --git a/youtube_dl/extractor/veoh.py b/youtube_dl/extractor/veoh.py
index 1c44c145c..4ce26ea32 100644
--- a/youtube_dl/extractor/veoh.py
+++ b/youtube_dl/extractor/veoh.py
@@ -9,7 +9,7 @@ from ..utils import (
 
 
 class VeohIE(InfoExtractor):
-    _VALID_URL = r'https?://(?:www\.)?veoh\.com/(?:watch|embed|iphone/#_Watch)/(?P<id>(?:v|e|yapi-)[\da-zA-Z]+)'
+    _VALID_URL = r'https?://(?:www\.)?veoh\.com/(?:watch|embed|videos|iphone/#_Watch)/(?P<id>(?:v|e|yapi-)[\da-zA-Z]+)'
 
     _TESTS = [{
         'url': 'http://www.veoh.com/watch/v56314296nk7Zdmz3',
diff --git a/youtube_dl/extractor/videobin.py b/youtube_dl/extractor/videobin.py
new file mode 100644
index 000000000..a625f8b5e
--- /dev/null
+++ b/youtube_dl/extractor/videobin.py
@@ -0,0 +1,57 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+from .common import InfoExtractor
+from ..utils import (
+    ExtractorError,
+    determine_ext,
+)
+
+
+class VideobinBaseIE(InfoExtractor):
+    IE_DESC = False  # Do not list
+
+
+class VideobinIE(VideobinBaseIE):
+    IE_NAME = 'videobin'
+    _VALID_URL = r'https?:\/\/(?:www\.)?videobin\.co/(?P<id>[a-z0-9]{10,})'
+    _TEST = {}
+    _URL_REGEX = re.compile(r'(?ms)sources:\s*(\[\s*"[^"]+"\s*(?:,\s*"[^"]*"\s*)*])\s*,')
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        webpage = self._download_webpage(url, video_id)
+
+        title = 'Watch mp4'
+
+        entries = self._parse_json(self._URL_REGEX.search(webpage).group(1), video_id)
+        if not entries:
+            raise ExtractorError('Video is unavailable for some reasons', expected=True)
+        entry = {
+            'formats': []
+        }
+        for url in entries:
+            entry['formats'] += self._media_formats(url, 'video', video_id)
+        self._sort_formats(entry['formats'])
+        entry.update({
+            'id': video_id,
+            'title': title,
+        })
+        return entry
+
+    def _media_formats(self, full_url, cur_media_type, video_id):
+        ext = determine_ext(full_url)
+        if ext == 'm3u8':
+            formats = self._extract_m3u8_formats(
+                full_url, video_id, ext='mp4', fatal=False)
+        elif ext == 'mpd':
+            formats = self._extract_mpd_formats(
+                full_url, video_id, fatal=False)
+        else:
+            formats = [{
+                'url': full_url,
+                'vcodec': 'none' if cur_media_type == 'audio' else None,
+            }]
+        return formats
diff --git a/youtube_dl/extractor/vimeo.py b/youtube_dl/extractor/vimeo.py
index 102687b82..f85ab57da 100644
--- a/youtube_dl/extractor/vimeo.py
+++ b/youtube_dl/extractor/vimeo.py
@@ -621,7 +621,8 @@ class VimeoIE(VimeoBaseInfoExtractor):
             return self._extract_from_api(video_id, unlisted_hash)
 
         orig_url = url
-        is_pro = 'vimeopro.com/' in url
+        parsed_orig_url = compat_urlparse.urlparse(orig_url)
+        is_pro = 'vimeopro.com' in parsed_orig_url.hostname
         if is_pro:
             # some videos require portfolio_id to be present in player url
             # https://github.com/ytdl-org/youtube-dl/issues/20070
diff --git a/youtube_dl/extractor/voicy.py b/youtube_dl/extractor/voicy.py
new file mode 100644
index 000000000..118c82aa6
--- /dev/null
+++ b/youtube_dl/extractor/voicy.py
@@ -0,0 +1,144 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+from ..compat import compat_str
+from ..utils import (
+    ExtractorError,
+    smuggle_url,
+    try_get,
+    unsmuggle_url,
+)
+
+from datetime import datetime
+import itertools
+
+
+class VoicyBaseIE(InfoExtractor):
+    # every queries are assumed to be a playlist
+    def _extract_from_playlist_data(self, value):
+        voice_id = compat_str(value['PlaylistId'])
+        upload_date = datetime.strptime(value['Published'], "%Y-%m-%dT%H:%M:%SZ").strftime('%Y%m%d')
+        items = [self._extract_single_article(voice_id, voice_data, index) for index, voice_data in enumerate(value['VoiceData'], start=1)]
+        result = self.playlist_result(items)
+        result.update({
+            'id': voice_id,
+            'title': compat_str(value['PlaylistName']),
+            'uploader': value['SpeakerName'],
+            'uploader_id': compat_str(value['SpeakerId']),
+            'channel': value['ChannelName'],
+            'channel_id': compat_str(value['ChannelId']),
+            'upload_date': upload_date,
+        })
+        return result
+
+    # NOTE: "article" in voicy = "track" in CDs = "chapter" in DVDs
+    def _extract_single_article(self, voice_id, entry, index=None):
+        formats = self._extract_m3u8_formats(
+            entry['VoiceHlsFile'], voice_id, ext='m4a', entry_protocol='m3u8_native',
+            m3u8_id='hls', note=None if index is None else 'Downloading information for track %d' % index)
+        formats.append({
+            'url': entry['VoiceFile'],
+            'format_id': 'mp3',
+            'ext': 'mp3',
+            'vcodec': 'none',
+            'acodec': 'mp3',
+        })
+        self._sort_formats(formats)
+        return {
+            'id': compat_str(entry['ArticleId']),
+            'title': entry['ArticleTitle'],
+            'description': entry['MediaName'],
+            'voice_id': compat_str(entry['VoiceId']),
+            'chapter_id': compat_str(entry['ChapterId']),
+            'formats': formats,
+        }
+
+    def _call_api(self, url, video_id, **kwargs):
+        response = self._download_json(url, video_id, **kwargs)
+        if response['Status'] != 0:
+            message = try_get(
+                response,
+                (lambda x: x['Value']['Error']['Message'],
+                 lambda x: 'There was a error in the response: %d' % x['Status'],
+                 lambda x: 'There was a error in the response'),
+                compat_str)
+            raise ExtractorError(message, expected=False)
+        return response['Value']
+
+
+class VoicyIE(VoicyBaseIE):
+    IE_NAME = 'voicy'
+    _VALID_URL = r'https?://voicy\.jp/channel/(?P<channel_id>\d+)/(?P<id>\d+)'
+    ARTICLE_LIST_API_URL = 'https://vmw.api.voicy.jp/articles_list?channel_id=%s&pid=%s'
+    _TESTS = [{
+        'note': 'chomado wa iizo (iitowaittenai)',
+        'url': 'https://voicy.jp/channel/1253/122754',
+        'info_dict': {
+            'id': '122754',
+            'title': '1/21(Êú®)Â£∞Êó•Ë®òÔºö„Å§„ÅÑ„Å´ÂéüÁ®øÁµÇ„Çè„Å£„ÅüÔºÅÔºÅ',
+            'uploader': '„Å°„Çá„Åæ„Å©@ IT„Ç®„É≥„Ç∏„Éã„Ç¢„Å™„Ç™„Çø„ÇØ',
+            'uploader_id': '7339',
+        },
+        'playlist_mincount': 9,
+    }]
+
+    # every queries are assumed to be a playlist
+    def _real_extract(self, url):
+        voice_id = self._match_id(url)
+        channel_id = compat_str(self._VALID_URL_RE.match(url).group('channel_id'))
+        url, article_list = unsmuggle_url(url)
+        if not article_list:
+            article_list = self._call_api(self.ARTICLE_LIST_API_URL % (channel_id, voice_id), voice_id)
+        return self._extract_from_playlist_data(article_list)
+
+
+class VoicyChannelIE(VoicyBaseIE):
+    IE_NAME = 'voicy:channel'
+    _VALID_URL = r'https?://voicy\.jp/channel/(?P<id>\d+)'
+    PROGRAM_LIST_API_URL = 'https://vmw.api.voicy.jp/program_list/all?channel_id=%s&limit=20&public_type=3%s'
+    _TESTS = [{
+        'note': 'chomado wa iizo (iitowaittenai)',
+        'url': 'https://voicy.jp/channel/1253/',
+        'info_dict': {
+            'id': '7339',
+            'title': '„ÇÜ„Çã„Åµ„ÇèÊó•Â∏∏„É©„Ç∏„Ç™ #„Å°„Çá„Åæ„É©„Ç∏',
+            'uploader': '„Å°„Çá„Åæ„Å©@ IT„Ç®„É≥„Ç∏„Éã„Ç¢„Å™„Ç™„Çø„ÇØ',
+            'uploader_id': '7339',
+        },
+        'playlist_mincount': 54,
+    }]
+
+    @classmethod
+    def suitable(cls, url):
+        return not VoicyIE.suitable(url) and super(VoicyChannelIE, cls).suitable(url)
+
+    def _real_extract(self, url):
+        channel_id = self._match_id(url)
+        articles = []
+        pager = ''
+        for count in itertools.count(1):
+            article_list = self._call_api(self.PROGRAM_LIST_API_URL % (channel_id, pager), channel_id, note='Paging #%d' % count)
+            playlist_data = article_list['PlaylistData']
+            if not playlist_data:
+                break
+            articles.extend(playlist_data)
+            last = playlist_data[-1]
+            pager = '&pid=%d&p_date=%s&play_count=%s' % (last['PlaylistId'], last['Published'], last['PlayCount'])
+
+        title = try_get(
+            articles[0],
+            (lambda x: x['ChannelName'],
+             lambda x: 'Uploaded from ' % x['SpeakerName'],
+             lambda x: 'Channel ID: %s' % channel_id), compat_str)
+
+        urls = [smuggle_url('https://voicy.jp/channel/%s/%d' % (channel_id, value['PlaylistId']), value) for value in articles]
+        playlist = [self.url_result(url_, VoicyIE.ie_key()) for url_ in urls]
+        result = self.playlist_result(playlist)
+        result.update({
+            'id': channel_id,
+            'title': title,
+            'channel': channel_id,
+            'channel_id': channel_id,
+        })
+        return result
diff --git a/youtube_dl/extractor/weibo.py b/youtube_dl/extractor/weibo.py
index 621df5b54..5b2cc5484 100644
--- a/youtube_dl/extractor/weibo.py
+++ b/youtube_dl/extractor/weibo.py
@@ -10,6 +10,7 @@ import re
 from ..compat import (
     compat_parse_qs,
     compat_str,
+    compat_urllib_parse,
 )
 from ..utils import (
     js_to_json,
@@ -35,8 +36,9 @@ class WeiboIE(InfoExtractor):
         webpage, urlh = self._download_webpage_handle(url, video_id)
 
         visitor_url = urlh.geturl()
+        parsed_visitor_url = compat_urllib_parse.urlparse(visitor_url)
 
-        if 'passport.weibo.com' in visitor_url:
+        if parsed_visitor_url.hostname.startswith('passport.weibo.com'):
             # first visit
             visitor_data = self._download_json(
                 'https://passport.weibo.com/visitor/genvisitor', video_id,
diff --git a/youtube_dl/extractor/whowatch.py b/youtube_dl/extractor/whowatch.py
new file mode 100644
index 000000000..bec6fbbb6
--- /dev/null
+++ b/youtube_dl/extractor/whowatch.py
@@ -0,0 +1,101 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+from .common import InfoExtractor
+from ..utils import (
+    int_or_none,
+    qualities,
+    try_get,
+    ExtractorError,
+)
+from ..compat import compat_str
+
+
+class WhoWatchIE(InfoExtractor):
+    IE_NAME = 'whowatch'
+    _VALID_URL = r'https?://whowatch\.tv/viewer/(?P<id>\d+)'
+
+    _TESTS = [{
+        'url': 'https://whowatch.tv/viewer/21450171',
+        'only_matching': True,
+    }]
+
+    def _real_extract(self, url):
+        video_id = self._match_id(url)
+        self._download_webpage(url, video_id)
+        metadata = self._download_json('https://api.whowatch.tv/lives/%s' % video_id, video_id)
+        live_data = self._download_json('https://api.whowatch.tv/lives/%s/play' % video_id, video_id)
+
+        title = try_get(None, (
+            lambda x: live_data['share_info']['live_title'][1:-1],
+            lambda x: metadata['live']['title'],
+        ), compat_str)
+
+        hls_url = live_data.get('hls_url')
+        if not hls_url:
+            raise ExtractorError(live_data.get('error_message') or 'The user is offline.', expected=True)
+
+        QUALITIES = qualities(['low', 'medium', 'high', 'veryhigh'])
+        formats = []
+
+        for i, fmt in enumerate(live_data.get('streams') or []):
+            name = fmt.get('quality') or fmt.get('name') or compat_str(i)
+            hls_url = fmt.get('hls_url')
+            rtmp_url = fmt.get('rtmp_url')
+            audio_only = fmt.get('audio_only')
+            quality = QUALITIES(fmt.get('quality')) if not audio_only else -1
+
+            if hls_url:
+                hls_fmts = self._extract_m3u8_formats(
+                    hls_url, video_id, ext='mp4', entry_protocol='m3u8',
+                    m3u8_id='hls-%s' % name, quality=quality)
+                formats.extend(hls_fmts)
+            else:
+                hls_fmts = []
+
+            # RTMP url for audio_only is same as high format, so skip it
+            if rtmp_url and not audio_only:
+                formats.append({
+                    'url': rtmp_url,
+                    'format_id': 'rtmp-%s' % name,
+                    'ext': 'mp4',
+                    'protocol': 'ffmpeg',  # ffmpeg can, while rtmpdump can't
+                    'vcodec': 'h264',
+                    'acodec': 'aac',
+                    'quality': quality,
+                    'format_note': fmt.get('label'),
+                    # note: HLS and RTMP have same resolution for now, so it's acceptable
+                    'width': try_get(hls_fmts, lambda x: x[0]['width'], int),
+                    'height': try_get(hls_fmts, lambda x: x[0]['height'], int),
+                })
+
+        # This contains the same formats as the above manifests and is used only as a fallback
+        formats.extend(self._extract_m3u8_formats(
+            hls_url, video_id, ext='mp4', entry_protocol='m3u8',
+            m3u8_id='hls'))
+        self._remove_duplicate_formats(formats)
+        self._sort_formats(formats)
+
+        uploader_url = try_get(metadata, lambda x: x['live']['user']['user_path'], compat_str)
+        if uploader_url:
+            uploader_url = 'https://whowatch.tv/profile/%s' % uploader_url
+        uploader_id = compat_str(try_get(metadata, lambda x: x['live']['user']['id'], int))
+        uploader = try_get(metadata, lambda x: x['live']['user']['name'], compat_str)
+        thumbnail = try_get(metadata, lambda x: x['live']['latest_thumbnail_url'], compat_str)
+        timestamp = int_or_none(try_get(metadata, lambda x: x['live']['started_at'], int), scale=1000)
+        view_count = try_get(metadata, lambda x: x['live']['total_view_count'], int)
+        comment_count = try_get(metadata, lambda x: x['live']['comment_count'], int)
+
+        return {
+            'id': video_id,
+            'title': title,
+            'uploader_id': uploader_id,
+            'uploader_url': uploader_url,
+            'uploader': uploader,
+            'formats': formats,
+            'thumbnail': thumbnail,
+            'timestamp': timestamp,
+            'view_count': view_count,
+            'comment_count': comment_count,
+            'is_live': True,
+        }
diff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py
index 0c52e5a8b..7f455c8c9 100644
--- a/youtube_dl/extractor/youtube.py
+++ b/youtube_dl/extractor/youtube.py
@@ -43,6 +43,7 @@ from ..utils import (
     url_or_none,
     urlencode_postdata,
     urljoin,
+    uppercase_escape,
 )
 
 
@@ -281,7 +282,7 @@ class YoutubeBaseInfoExtractor(InfoExtractor):
 
     _YT_INITIAL_DATA_RE = r'(?:window\s*\[\s*["\']ytInitialData["\']\s*\]|ytInitialData)\s*=\s*({.+?})\s*;'
     _YT_INITIAL_PLAYER_RESPONSE_RE = r'ytInitialPlayerResponse\s*=\s*({.+?})\s*;'
-    _YT_INITIAL_BOUNDARY_RE = r'(?:var\s+meta|</script|\n)'
+    _YT_INITIAL_BOUNDARY_RE = r'(?:var\s+meta|</script|if\s*\(window\.ytcsi\)|\n)'
 
     def _call_api(self, ep, query, video_id, fatal=True):
         data = self._DEFAULT_API_DATA.copy()
@@ -301,6 +302,41 @@ class YoutubeBaseInfoExtractor(InfoExtractor):
                  self._YT_INITIAL_DATA_RE), webpage, 'yt initial data'),
             video_id)
 
+    def _extract_yt_initial_variable(self, webpage, video_id, name):
+        patterns = (
+            r';ytplayer\.config\s*=\s*({.+?});ytplayer',
+            r';ytplayer\.config\s*=\s*({.+?});',
+        )
+        config = self._search_regex(
+            patterns, webpage, 'ytplayer.config', default=None)
+        if config:
+            config = self._parse_json(self._parse_json(
+                uppercase_escape(config), name, fatal=False)['args']['player_config'],
+                name, fatal=False)
+        if config:
+            return config
+        # below is to extract error reason
+        patterns = (
+            r'(?m)window\["ytInitialPlayerResponse"\]\s*=\s*({.+});$',
+            r'%s\s*%s' % (self._YT_INITIAL_PLAYER_RESPONSE_RE, self._YT_INITIAL_BOUNDARY_RE),
+            self._YT_INITIAL_PLAYER_RESPONSE_RE,
+        )
+        config = self._search_regex(
+            patterns, webpage, 'initial player response', default=None)
+        if config:
+            config = self._parse_json(config, name, fatal=False)
+        if config:
+            return config
+        embedded_config = self._search_regex(
+            r'setConfig\(({.*})\);',
+            webpage, 'ytInitialData', default=None)
+        if embedded_config:
+            return try_get(
+                embedded_config,
+                lambda x: self._parse_json(self._parse_json(x, name)['PLAYER_VARS']['embedded_player_response'], name),
+                dict
+            )
+
     def _extract_ytcfg(self, video_id, webpage):
         return self._parse_json(
             self._search_regex(
@@ -414,6 +450,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
                             youtu\.be|                                        # just youtu.be/xxxx
                             vid\.plus|                                        # or vid.plus/xxxx
                             zwearz\.com/watch|                                # or zwearz.com/watch/xxxx
+                            i\.ytimg\.com/vi|                                 # or i.ytimg.com/vi/xxx
+                            y2u\.be|                                          # y2u.be
                             %(invidious)s
                          )/
                          |(?:www\.)?cleanvideosearch\.com/media/action/yt/watch\?videoId=
@@ -1236,10 +1274,9 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
         for player_re in cls._PLAYER_INFO_RE:
             id_m = re.search(player_re, player_url)
             if id_m:
-                break
+                return id_m.group('id')
         else:
             raise ExtractorError('Cannot identify player %r' % player_url)
-        return id_m.group('id')
 
     def _extract_signature_function(self, video_id, player_url, example_sig):
         player_id = self._extract_player_info(player_url)
@@ -1277,8 +1314,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
                 return 's[%s%s%s]' % (starts, ends, steps)
 
             step = None
-            # Quelch pyflakes warnings - start will be set when step is set
-            start = '(Never used)'
+            # Quelch Pylance warnings - start will be set when step is set
+            start, i = '(Never used)', None
             for i, prev in zip(idxs[1:], idxs[:-1]):
                 if step is not None:
                     if i - prev == step:
@@ -1468,11 +1505,6 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
             })
         return chapters
 
-    def _extract_yt_initial_variable(self, webpage, regex, video_id, name):
-        return self._parse_json(self._search_regex(
-            (r'%s\s*%s' % (regex, self._YT_INITIAL_BOUNDARY_RE),
-             regex), webpage, name, default='{}'), video_id, fatal=False)
-
     def _real_extract(self, url):
         url, smuggled_data = unsmuggle_url(url, {})
         video_id = self._match_id(url)
@@ -1484,14 +1516,13 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
         player_response = None
         if webpage:
             player_response = self._extract_yt_initial_variable(
-                webpage, self._YT_INITIAL_PLAYER_RESPONSE_RE,
-                video_id, 'initial player response')
+                webpage, video_id, 'initial player response')
         if not player_response:
             player_response = self._call_api(
                 'player', {'videoId': video_id}, video_id)
 
         playability_status = player_response.get('playabilityStatus') or {}
-        if playability_status.get('reason') == 'Sign in to confirm your age':
+        if playability_status.get('reason') in ('Sign in to confirm your age', 'This video may be inappropriate for some users.'):
             pr = self._parse_json(try_get(compat_parse_qs(
                 self._download_webpage(
                     base_url + 'get_video_info', video_id,
@@ -1499,6 +1530,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
                     'unable to download video info webpage', query={
                         'video_id': video_id,
                         'eurl': 'https://youtube.googleapis.com/v/' + video_id,
+                        'html5': '1',
                     }, fatal=False)),
                 lambda x: x['player_response'][0],
                 compat_str) or '{}', video_id)
@@ -1759,7 +1791,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
             video_details.get('lengthSeconds')
             or microformat.get('lengthSeconds')) \
             or parse_duration(search_meta('duration'))
-        is_live = video_details.get('isLive')
+        is_live = bool(video_details.get('isLive'))
         owner_profile_url = microformat.get('ownerProfileUrl')
 
         info = {
@@ -1790,6 +1822,8 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
             'categories': [category] if category else None,
             'tags': keywords,
             'is_live': is_live,
+            'subtitles': {},
+            'automatic_captions': {},
         }
 
         pctr = try_get(
@@ -1808,7 +1842,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
                     })
                 container[lang_code] = lang_subs
 
-            subtitles = {}
+            subtitles = info['subtitles']
             for caption_track in (pctr.get('captionTracks') or []):
                 base_url = caption_track.get('baseUrl')
                 if not base_url:
@@ -1820,7 +1854,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
                     process_language(
                         subtitles, base_url, lang_code, {})
                     continue
-                automatic_captions = {}
+                automatic_captions = info['automatic_captions']
                 for translation_language in (pctr.get('translationLanguages') or []):
                     translation_language_code = translation_language.get('languageCode')
                     if not translation_language_code:
@@ -1828,8 +1862,6 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
                     process_language(
                         automatic_captions, base_url, translation_language_code,
                         {'tlang': translation_language_code})
-                info['automatic_captions'] = automatic_captions
-            info['subtitles'] = subtitles
 
         parsed_url = compat_urllib_parse_urlparse(url)
         for component in [parsed_url.fragment, parsed_url.query]:
@@ -1860,12 +1892,24 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
         initial_data = None
         if webpage:
             initial_data = self._extract_yt_initial_variable(
-                webpage, self._YT_INITIAL_DATA_RE, video_id,
-                'yt initial data')
+                webpage, video_id, 'yt initial data')
         if not initial_data:
             initial_data = self._call_api(
                 'next', {'videoId': video_id}, video_id, fatal=False)
 
+        if not is_live and initial_data:
+            try:
+                # This will error if there is no livechat
+                # initial_data['contents']['twoColumnWatchNextResults']['conversationBar']['liveChatRenderer']['continuations'][0]['reloadContinuationData']['continuation']
+                info['subtitles']['live_chat'] = [{
+                    'url': 'https://www.youtube.com/watch?v=%s' % video_id,  # url is needed to set cookies
+                    'video_id': video_id,
+                    'ext': 'json',
+                    'protocol': 'youtube_live_chat_replay',
+                }]
+            except (KeyError, IndexError, TypeError):
+                pass
+
         if initial_data:
             chapters = self._extract_chapters_from_json(
                 initial_data, video_id, duration)
@@ -2607,6 +2651,8 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):
             'context': context,
         }
 
+        # note for merging: revert to "Incoming Change" once and add retry section again
+        # this is easier
         for page_num in itertools.count(1):
             if not continuation:
                 break
@@ -2616,9 +2662,13 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):
             data['clickTracking'] = {
                 'clickTrackingParams': continuation['itct']
             }
-            count = 0
-            retries = 3
-            while count <= retries:
+            retries = self._downloader.params.get('extractor_retries', 3)
+            count = -1
+            last_error = None
+            while count < retries:
+                count += 1
+                if last_error:
+                    self.report_warning('%s. Retrying...' % last_error)
                 try:
                     # Downloading page may result in intermittent 5xx HTTP error
                     # that is usually worked around with a retry
@@ -2628,11 +2678,15 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):
                         headers=headers, data=json.dumps(data).encode('utf8'))
                     break
                 except ExtractorError as e:
-                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503):
-                        count += 1
-                        if count <= retries:
+                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503, 404):
+                        # Downloading page may result in intermittent 5xx HTTP error
+                        # Sometimes a 404 is also recieved. See: https://github.com/ytdl-org/youtube-dl/issues/28289
+                        last_error = 'HTTP Error %s' % e.cause.code
+                        if count < retries:
                             continue
-                    raise
+                    last_error = e
+            if last_error:
+                raise last_error
             if not response:
                 break
 
@@ -2796,6 +2850,24 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):
             self._playlist_entries(playlist), playlist_id=playlist_id,
             playlist_title=title)
 
+    @staticmethod
+    def _extract_alerts(data):
+        for alert_dict in try_get(data, lambda x: x['alerts'], list) or []:
+            if not isinstance(alert_dict, dict):
+                continue
+            for renderer in alert_dict:
+                alert = alert_dict[renderer]
+                alert_type = alert.get('type')
+                if not alert_type:
+                    continue
+                message = try_get(alert, lambda x: x['text']['simpleText'], compat_str)
+                if message:
+                    yield alert_type, message
+                for run in try_get(alert, lambda x: x['text']['runs'], list) or []:
+                    message = try_get(run, lambda x: x['text'], compat_str)
+                    if message:
+                        yield alert_type, message
+
     def _extract_identity_token(self, ytcfg, webpage):
         if ytcfg:
             token = try_get(ytcfg, lambda x: x['ID_TOKEN'], compat_str)
@@ -2818,8 +2890,31 @@ class YoutubeTabIE(YoutubeBaseInfoExtractor):
                 self.to_screen('Downloading just video %s because of --no-playlist' % video_id)
                 return self.url_result(video_id, ie=YoutubeIE.ie_key(), video_id=video_id)
             self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))
-        webpage = self._download_webpage(url, item_id)
-        data = self._extract_yt_initial_data(item_id, webpage)
+
+        retries = self._downloader.params.get('extractor_retries', 3)
+        count = -1
+        while count < retries:
+            count += 1
+            # Sometimes youtube returns a webpage with incomplete ytInitialData
+            # See: https://github.com/yt-dlp/yt-dlp/issues/116
+            if count:
+                self.report_warning('Incomplete yt initial data recieved. Retrying...')
+            webpage = self._download_webpage(
+                url, item_id, note='Downloading webpage%s' % (' (retry #%d)' % count if count else ''))
+            data = self._extract_yt_initial_data(item_id, webpage)
+            err_msg = None
+            for alert_type, alert_message in self._extract_alerts(data):
+                if alert_type.lower() == 'error':
+                    if err_msg:
+                        self._downloader.report_warning('YouTube said: %s - %s' % ('ERROR', err_msg))
+                    err_msg = alert_message
+                else:
+                    self._downloader.report_warning('YouTube said: %s - %s' % (alert_type, alert_message))
+            if err_msg:
+                raise ExtractorError('YouTube said: %s' % err_msg, expected=True)
+            if data.get('contents') or data.get('currentVideoEndpoint'):
+                break
+
         tabs = try_get(
             data, lambda x: x['contents']['twoColumnBrowseResultsRenderer']['tabs'], list)
         if tabs:
diff --git a/youtube_dl/extractor/youtube_matrix.py b/youtube_dl/extractor/youtube_matrix.py
new file mode 100644
index 000000000..a9b6c56cf
--- /dev/null
+++ b/youtube_dl/extractor/youtube_matrix.py
@@ -0,0 +1,108 @@
+# coding: utf-8
+# NOTE: no longer used after 2021/05/02, but keep remained
+#   for future use and reference
+from __future__ import unicode_literals
+
+from ..compat import compat_str
+from ..utils import str_or_none
+
+# keep it sync with extractors.py
+from .youtube import (
+    YoutubeFavouritesIE,
+    YoutubeHistoryIE,
+    YoutubeTabIE,
+    YoutubePlaylistIE,
+    YoutubeRecommendedIE,
+    YoutubeSearchDateIE,
+    YoutubeSearchIE,
+    # YoutubeSearchURLIE,
+    YoutubeSubscriptionsIE,
+    YoutubeTruncatedIDIE,
+    YoutubeTruncatedURLIE,
+    YoutubeYtBeIE,
+    YoutubeYtUserIE,
+    YoutubeWatchLaterIE,
+)
+
+
+ytie = (
+    YoutubeFavouritesIE,
+    YoutubeHistoryIE,
+    YoutubeTabIE,
+    YoutubePlaylistIE,
+    YoutubeRecommendedIE,
+    YoutubeSearchDateIE,
+    YoutubeSearchIE,
+    YoutubeSubscriptionsIE,
+    YoutubeTruncatedIDIE,
+    YoutubeTruncatedURLIE,
+    YoutubeYtBeIE,
+    YoutubeYtUserIE,
+    YoutubeWatchLaterIE,
+)
+
+
+def _convert_result(ret, prefix):
+    if not isinstance(ret, dict):
+        return ret
+
+    type = str_or_none(ret.get('_type')) or ''
+    if type == 'playlist':
+        ret['entries'] = [_convert_result(x, prefix) for x in ret['entries']]
+    elif type.startswith('url'):
+        ret['url'] = prefix + ret['url']
+
+    if 'ie_key' in ret:
+        del ret['ie_key']
+
+    return ret
+
+
+def _convert_test_only_matching(test, prefix):
+    return {
+        'url': prefix + test['url'],
+        'only_matching': True,
+    }
+
+
+def ___real_extract(self, url):
+    url = self.remove_prefix(url)
+    try:  # Python 2.x
+        real_extract_func = self.BASE_IE._real_extract.__func__
+    except (TypeError, AttributeError):
+        real_extract_func = self.BASE_IE._real_extract
+    ret = real_extract_func(self, url)
+    return _convert_result(ret, self.PREFIXES[0])
+
+
+for base_ie in tuple():
+    for value in ytie:
+        key = value.__name__
+        obj = value()
+        clazz_name = str(base_ie.__name__[:-2] + key[7:])
+        clazz_dict = {
+            'BASE_IE': value,
+            '_real_extract': ___real_extract,
+        }
+
+        if hasattr(value, '_TEST') and isinstance(getattr(value, '_TEST'), dict):
+            clazz_dict['_TEST'] = _convert_test_only_matching(obj._TEST, base_ie.PREFIXES[0])
+        if hasattr(value, '_TESTS') and isinstance(getattr(value, '_TESTS'), list):
+            clazz_dict['_TESTS'] = [_convert_test_only_matching(x, base_ie.PREFIXES[0]) for x in obj._TESTS]
+
+        if hasattr(value, 'IE_NAME'):
+            ie_name = obj.IE_NAME
+            if not isinstance(value.IE_NAME, compat_str):
+                ie_name = '%s' % ie_name
+            if ie_name.startswith('youtube:'):
+                ie_name = base_ie.IE_NAME + ie_name[7:]
+            elif ie_name == key[:-2]:
+                ie_name = clazz_name[:-2]
+            else:
+                ie_name = base_ie.IE_NAME + ':' + ie_name
+            clazz_dict['IE_NAME'] = ie_name
+
+        if hasattr(value, '_VALID_URL'):
+            clazz_dict['_VALID_URL'] = value._VALID_URL
+
+        globals()[clazz_name] = type(clazz_name, (base_ie, value), clazz_dict)
diff --git a/youtube_dl/longname.py b/youtube_dl/longname.py
new file mode 100644
index 000000000..247d56914
--- /dev/null
+++ b/youtube_dl/longname.py
@@ -0,0 +1,283 @@
+# coding: utf-8
+from __future__ import unicode_literals
+
+import re
+
+try:
+    from os import PathLike, fsdecode
+    from typing import Union
+except ImportError:
+    PathLike, fsdecode = None, None
+
+from os import remove, rename, sep, stat, utime, unlink, makedirs
+from os.path import exists, isfile, getsize, normpath, join, basename, dirname
+from .compat import compat_str
+from .utils import (
+    sanitize_open,
+    get_filesystem_encoding,
+)
+
+# this file is to escape long file names in following manner:
+# 1. split each path segment in 255-N bytes (N=byte length of DEFAULT_DELIMITER below)
+# 2. prepend DEFAULT_DELIMITER, and then path segments are split within filesystem limit,
+#      with a marker on each split chunks
+
+FS_LENGTH_LIMIT = 255  # length limit from filesystem
+DEFAULT_DELIMITER = "~~"
+
+# http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/dc4322602480/src/share/classes/java/lang/Character.java
+# Constants from JDK
+MIN_HIGH_SURROGATE = 0xD800
+MAX_HIGH_SURROGATE = 0xDBFF
+MIN_LOW_SURROGATE = 0xDC00
+MAX_LOW_SURROGATE = 0xDFFF
+
+
+def split_longname(input, encoding=get_filesystem_encoding()):
+    # type: (Union[bytes, compat_str, PathLike], compat_str) -> bytes
+    if PathLike and isinstance(input, PathLike):
+        input = fsdecode(input)
+
+    was_bytes = isinstance(input, bytes)
+    if was_bytes:
+        input = input.decode(encoding)
+
+    result = split_longname_str(input, encoding)
+
+    if was_bytes:
+        result = result.encode(encoding)
+    return result
+
+
+def combine_longname(input, encoding=get_filesystem_encoding()):
+    # type: (Union[bytes, compat_str, PathLike], compat_str) -> bytes
+    if PathLike and isinstance(input, PathLike):
+        input = fsdecode(input)
+
+    was_bytes = isinstance(input, bytes)
+    if was_bytes:
+        input = input.decode(encoding)
+
+    result = combine_longname_str(input, encoding)
+
+    if was_bytes:
+        result = result.encode(encoding)
+    return result
+
+
+def split_longname_str(input, encoding=get_filesystem_encoding()):
+    # type: (compat_str, compat_str) -> compat_str
+    # https://docs.python.org/3/library/codecs.html
+    chunks = re.split(r'[\\/]', input)
+    result = []
+    if encoding in ('utf_8', 'U8', 'UTF', 'utf8', 'cp65001'):
+        # fast(er) path: UTF-8
+        CHUNK_LENGTH = FS_LENGTH_LIMIT - 2
+        for chunk in chunks:
+            if utf8_byte_length_all_chr(chunk) <= FS_LENGTH_LIMIT:
+                result.append(chunk)
+                continue
+            current_split, current_length = '', 0
+
+            for chr in chunk:
+                chrlen = utf8_byte_length(chr)
+                print(current_split)
+                if current_length + chrlen > CHUNK_LENGTH:
+                    if current_split:
+                        result.append(current_split + DEFAULT_DELIMITER)
+                    current_split, current_length = '', 0
+                current_split += chr
+                current_length += chrlen
+
+            if current_split:
+                result.append(current_split)
+    elif encoding in ('utf_16', 'U16', 'utf16', 'utf_16_be', 'UTF-16BE', 'utf_16_le', 'UTF-16LE'):
+        # fast path: UTF-16 ANY Endian
+        CHUNK_LENGTH = FS_LENGTH_LIMIT - 4
+        for chunk in chunks:
+            if len(chunk) * 2 <= FS_LENGTH_LIMIT:
+                result.append(chunk)
+                continue
+            current_split, current_length = '', 0
+
+            for chr in chunk:
+                chrord = ord(chr)
+                chrlen = 2
+                if chrord >= MIN_HIGH_SURROGATE and chrord < (MAX_HIGH_SURROGATE + 1):
+                    chrlen = 4
+                elif chrord >= MIN_LOW_SURROGATE and chrord < (MAX_LOW_SURROGATE + 1):
+                    chrlen = 0  # same reason as UTF-8 does
+
+                if current_length + chrlen > CHUNK_LENGTH:
+                    if current_split:
+                        result.append(current_split + DEFAULT_DELIMITER)
+                    current_split, current_length = '', 0
+                current_split += chr
+                current_length += chrlen
+
+            if current_split:
+                result.append(current_split)
+    elif encoding in ('utf_32', 'U32', 'utf32', 'utf_32_be', 'UTF-32BE', 'utf_32_le', 'UTF-32LE'):
+        # (very) fast path: UTF-32 ANY Endian
+        CHUNK_LENGTH = FS_LENGTH_LIMIT - 8
+        for chunk in chunks:
+            chunk_len = len(chunk)
+            if chunk_len * 4 <= FS_LENGTH_LIMIT:
+                result.append(chunk)
+                continue
+
+            for i in range(0, chunk_len, 4):
+                if chunk_len < i + 4:
+                    result.append(chunk[i:i + 4] + DEFAULT_DELIMITER)
+                else:
+                    result.append(chunk[i:i + 4])
+    else:
+        # slow path: encode each charaters
+        # any encoding with header/marking will break this (like UTF-16 with BOM, or 'idna')
+        CHUNK_LENGTH = FS_LENGTH_LIMIT - len(DEFAULT_DELIMITER.encode(encoding))
+        for chunk in chunks:
+            if len(chunk.encode(encoding)) <= FS_LENGTH_LIMIT:
+                result.append(chunk)
+                continue
+            current_split, current_length = '', 0
+
+            for chr in chunk:
+                chrlen = len(chr.encode(encoding))
+                if current_length + chrlen > CHUNK_LENGTH:
+                    if current_split:
+                        result.append(current_split + DEFAULT_DELIMITER)
+                    current_split, current_length = '', 0
+                current_split += chr
+                current_length += chrlen
+
+            if current_split:
+                result.append(current_split)
+
+    return sep.join(result)
+
+
+def combine_longname_str(input, encoding=get_filesystem_encoding()):
+    # type: (compat_str, compat_str) -> str
+    result = []
+    for part in re.split(r'[\\/]', input):
+        if result and result[-1].endswith(DEFAULT_DELIMITER):
+            result[-1] = result[-1][:-2] + part
+        else:
+            result.append(part)
+    return sep.join(result)
+
+
+def utf8_byte_length(chr):
+    "Calculates byte length in UTF-8 without encode/decode"
+    # type: (Union[compat_str, int]) -> int
+    if isinstance(chr, compat_str):
+        chr = ord(chr[0])
+
+    if chr <= 0x7F:
+        return 1
+    if chr <= 0x7FF:
+        return 2
+    # refer to Character.isHighSurrogate from Java
+    if chr >= MIN_HIGH_SURROGATE and chr < (MAX_HIGH_SURROGATE + 1):
+        return 4  # HIGH+LOW, low should be added later without cost
+    if chr >= MIN_LOW_SURROGATE and chr < (MAX_LOW_SURROGATE + 1):
+        return 0  # length for this is already accounted at high surrogate
+
+    return 3
+
+
+def utf8_byte_length_all_chr(string):
+    "Calculates byte length in UTF-8 without encode/decode"
+    # type: (compat_str) -> int
+    result = 0
+    for chr in string:
+        chr = ord(chr[0])
+        if chr <= 0x7F:
+            result += 1
+        elif chr <= 0x7FF:
+            result += 2
+        # refer to Character.isHighSurrogate from Java
+        elif chr >= MIN_HIGH_SURROGATE and chr < (MAX_HIGH_SURROGATE + 1):
+            result += 4  # HIGH+LOW, low should be added later without cost
+        elif chr >= MIN_LOW_SURROGATE and chr < (MAX_LOW_SURROGATE + 1):
+            result += 0  # length for this is already accounted at high surrogate
+        else:
+            result += 3
+    return result
+
+
+def ensure_directory(filename):
+    split = split_longname(filename, get_filesystem_encoding())
+    if split != filename:
+        try:
+            makedirs(normpath(join(split, '..')))
+        except FileExistsError:
+            pass
+    return split
+
+
+def escaped_open(filename, open_mode, **kwargs):
+    "open() that escapes long names"
+    split = ensure_directory(filename)
+    return open(split, open_mode, **kwargs)
+
+
+def escaped_sanitize_open(filename, open_mode):
+    "sanitized_open() that escapes long names"
+    split = ensure_directory(filename)
+    a, b = sanitize_open(split, open_mode)
+    b = combine_longname(b)
+    return a, b
+
+
+def escaped_stat(path, *args, **kwargs):
+    "os.stat() that escapes long names"
+    return stat(split_longname(path, get_filesystem_encoding()), *args, **kwargs)
+
+
+def escaped_unlink(path, *args, **kwargs):
+    "os.unlink() that escapes long names"
+    unlink(split_longname(path, get_filesystem_encoding()), *args, **kwargs)
+
+
+def escaped_path_isfile(path):
+    "os.path.isfile() that escapes long names"
+    return isfile(split_longname(path, get_filesystem_encoding()))
+
+
+def escaped_path_exists(path):
+    "os.path.exists() that escapes long names"
+    return exists(split_longname(path, get_filesystem_encoding()))
+
+
+def escaped_path_getsize(filename):
+    "os.path.getsize() that escapes long names"
+    return getsize(split_longname(filename, get_filesystem_encoding()))
+
+
+def escaped_utime(path, *args, **kwargs):
+    "os.utime() that escapes long names"
+    utime(split_longname(path, get_filesystem_encoding()), *args, **kwargs)
+
+
+def escaped_rename(src, dst, *args, **kwargs):
+    "os.rename() that escapes long names"
+    dst = ensure_directory(dst)
+    rename(
+        split_longname(src, get_filesystem_encoding()),
+        dst, *args, **kwargs)
+
+
+def escaped_remove(path, *args, **kwargs):
+    "os.remove() that escapes long names"
+    remove(split_longname(path, get_filesystem_encoding()), *args, **kwargs)
+
+
+def escaped_basename(path):
+    "os.path.basename() that escapes long names"
+    return basename(split_longname(path, get_filesystem_encoding()))
+
+
+def escaped_dirname(path):
+    "os.path.dirname() that escapes long names"
+    return dirname(split_longname(path, get_filesystem_encoding()))
diff --git a/youtube_dl/options.py b/youtube_dl/options.py
index 0a0641bd4..340b3e021 100644
--- a/youtube_dl/options.py
+++ b/youtube_dl/options.py
@@ -134,7 +134,7 @@ def parseOpts(overrideArguments=None):
         action='help',
         help='Print this help text and exit')
     general.add_option(
-        '--version',
+        '-V', '--version',
         action='version',
         help='Print program version and exit')
     general.add_option(
@@ -198,6 +198,38 @@ def parseOpts(overrideArguments=None):
         action='store_true', dest='no_color',
         default=False,
         help='Do not emit color codes in output')
+    general.add_option(
+        '--check-mastodon-instance',
+        action='store_true', dest='check_mastodon_instance',
+        default=False,
+        help='Always perform online checks for Mastodon-like URL')
+    general.add_option(
+        '--check-peertube-instance',
+        action='store_true', dest='check_peertube_instance',
+        default=False,
+        help='Always perform online checks for PeerTube-like URL')
+    general.add_option(
+        '--test-filename',
+        metavar='CMD', dest='test_filename',
+        help='Like --exec option, but used for testing if downloading should be started. '
+             'You can begin with "re:" to use regex instead of commands')
+    general.add_option(
+        '--print-infojson-types',
+        action='store_true', dest='printjsontypes',
+        default=False,
+        help='DO NOT USE. IT\'S MEANINGLESS FOR MOST PEOPLE. Prints types of object in info json. '
+             'Use this for extractors that --print-json won\' work.')
+    general.add_option(
+        '--enable-lock',
+        action='store_true', dest='lock_exclusive',
+        default=True,
+        help='Locks downloading exclusively. Blocks other ytdl-patched process downloading the same video.')
+    general.add_option(
+        '--no-lock',
+        action='store_false', dest='lock_exclusive',
+        default=True,
+        help='Do not lock downloading exclusively. '
+             'Download will start even if other process is working on it.')
 
     network = optparse.OptionGroup(parser, 'Network Options')
     network.add_option(
@@ -205,7 +237,7 @@ def parseOpts(overrideArguments=None):
         default=None, metavar='URL',
         help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable '
              'SOCKS proxy, specify a proper scheme. For example '
-             'socks5://127.0.0.1:1080/. Pass in an empty string (--proxy "") '
+             'socks5h://127.0.0.1:1080/. Pass in an empty string (--proxy "") '
              'for direct connection')
     network.add_option(
         '--socket-timeout',
@@ -254,6 +286,12 @@ def parseOpts(overrideArguments=None):
         dest='geo_bypass_ip_block', default=None,
         help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
 
+    extractor = optparse.OptionGroup(parser, 'Extractor Options')
+    extractor.add_option(
+        '--extractor-retries',
+        dest='extractor_retries', metavar='RETRIES', default=3,
+        help='Number of retries for known extractor errors (default is 3), or "infinite"')
+
     selection = optparse.OptionGroup(parser, 'Video Selection')
     selection.add_option(
         '--playlist-start',
@@ -344,6 +382,10 @@ def parseOpts(overrideArguments=None):
         '--download-archive', metavar='FILE',
         dest='download_archive',
         help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
+    selection.add_option(
+        '--failed-archive', metavar='FILE',
+        dest='failed_archive',
+        help='Record the URLs or IDs of all downloading-failed videos in it.')
     selection.add_option(
         '--include-ads',
         dest='include_ads', action='store_true',
@@ -402,6 +444,10 @@ def parseOpts(overrideArguments=None):
         '--prefer-free-formats',
         action='store_true', dest='prefer_free_formats', default=False,
         help='Prefer free video formats unless a specific one is requested')
+    video_format.add_option(
+        '--prefer-smaller-formats',
+        action='store_true', dest='prefer_smaller_formats', default=False,
+        help='Prefer smaller video formats unless a specific one is requested')
     video_format.add_option(
         '-F', '--list-formats',
         action='store_true', dest='listformats',
@@ -421,6 +467,12 @@ def parseOpts(overrideArguments=None):
             'If a merge is required (e.g. bestvideo+bestaudio), '
             'output to given container format. One of mkv, mp4, ogg, webm, flv. '
             'Ignored if no merge is required'))
+    video_format.add_option(
+        '--live-download-mkv',
+        action='store_true', dest='live_download_mkv', default=False,
+        help=(
+            'Changes video file format to MKV when downloading a live. '
+            'This is useful if the computer could shutdown while downloading.'))
 
     subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
     subtitles.add_option(
@@ -572,6 +624,27 @@ def parseOpts(overrideArguments=None):
             'Upper bound of a range for randomized sleep before each download '
             '(maximum possible number of seconds to sleep). Must only be used '
             'along with --min-sleep-interval.'))
+    workarounds.add_option(
+        '--sleep-before-extract', '--min-sleep-before-extract', metavar='SECONDS',
+        dest='sleep_before_extract', type=float,
+        help=(
+            'Number of seconds to sleep before each extraction when used alone '
+            'or a lower bound of a range for randomized sleep before each extraction '
+            '(minimum possible number of seconds to sleep) when used along with '
+            '--max-sleep-before-extract.'))
+    workarounds.add_option(
+        '--max-sleep-before-extract', metavar='SECONDS',
+        dest='max_sleep_before_extract', type=float,
+        help=(
+            'Upper bound of a range for randomized sleep before each extraction '
+            '(maximum possible number of seconds to sleep). Must only be used '
+            'along with --min-sleep-before-extract.'))
+    workarounds.add_option(
+        '--escape-long-names',
+        action='store_true', dest='escape_long_names', default=False,
+        help=(
+            'Split filename longer than 255 bytes into few path segments. '
+            'This may create dumb directories.'))
 
     verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
     verbosity.add_option(
@@ -767,6 +840,10 @@ def parseOpts(overrideArguments=None):
         '--rm-cache-dir',
         action='store_true', dest='rm_cachedir',
         help='Delete all filesystem cache files')
+    filesystem.add_option(
+        '--rm-long-name-dir',
+        action='store_true', dest='rm_longnamedir',
+        help='Deletes all filename-splitting-related empty directories in working directory')
 
     thumbnail = optparse.OptionGroup(parser, 'Thumbnail Options')
     thumbnail.add_option(
@@ -862,11 +939,50 @@ def parseOpts(overrideArguments=None):
         '--convert-subs', '--convert-subtitles',
         metavar='FORMAT', dest='convertsubtitles', default=None,
         help='Convert the subtitles to other format (currently supported: srt|ass|vtt|lrc)')
+    postproc.add_option(
+        '--convert-thumbnails',
+        metavar='FORMAT', dest='convertthumbnails', default=None,
+        help='Convert the thumbnails to another format (currently supported: jpg)')
+
+    sponskrub = optparse.OptionGroup(parser, 'SponSkrub (SponsorBlock) Options', description=(
+        'SponSkrub (https://github.com/yt-dlp/SponSkrub) is a utility to mark/remove sponsor segments '
+        'from downloaded YouTube videos using SponsorBlock API (https://sponsor.ajay.app)'))
+    sponskrub.add_option(
+        '--sponskrub',
+        action='store_true', dest='sponskrub', default=None,
+        help=(
+            'Use sponskrub to mark sponsored sections. '
+            'This is enabled by default if the sponskrub binary exists (Youtube only)'))
+    sponskrub.add_option(
+        '--no-sponskrub',
+        action='store_false', dest='sponskrub',
+        help='Do not use sponskrub')
+    sponskrub.add_option(
+        '--sponskrub-cut', default=False,
+        action='store_true', dest='sponskrub_cut',
+        help='Cut out the sponsor sections instead of simply marking them')
+    sponskrub.add_option(
+        '--no-sponskrub-cut',
+        action='store_false', dest='sponskrub_cut',
+        help='Simply mark the sponsor sections, not cut them out (default)')
+    sponskrub.add_option(
+        '--sponskrub-force', default=False,
+        action='store_true', dest='sponskrub_force',
+        help='Run sponskrub even if the video was already downloaded')
+    sponskrub.add_option(
+        '--no-sponskrub-force',
+        action='store_true', dest='sponskrub_force',
+        help='Do not cut out the sponsor sections if the video was already downloaded (default)')
+    sponskrub.add_option(
+        '--sponskrub-location', metavar='PATH',
+        dest='sponskrub_path', default='',
+        help='Location of the sponskrub binary; either the path to the binary or its containing directory')
 
     parser.add_option_group(general)
     parser.add_option_group(network)
     parser.add_option_group(geo)
     parser.add_option_group(selection)
+    parser.add_option_group(extractor)
     parser.add_option_group(downloader)
     parser.add_option_group(filesystem)
     parser.add_option_group(thumbnail)
@@ -877,6 +993,7 @@ def parseOpts(overrideArguments=None):
     parser.add_option_group(authentication)
     parser.add_option_group(adobe_pass)
     parser.add_option_group(postproc)
+    parser.add_option_group(sponskrub)
 
     if overrideArguments is not None:
         opts, args = parser.parse_args(overrideArguments)
diff --git a/youtube_dl/postprocessor/__init__.py b/youtube_dl/postprocessor/__init__.py
index 3ea518399..138c03500 100644
--- a/youtube_dl/postprocessor/__init__.py
+++ b/youtube_dl/postprocessor/__init__.py
@@ -12,10 +12,12 @@ from .ffmpeg import (
     FFmpegMetadataPP,
     FFmpegVideoConvertorPP,
     FFmpegSubtitlesConvertorPP,
+    FFmpegThumbnailsConvertorPP,
 )
 from .xattrpp import XAttrMetadataPP
 from .execafterdownload import ExecAfterDownloadPP
 from .metadatafromtitle import MetadataFromTitlePP
+from .sponskrub import SponSkrubPP
 
 
 def get_postprocessor(key):
@@ -34,7 +36,9 @@ __all__ = [
     'FFmpegMetadataPP',
     'FFmpegPostProcessor',
     'FFmpegSubtitlesConvertorPP',
+    'FFmpegThumbnailsConvertorPP',
     'FFmpegVideoConvertorPP',
     'MetadataFromTitlePP',
+    'SponSkrubPP',
     'XAttrMetadataPP',
 ]
diff --git a/youtube_dl/postprocessor/common.py b/youtube_dl/postprocessor/common.py
index 599dd1df2..98a427ef6 100644
--- a/youtube_dl/postprocessor/common.py
+++ b/youtube_dl/postprocessor/common.py
@@ -1,12 +1,11 @@
 from __future__ import unicode_literals
 
-import os
-
 from ..utils import (
     PostProcessingError,
     cli_configuration_args,
     encodeFilename,
 )
+from ..compat import compat_str
 
 
 class PostProcessor(object):
@@ -33,6 +32,12 @@ class PostProcessor(object):
 
     def __init__(self, downloader=None):
         self._downloader = downloader
+        self.PP_NAME = self.pp_key()
+
+    @classmethod
+    def pp_key(cls):
+        name = cls.__name__[:-2]
+        return compat_str(name[6:]) if name[:6].lower() == 'ffmpeg' else name
 
     def set_downloader(self, downloader):
         """Sets the downloader for this PP."""
@@ -57,12 +62,51 @@ class PostProcessor(object):
 
     def try_utime(self, path, atime, mtime, errnote='Cannot update utime of file'):
         try:
-            os.utime(encodeFilename(path), (atime, mtime))
+            self._downloader.utime(encodeFilename(path), (atime, mtime))
         except Exception:
             self._downloader.report_warning(errnote)
 
-    def _configuration_args(self, default=[]):
-        return cli_configuration_args(self._downloader.params, 'postprocessor_args', default)
+    def _configuration_args(self, exe=None, keys=None, default=[], use_compat=True):
+        if not exe:
+            return cli_configuration_args(self._downloader.params.get('postprocessor_args'), default)
+        pp_key = self.pp_key().lower()
+        exe = exe.lower()
+        root_key = exe if pp_key == exe else '%s+%s' % (pp_key, exe)
+        keys = ['%s%s' % (root_key, k) for k in (keys or [''])]
+        if root_key in keys:
+            keys += [root_key] + ([] if pp_key == exe else [(self.pp_key(), exe)]) + ['default']
+        else:
+            use_compat = False
+        return cli_configuration_args(
+            self._downloader.params.get('postprocessor_args'),
+            keys, default, use_compat)
+
+    def to_screen(self, text, prefix=True, *args, **kwargs):
+        tag = '[%s] ' % self.PP_NAME if prefix else ''
+        if self._downloader:
+            return self._downloader.to_screen('%s%s' % (tag, text), *args, **kwargs)
+
+    def report_warning(self, text, *args, **kwargs):
+        if self._downloader:
+            return self._downloader.report_warning(text, *args, **kwargs)
+
+    def report_error(self, text, *args, **kwargs):
+        if self._downloader:
+            return self._downloader.report_error(text, *args, **kwargs)
+
+    def write_debug(self, text, prefix=True, *args, **kwargs):
+        tag = '[debug] ' if prefix else ''
+        if self.get_param('verbose', False) and self._downloader:
+            return self._downloader.to_screen('%s%s' % (tag, text), *args, **kwargs)
+
+    def get_param(self, name, default=None, *args, **kwargs):
+        if self._downloader:
+            return self._downloader.params.get(name, default, *args, **kwargs)
+        return default
+
+    @property
+    def params(self):
+        return self._downloader.params
 
 
 class AudioConversionError(PostProcessingError):
diff --git a/youtube_dl/postprocessor/embedthumbnail.py b/youtube_dl/postprocessor/embedthumbnail.py
index 3990908b6..5548f3636 100644
--- a/youtube_dl/postprocessor/embedthumbnail.py
+++ b/youtube_dl/postprocessor/embedthumbnail.py
@@ -1,20 +1,31 @@
 # coding: utf-8
 from __future__ import unicode_literals
 
-
 import os
 import subprocess
-
-from .ffmpeg import FFmpegPostProcessor
-
+import struct
+import re
+import base64
+
+try:
+    import mutagen
+    has_mutagen = True
+except ImportError:
+    has_mutagen = False
+
+from .ffmpeg import (
+    FFmpegPostProcessor,
+    FFmpegThumbnailsConvertorPP,
+)
 from ..utils import (
     check_executable,
     encodeArgument,
     encodeFilename,
+    error_to_compat_str,
     PostProcessingError,
     prepend_extension,
-    replace_extension,
-    shell_quote
+    process_communicate_or_kill,
+    shell_quote,
 )
 
 
@@ -24,7 +35,7 @@ class EmbedThumbnailPPError(PostProcessingError):
 
 class EmbedThumbnailPP(FFmpegPostProcessor):
     def __init__(self, downloader=None, already_have_thumbnail=False):
-        super(EmbedThumbnailPP, self).__init__(downloader)
+        FFmpegPostProcessor.__init__(self, downloader)
         self._already_have_thumbnail = already_have_thumbnail
 
     def run(self, info):
@@ -32,99 +43,136 @@ class EmbedThumbnailPP(FFmpegPostProcessor):
         temp_filename = prepend_extension(filename, 'temp')
 
         if not info.get('thumbnails'):
-            self._downloader.to_screen('[embedthumbnail] There aren\'t any thumbnails to embed')
+            self.to_screen('There aren\'t any thumbnails to embed')
             return [], info
 
         thumbnail_filename = info['thumbnails'][-1]['filename']
-
         if not os.path.exists(encodeFilename(thumbnail_filename)):
-            self._downloader.report_warning(
-                'Skipping embedding the thumbnail because the file is missing.')
+            self.report_warning('Skipping embedding the thumbnail because the file is missing.')
             return [], info
 
-        def is_webp(path):
-            with open(encodeFilename(path), 'rb') as f:
-                b = f.read(12)
-            return b[0:4] == b'RIFF' and b[8:] == b'WEBP'
-
         # Correct extension for WebP file with wrong extension (see #25687, #25717)
-        _, thumbnail_ext = os.path.splitext(thumbnail_filename)
-        if thumbnail_ext:
-            thumbnail_ext = thumbnail_ext[1:].lower()
-            if thumbnail_ext != 'webp' and is_webp(thumbnail_filename):
-                self._downloader.to_screen(
-                    '[ffmpeg] Correcting extension to webp and escaping path for thumbnail "%s"' % thumbnail_filename)
-                thumbnail_webp_filename = replace_extension(thumbnail_filename, 'webp')
-                os.rename(encodeFilename(thumbnail_filename), encodeFilename(thumbnail_webp_filename))
-                thumbnail_filename = thumbnail_webp_filename
-                thumbnail_ext = 'webp'
+        convertor = FFmpegThumbnailsConvertorPP(self._downloader)
+        convertor.fixup_webp(info, -1)
+
+        original_thumbnail = thumbnail_filename = info['thumbnails'][-1]['filename']
 
         # Convert unsupported thumbnail formats to JPEG (see #25687, #25717)
-        if thumbnail_ext not in ['jpg', 'png']:
-            # NB: % is supposed to be escaped with %% but this does not work
-            # for input files so working around with standard substitution
-            escaped_thumbnail_filename = thumbnail_filename.replace('%', '#')
-            os.rename(encodeFilename(thumbnail_filename), encodeFilename(escaped_thumbnail_filename))
-            escaped_thumbnail_jpg_filename = replace_extension(escaped_thumbnail_filename, 'jpg')
-            self._downloader.to_screen('[ffmpeg] Converting thumbnail "%s" to JPEG' % escaped_thumbnail_filename)
-            self.run_ffmpeg(escaped_thumbnail_filename, escaped_thumbnail_jpg_filename, ['-bsf:v', 'mjpeg2jpeg'])
-            os.remove(encodeFilename(escaped_thumbnail_filename))
-            thumbnail_jpg_filename = replace_extension(thumbnail_filename, 'jpg')
-            # Rename back to unescaped for further processing
-            os.rename(encodeFilename(escaped_thumbnail_jpg_filename), encodeFilename(thumbnail_jpg_filename))
-            thumbnail_filename = thumbnail_jpg_filename
+        thumbnail_ext = os.path.splitext(thumbnail_filename)[1][1:]
+        if thumbnail_ext not in ('jpg', 'png'):
+            thumbnail_filename = convertor.convert_thumbnail(thumbnail_filename, 'jpg')
+            thumbnail_ext = 'jpg'
 
+        mtime = os.stat(encodeFilename(filename)).st_mtime
+
+        success = True
         if info['ext'] == 'mp3':
             options = [
-                '-c', 'copy', '-map', '0', '-map', '1',
-                '-metadata:s:v', 'title="Album cover"', '-metadata:s:v', 'comment="Cover (Front)"']
-
-            self._downloader.to_screen('[ffmpeg] Adding thumbnail to "%s"' % filename)
+                '-c', 'copy', '-map', '0:0', '-map', '1:0', '-id3v2_version', '3',
+                '-metadata:s:v', 'title="Album cover"', '-metadata:s:v', 'comment="Cover (front)"']
 
+            self.to_screen('Adding thumbnail to "%s"' % filename)
             self.run_ffmpeg_multiple_files([filename, thumbnail_filename], temp_filename, options)
 
-            if not self._already_have_thumbnail:
-                os.remove(encodeFilename(thumbnail_filename))
+        elif info['ext'] in ['mkv', 'mka']:
+            options = ['-c', 'copy', '-map', '0', '-dn']
+
+            mimetype = 'image/%s' % ('png' if thumbnail_ext == 'png' else 'jpeg')
+            old_stream, new_stream = self.get_stream_number(
+                filename, ('tags', 'mimetype'), mimetype)
+            if old_stream is not None:
+                options.extend(['-map', '-0:%d' % old_stream])
+                new_stream -= 1
+            options.extend([
+                '-attach', thumbnail_filename,
+                '-metadata:s:%d' % new_stream, 'mimetype=%s' % mimetype,
+                '-metadata:s:%d' % new_stream, 'filename=cover.%s' % thumbnail_ext])
+
+            self.to_screen('Adding thumbnail to "%s"' % filename)
+            self.run_ffmpeg(filename, temp_filename, options)
+
+        elif info['ext'] in ['m4a', 'mp4', 'mov']:
+            try:
+                options = ['-c', 'copy', '-map', '0', '-dn', '-map', '1']
+
+                old_stream, new_stream = self.get_stream_number(
+                    filename, ('disposition', 'attached_pic'), 1)
+                if old_stream is not None:
+                    options.extend(['-map', '-0:%d' % old_stream])
+                    new_stream -= 1
+                options.extend(['-disposition:%s' % new_stream, 'attached_pic'])
+
+                self.to_screen('Adding thumbnail to "%s"' % filename)
+                self.run_ffmpeg_multiple_files([filename, thumbnail_filename], temp_filename, options)
+
+            except PostProcessingError as err:
+                self.report_warning('unable to embed using ffprobe & ffmpeg; %s' % error_to_compat_str(err))
+                atomicparsley = next((
+                    x for x in ['AtomicParsley', 'atomicparsley']
+                    if check_executable(x, ['-v'])), None)
+                if atomicparsley is None:
+                    raise EmbedThumbnailPPError('AtomicParsley was not found. Please install')
+
+                cmd = [encodeFilename(atomicparsley, True),
+                       encodeFilename(filename, True),
+                       encodeArgument('--artwork'),
+                       encodeFilename(thumbnail_filename, True),
+                       encodeArgument('-o'),
+                       encodeFilename(temp_filename, True)]
+                cmd += [encodeArgument(o) for o in self._configuration_args('AtomicParsley')]
+
+                self.to_screen('Adding thumbnail to "%s"' % filename)
+                self.write_debug('AtomicParsley command line: %s' % shell_quote(cmd))
+                p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+                stdout, stderr = process_communicate_or_kill(p)
+                if p.returncode != 0:
+                    msg = stderr.decode('utf-8', 'replace').strip()
+                    raise EmbedThumbnailPPError(msg)
+                # for formats that don't support thumbnails (like 3gp) AtomicParsley
+                # won't create to the temporary file
+                if b'No changes' in stdout:
+                    self.report_warning('The file format doesn\'t support embedding a thumbnail')
+                    success = False
+
+        elif info['ext'] in ['ogg', 'opus']:
+            if not has_mutagen:
+                raise EmbedThumbnailPPError('module mutagen was not found. Please install using `python -m pip install mutagen`')
+            self.to_screen('Adding thumbnail to "%s"' % filename)
+
+            size_regex = r',\s*(?P<w>\d+)x(?P<h>\d+)\s*[,\[]'
+            size_result = self.run_ffmpeg(thumbnail_filename, thumbnail_filename, ['-hide_banner'])
+            mobj = re.search(size_regex, size_result)
+            width, height = int(mobj.group('w')), int(mobj.group('h'))
+            mimetype = ('image/%s' % ('png' if thumbnail_ext == 'png' else 'jpeg')).encode('ascii')
+
+            # https://xiph.org/flac/format.html#metadata_block_picture
+            data = bytearray()
+            data += struct.pack('>II', 3, len(mimetype))
+            data += mimetype
+            data += struct.pack('>IIIIII', 0, width, height, 8, 0, os.stat(thumbnail_filename).st_size)  # 32 if png else 24
+
+            fin = open(thumbnail_filename, "rb")
+            data += fin.read()
+            fin.close()
+
+            temp_filename = filename
+            f = mutagen.File(temp_filename)
+            f.tags['METADATA_BLOCK_PICTURE'] = base64.b64encode(data).decode('ascii')
+            f.save()
+
+        else:
+            raise EmbedThumbnailPPError('Supported filetypes for thumbnail embedding are: mp3, mkv/mka, ogg/opus, m4a/mp4/mov')
+
+        if success and temp_filename != filename:
             os.remove(encodeFilename(filename))
             os.rename(encodeFilename(temp_filename), encodeFilename(filename))
 
-        elif info['ext'] in ['m4a', 'mp4']:
-            atomicparsley = next((x
-                                  for x in ['AtomicParsley', 'atomicparsley']
-                                  if check_executable(x, ['-v'])), None)
-
-            if atomicparsley is None:
-                raise EmbedThumbnailPPError('AtomicParsley was not found. Please install.')
-
-            cmd = [encodeFilename(atomicparsley, True),
-                   encodeFilename(filename, True),
-                   encodeArgument('--artwork'),
-                   encodeFilename(thumbnail_filename, True),
-                   encodeArgument('-o'),
-                   encodeFilename(temp_filename, True)]
-
-            self._downloader.to_screen('[atomicparsley] Adding thumbnail to "%s"' % filename)
-
-            if self._downloader.params.get('verbose', False):
-                self._downloader.to_screen('[debug] AtomicParsley command line: %s' % shell_quote(cmd))
-
-            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
-            stdout, stderr = p.communicate()
-
-            if p.returncode != 0:
-                msg = stderr.decode('utf-8', 'replace').strip()
-                raise EmbedThumbnailPPError(msg)
-
-            if not self._already_have_thumbnail:
-                os.remove(encodeFilename(thumbnail_filename))
-            # for formats that don't support thumbnails (like 3gp) AtomicParsley
-            # won't create to the temporary file
-            if b'No changes' in stdout:
-                self._downloader.report_warning('The file format doesn\'t support embedding a thumbnail')
-            else:
-                os.remove(encodeFilename(filename))
-                os.rename(encodeFilename(temp_filename), encodeFilename(filename))
-        else:
-            raise EmbedThumbnailPPError('Only mp3 and m4a/mp4 are supported for thumbnail embedding for now.')
+        self.try_utime(filename, mtime, mtime)
 
-        return [], info
+        files_to_delete = [thumbnail_filename]
+        if self._already_have_thumbnail:
+            if original_thumbnail == thumbnail_filename:
+                files_to_delete = []
+        elif original_thumbnail != thumbnail_filename:
+            files_to_delete.append(original_thumbnail)
+        return files_to_delete, info
diff --git a/youtube_dl/postprocessor/ffmpeg.py b/youtube_dl/postprocessor/ffmpeg.py
index 5f7298345..f73f5c304 100644
--- a/youtube_dl/postprocessor/ffmpeg.py
+++ b/youtube_dl/postprocessor/ffmpeg.py
@@ -5,10 +5,12 @@ import os
 import subprocess
 import time
 import re
+import json
 
 
 from .common import AudioConversionError, PostProcessor
 
+from ..compat import compat_str, compat_numeric_types
 from ..utils import (
     encodeArgument,
     encodeFilename,
@@ -17,10 +19,11 @@ from ..utils import (
     PostProcessingError,
     prepend_extension,
     shell_quote,
-    subtitles_filename,
     dfxp2srt,
     ISO639Utils,
+    process_communicate_or_kill,
     replace_extension,
+    traverse_dict,
 )
 
 
@@ -58,15 +61,14 @@ class FFmpegPostProcessor(PostProcessor):
 
     def check_version(self):
         if not self.available:
-            raise FFmpegPostProcessorError('ffmpeg or avconv not found. Please install one.')
+            raise FFmpegPostProcessorError('ffmpeg not found. Please install or provide the path using --ffmpeg-location')
 
         required_version = '10-0' if self.basename == 'avconv' else '1.0'
         if is_outdated_version(
                 self._versions[self.basename], required_version):
             warning = 'Your copy of %s is outdated, update %s to version %s or newer if you encounter any errors.' % (
                 self.basename, self.basename, required_version)
-            if self._downloader:
-                self._downloader.report_warning(warning)
+            self.report_warning(warning)
 
     @staticmethod
     def get_versions(downloader=None):
@@ -96,21 +98,21 @@ class FFmpegPostProcessor(PostProcessor):
         self._paths = None
         self._versions = None
         if self._downloader:
-            prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)
-            location = self._downloader.params.get('ffmpeg_location')
+            prefer_ffmpeg = self.get_param('prefer_ffmpeg', True)
+            location = self.get_param('ffmpeg_location')
             if location is not None:
                 if not os.path.exists(location):
-                    self._downloader.report_warning(
+                    self.report_warning(
                         'ffmpeg-location %s does not exist! '
-                        'Continuing without avconv/ffmpeg.' % (location))
+                        'Continuing without ffmpeg.' % (location))
                     self._versions = {}
                     return
                 elif not os.path.isdir(location):
                     basename = os.path.splitext(os.path.basename(location))[0]
                     if basename not in programs:
-                        self._downloader.report_warning(
+                        self.report_warning(
                             'Cannot identify executable %s, its basename should be one of %s. '
-                            'Continuing without avconv/ffmpeg.' %
+                            'Continuing without ffmpeg.' %
                             (location, ', '.join(programs)))
                         self._versions = {}
                         return None
@@ -163,7 +165,7 @@ class FFmpegPostProcessor(PostProcessor):
 
     def get_audio_codec(self, path):
         if not self.probe_available and not self.available:
-            raise PostProcessingError('ffprobe/avprobe and ffmpeg/avconv not found. Please install one.')
+            raise PostProcessingError('ffprobe and ffmpeg not found. Please install or provide the path using --ffmpeg-location')
         try:
             if self.probe_available:
                 cmd = [
@@ -174,13 +176,11 @@ class FFmpegPostProcessor(PostProcessor):
                     encodeFilename(self.executable, True),
                     encodeArgument('-i')]
             cmd.append(encodeFilename(self._ffmpeg_filename_argument(path), True))
-            if self._downloader.params.get('verbose', False):
-                self._downloader.to_screen(
-                    '[debug] %s command line: %s' % (self.basename, shell_quote(cmd)))
+            self.write_debug('%s command line: %s' % (self.basename, shell_quote(cmd)))
             handle = subprocess.Popen(
                 cmd, stderr=subprocess.PIPE,
                 stdout=subprocess.PIPE, stdin=subprocess.PIPE)
-            stdout_data, stderr_data = handle.communicate()
+            stdout_data, stderr_data = process_communicate_or_kill(handle)
             expected_ret = 0 if self.probe_available else 1
             if handle.wait() != expected_ret:
                 return None
@@ -203,50 +203,95 @@ class FFmpegPostProcessor(PostProcessor):
                 return mobj.group(1)
         return None
 
+    def get_metadata_object(self, path, opts=[]):
+        if self.probe_basename != 'ffprobe':
+            if self.probe_available:
+                self.report_warning('Only ffprobe is supported for metadata extraction')
+            raise PostProcessingError('ffprobe not found. Please install or provide the path using --ffmpeg-location')
+        self.check_version()
+
+        cmd = [
+            encodeFilename(self.probe_executable, True),
+            encodeArgument('-hide_banner'),
+            encodeArgument('-show_format'),
+            encodeArgument('-show_streams'),
+            encodeArgument('-print_format'),
+            encodeArgument('json'),
+        ]
+
+        cmd += opts
+        cmd.append(encodeFilename(self._ffmpeg_filename_argument(path), True))
+        self.write_debug('ffprobe command line: %s' % shell_quote(cmd))
+        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)
+        stdout, stderr = p.communicate()
+        return json.loads(stdout.decode('utf-8', 'replace'))
+
+    def get_stream_number(self, path, keys, value):
+        streams = self.get_metadata_object(path)['streams']
+        num = next(
+            (i for i, stream in enumerate(streams) if traverse_dict(stream, keys, casesense=False) == value),
+            None)
+        return num, len(streams)
+
     def run_ffmpeg_multiple_files(self, input_paths, out_path, opts):
+        return self.real_run_ffmpeg(
+            [(path, []) for path in input_paths],
+            [(out_path, opts)])
+
+    def real_run_ffmpeg(self, input_path_opts, output_path_opts):
         self.check_version()
 
         oldest_mtime = min(
-            os.stat(encodeFilename(path)).st_mtime for path in input_paths)
-
-        opts += self._configuration_args()
+            os.stat(encodeFilename(path)).st_mtime for path, _ in input_path_opts)
 
-        files_cmd = []
-        for path in input_paths:
-            files_cmd.extend([
-                encodeArgument('-i'),
-                encodeFilename(self._ffmpeg_filename_argument(path), True)
-            ])
         cmd = [encodeFilename(self.executable, True), encodeArgument('-y')]
         # avconv does not have repeat option
         if self.basename == 'ffmpeg':
             cmd += [encodeArgument('-loglevel'), encodeArgument('repeat+info')]
-        cmd += (files_cmd
-                + [encodeArgument(o) for o in opts]
-                + [encodeFilename(self._ffmpeg_filename_argument(out_path), True)])
 
-        if self._downloader.params.get('verbose', False):
-            self._downloader.to_screen('[debug] ffmpeg command line: %s' % shell_quote(cmd))
+        def make_args(file, args, name, number):
+            keys = ['_%s%d' % (name, number), '_%s' % name]
+            if name == 'o' and number == 1:
+                keys.append('')
+            args += self._configuration_args(self.basename, keys)
+            if name == 'i':
+                args.append('-i')
+            return (
+                [encodeArgument(arg) for arg in args]
+                + [encodeFilename(self._ffmpeg_filename_argument(file), True)])
+
+        for arg_type, path_opts in (('i', input_path_opts), ('o', output_path_opts)):
+            cmd += [arg for i, o in enumerate(path_opts)
+                    for arg in make_args(o[0], o[1], arg_type, i + 1)]
+
+        self.write_debug('ffmpeg command line: %s' % shell_quote(cmd))
         p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)
-        stdout, stderr = p.communicate()
+        stdout, stderr = process_communicate_or_kill(p)
         if p.returncode != 0:
-            stderr = stderr.decode('utf-8', 'replace')
-            msg = stderr.strip().split('\n')[-1]
-            raise FFmpegPostProcessorError(msg)
-        self.try_utime(out_path, oldest_mtime, oldest_mtime)
+            stderr = stderr.decode('utf-8', 'replace').strip()
+            if self.get_param('verbose', False):
+                self.report_error(stderr)
+            raise FFmpegPostProcessorError(stderr.split('\n')[-1])
+        for out_path, _ in output_path_opts:
+            self.try_utime(out_path, oldest_mtime, oldest_mtime)
+        return stderr.decode('utf-8', 'replace')
 
     def run_ffmpeg(self, path, out_path, opts):
-        self.run_ffmpeg_multiple_files([path], out_path, opts)
+        return self.run_ffmpeg_multiple_files([path], out_path, opts)
 
     def _ffmpeg_filename_argument(self, fn):
         # Always use 'file:' because the filename may contain ':' (ffmpeg
         # interprets that as a protocol) or can start with '-' (-- is broken in
         # ffmpeg, see https://ffmpeg.org/trac/ffmpeg/ticket/2127 for details)
         # Also leave '-' intact in order not to break streaming to stdout.
+        if fn.startswith(('http://', 'https://')):
+            return fn
         return 'file:' + fn if fn != '-' else fn
 
 
 class FFmpegExtractAudioPP(FFmpegPostProcessor):
+    COMMON_AUDIO_EXTENSIONS = ('wav', 'flac', 'm4a', 'aiff', 'mp3', 'ogg', 'mka', 'opus', 'wma')
+
     def __init__(self, downloader=None, preferredcodec=None, preferredquality=None, nopostoverwrites=False):
         FFmpegPostProcessor.__init__(self, downloader)
         if preferredcodec is None:
@@ -268,6 +313,11 @@ class FFmpegExtractAudioPP(FFmpegPostProcessor):
 
     def run(self, information):
         path = information['filepath']
+        orig_ext = information['ext']
+
+        if self._preferredcodec == 'best' and orig_ext in self.COMMON_AUDIO_EXTENSIONS:
+            self.to_screen('Skipping audio extraction since the file is already in a common audio format')
+            return [], information
 
         filecodec = self.get_audio_codec(path)
         if filecodec is None:
@@ -322,17 +372,17 @@ class FFmpegExtractAudioPP(FFmpegPostProcessor):
         prefix, sep, ext = path.rpartition('.')  # not os.path.splitext, since the latter does not work on unicode in all setups
         new_path = prefix + sep + extension
 
-        information['filepath'] = new_path
+        information['filename'] = new_path
         information['ext'] = extension
 
         # If we download foo.mp3 and convert it to... foo.mp3, then don't delete foo.mp3, silly.
         if (new_path == path
                 or (self._nopostoverwrites and os.path.exists(encodeFilename(new_path)))):
-            self._downloader.to_screen('[ffmpeg] Post-process file %s exists, skipping' % new_path)
+            self.to_screen('Post-process file %s exists, skipping' % new_path)
             return [], information
 
         try:
-            self._downloader.to_screen('[ffmpeg] Destination: ' + new_path)
+            self.to_screen('Destination: ' + new_path)
             self.run_ffmpeg(path, new_path, acodec, more_opts)
         except AudioConversionError as e:
             raise PostProcessingError(
@@ -349,6 +399,41 @@ class FFmpegExtractAudioPP(FFmpegPostProcessor):
         return [path], information
 
 
+class FFmpegVideoRemuxerPP(FFmpegPostProcessor):
+    def __init__(self, downloader=None, preferedformat=None):
+        super(FFmpegVideoRemuxerPP, self).__init__(downloader)
+        self._preferedformats = preferedformat.lower().split('/')
+
+    def run(self, information):
+        path = information['filename']
+        sourceext, targetext = information['ext'].lower(), None
+        for pair in self._preferedformats:
+            kv = pair.split('>')
+            if len(kv) == 1 or kv[0].strip() == sourceext:
+                targetext = kv[-1].strip()
+                break
+
+        _skip_msg = (
+            'could not find a mapping for %s' if not targetext
+            else 'already is in target format %s' if sourceext == targetext
+            else None)
+        if _skip_msg:
+            self.to_screen('Not remuxing media file %s; %s' % (path, _skip_msg % sourceext))
+            return [], information
+
+        options = ['-c', 'copy', '-map', '0', '-dn']
+        if targetext in ['mp4', 'm4a', 'mov']:
+            options.extend(['-movflags', '+faststart'])
+        prefix, sep, oldext = path.rpartition('.')
+        outpath = prefix + sep + targetext
+        self.to_screen('Remuxing video from %s to %s; Destination: %s' % (sourceext, targetext, outpath))
+        self.run_ffmpeg(path, outpath, options)
+        information['filename'] = outpath
+        information['format'] = targetext
+        information['ext'] = targetext
+        return [path], information
+
+
 class FFmpegVideoConvertorPP(FFmpegPostProcessor):
     def __init__(self, downloader=None, preferedformat=None):
         super(FFmpegVideoConvertorPP, self).__init__(downloader)
@@ -357,14 +442,14 @@ class FFmpegVideoConvertorPP(FFmpegPostProcessor):
     def run(self, information):
         path = information['filepath']
         if information['ext'] == self._preferedformat:
-            self._downloader.to_screen('[ffmpeg] Not converting video file %s - already is in target format %s' % (path, self._preferedformat))
+            self.to_screen('Not converting video file %s - already is in target format %s' % (path, self._preferedformat))
             return [], information
         options = []
         if self._preferedformat == 'avi':
             options.extend(['-c:v', 'libxvid', '-vtag', 'XVID'])
         prefix, sep, ext = path.rpartition('.')
         outpath = prefix + sep + self._preferedformat
-        self._downloader.to_screen('[' + 'ffmpeg' + '] Converting video from %s to %s, Destination: ' % (information['ext'], self._preferedformat) + outpath)
+        self.to_screen('Converting video from %s to %s, Destination: ' % (information['ext'], self._preferedformat) + outpath)
         self.run_ffmpeg(path, outpath, options)
         information['filepath'] = outpath
         information['format'] = self._preferedformat
@@ -373,13 +458,17 @@ class FFmpegVideoConvertorPP(FFmpegPostProcessor):
 
 
 class FFmpegEmbedSubtitlePP(FFmpegPostProcessor):
+    def __init__(self, downloader=None, already_have_subtitle=False):
+        super(FFmpegEmbedSubtitlePP, self).__init__(downloader)
+        self._already_have_subtitle = already_have_subtitle
+
     def run(self, information):
         if information['ext'] not in ('mp4', 'webm', 'mkv'):
-            self._downloader.to_screen('[ffmpeg] Subtitles can only be embedded in mp4, webm or mkv files')
+            self.to_screen('Subtitles can only be embedded in mp4, webm or mkv files')
             return [], information
         subtitles = information.get('requested_subtitles')
         if not subtitles:
-            self._downloader.to_screen('[ffmpeg] There aren\'t any subtitles to embed')
+            self.to_screen('There aren\'t any subtitles to embed')
             return [], information
 
         filename = information['filepath']
@@ -388,16 +477,22 @@ class FFmpegEmbedSubtitlePP(FFmpegPostProcessor):
         sub_langs = []
         sub_filenames = []
         webm_vtt_warn = False
+        mp4_ass_warn = False
 
         for lang, sub_info in subtitles.items():
             sub_ext = sub_info['ext']
-            if ext != 'webm' or ext == 'webm' and sub_ext == 'vtt':
+            if sub_ext == 'json':
+                self.report_warning('JSON subtitles cannot be embedded')
+            elif ext != 'webm' or ext == 'webm' and sub_ext == 'vtt':
                 sub_langs.append(lang)
-                sub_filenames.append(subtitles_filename(filename, lang, sub_ext, ext))
+                sub_filenames.append(sub_info['filename'])
             else:
                 if not webm_vtt_warn and ext == 'webm' and sub_ext != 'vtt':
                     webm_vtt_warn = True
-                    self._downloader.to_screen('[ffmpeg] Only WebVTT subtitles can be embedded in webm files')
+                    self.report_warning('Only WebVTT subtitles can be embedded in webm files')
+            if not mp4_ass_warn and ext == 'mp4' and sub_ext == 'ass':
+                mp4_ass_warn = True
+                self.report_warning('ASS subtitles cannot be properly embedded in mp4 files; expect issues')
 
         if not sub_langs:
             return [], information
@@ -405,8 +500,7 @@ class FFmpegEmbedSubtitlePP(FFmpegPostProcessor):
         input_files = [filename] + sub_filenames
 
         opts = [
-            '-map', '0',
-            '-c', 'copy',
+            '-c', 'copy', '-map', '0', '-dn',
             # Don't copy the existing subtitles, we may be running the
             # postprocessor a second time
             '-map', '-0:s',
@@ -422,12 +516,13 @@ class FFmpegEmbedSubtitlePP(FFmpegPostProcessor):
             opts.extend(['-metadata:s:s:%d' % i, 'language=%s' % lang_code])
 
         temp_filename = prepend_extension(filename, 'temp')
-        self._downloader.to_screen('[ffmpeg] Embedding subtitles in \'%s\'' % filename)
+        self.to_screen('Embedding subtitles in "%s"' % filename)
         self.run_ffmpeg_multiple_files(input_files, temp_filename, opts)
         os.remove(encodeFilename(filename))
         os.rename(encodeFilename(temp_filename), encodeFilename(filename))
 
-        return sub_filenames, information
+        files_to_delete = [] if self._already_have_subtitle else sub_filenames
+        return files_to_delete, information
 
 
 class FFmpegMetadataPP(FFmpegPostProcessor):
@@ -435,6 +530,8 @@ class FFmpegMetadataPP(FFmpegPostProcessor):
         metadata = {}
 
         def add(meta_list, info_list=None):
+            if not meta_list:
+                return
             if not info_list:
                 info_list = meta_list
             if not isinstance(meta_list, (list, tuple)):
@@ -442,7 +539,7 @@ class FFmpegMetadataPP(FFmpegPostProcessor):
             if not isinstance(info_list, (list, tuple)):
                 info_list = (info_list,)
             for info_f in info_list:
-                if info.get(info_f) is not None:
+                if isinstance(info.get(info_f), (compat_str, compat_numeric_types)):
                     for meta_f in meta_list:
                         metadata[meta_f] = info[info_f]
                     break
@@ -452,12 +549,11 @@ class FFmpegMetadataPP(FFmpegPostProcessor):
         # 1. https://kdenlive.org/en/project/adding-meta-data-to-mp4-video/
         # 2. https://wiki.multimedia.cx/index.php/FFmpeg_Metadata
         # 3. https://kodi.wiki/view/Video_file_tagging
-        # 4. http://atomicparsley.sourceforge.net/mpeg-4files.html
 
         add('title', ('track', 'title'))
         add('date', 'upload_date')
-        add(('description', 'comment'), 'description')
-        add('purl', 'webpage_url')
+        add(('description', 'synopsis'), 'description')
+        add(('purl', 'comment'), 'webpage_url')
         add('track', 'track_number')
         add('artist', ('artist', 'creator', 'uploader', 'uploader_id'))
         add('genre')
@@ -469,21 +565,25 @@ class FFmpegMetadataPP(FFmpegPostProcessor):
         add('episode_id', ('episode', 'episode_id'))
         add('episode_sort', 'episode_number')
 
+        prefix = 'meta_'
+        for key in filter(lambda k: k.startswith(prefix), info.keys()):
+            add(key[len(prefix):], key)
+
         if not metadata:
-            self._downloader.to_screen('[ffmpeg] There isn\'t any metadata to add')
+            self.to_screen('There isn\'t any metadata to add')
             return [], info
 
         filename = info['filepath']
         temp_filename = prepend_extension(filename, 'temp')
         in_filenames = [filename]
-        options = []
+        options = ['-map', '0', '-dn']
 
         if info['ext'] == 'm4a':
             options.extend(['-vn', '-acodec', 'copy'])
         else:
             options.extend(['-c', 'copy'])
 
-        for (name, value) in metadata.items():
+        for name, value in metadata.items():
             options.extend(['-metadata', '%s=%s' % (name, value)])
 
         chapters = info.get('chapters', [])
@@ -505,7 +605,19 @@ class FFmpegMetadataPP(FFmpegPostProcessor):
                 in_filenames.append(metadata_filename)
                 options.extend(['-map_metadata', '1'])
 
-        self._downloader.to_screen('[ffmpeg] Adding metadata to \'%s\'' % filename)
+        if '__infojson_filename' in info and info['ext'] in ('mkv', 'mka'):
+            old_stream, new_stream = self.get_stream_number(
+                filename, ('tags', 'mimetype'), 'application/json')
+            if old_stream is not None:
+                options.extend(['-map', '-0:%d' % old_stream])
+                new_stream -= 1
+
+            options.extend([
+                '-attach', info['__infojson_filename'],
+                '-metadata:s:%d' % new_stream, 'mimetype=application/json'
+            ])
+
+        self.to_screen('Adding metadata to \'%s\'' % filename)
         self.run_ffmpeg_multiple_files(in_filenames, temp_filename, options)
         if chapters:
             os.remove(metadata_filename)
@@ -518,8 +630,13 @@ class FFmpegMergerPP(FFmpegPostProcessor):
     def run(self, info):
         filename = info['filepath']
         temp_filename = prepend_extension(filename, 'temp')
-        args = ['-c', 'copy', '-map', '0:v:0', '-map', '1:a:0']
-        self._downloader.to_screen('[ffmpeg] Merging formats into "%s"' % filename)
+        args = ['-c', 'copy']
+        for (i, fmt) in enumerate(info['requested_formats']):
+            if fmt.get('acodec') != 'none':
+                args.extend(['-map', '%u:a:0' % (i)])
+            if fmt.get('vcodec') != 'none':
+                args.extend(['-map', '%u:v:0' % (i)])
+        self.to_screen('Merging formats into "%s"' % filename)
         self.run_ffmpeg_multiple_files(info['__files_to_merge'], temp_filename, args)
         os.rename(encodeFilename(temp_filename), encodeFilename(filename))
         return info['__files_to_merge'], info
@@ -533,11 +650,10 @@ class FFmpegMergerPP(FFmpegPostProcessor):
         if is_outdated_version(
                 self._versions[self.basename], required_version):
             warning = ('Your copy of %s is outdated and unable to properly mux separate video and audio files, '
-                       'youtube-dl will download single file media. '
+                       'yt-dlp will download single file media. '
                        'Update %s to version %s or newer to fix this.') % (
                            self.basename, self.basename, required_version)
-            if self._downloader:
-                self._downloader.report_warning(warning)
+            self.report_warning(warning)
             return False
         return True
 
@@ -551,8 +667,8 @@ class FFmpegFixupStretchedPP(FFmpegPostProcessor):
         filename = info['filepath']
         temp_filename = prepend_extension(filename, 'temp')
 
-        options = ['-c', 'copy', '-aspect', '%f' % stretched_ratio]
-        self._downloader.to_screen('[ffmpeg] Fixing aspect ratio in "%s"' % filename)
+        options = ['-c', 'copy', '-map', '0', '-dn', '-aspect', '%f' % stretched_ratio]
+        self.to_screen('Fixing aspect ratio in "%s"' % filename)
         self.run_ffmpeg(filename, temp_filename, options)
 
         os.remove(encodeFilename(filename))
@@ -569,8 +685,8 @@ class FFmpegFixupM4aPP(FFmpegPostProcessor):
         filename = info['filepath']
         temp_filename = prepend_extension(filename, 'temp')
 
-        options = ['-c', 'copy', '-f', 'mp4']
-        self._downloader.to_screen('[ffmpeg] Correcting container in "%s"' % filename)
+        options = ['-c', 'copy', '-map', '0', '-dn', '-f', 'mp4']
+        self.to_screen('Correcting container in "%s"' % filename)
         self.run_ffmpeg(filename, temp_filename, options)
 
         os.remove(encodeFilename(filename))
@@ -585,8 +701,8 @@ class FFmpegFixupM3u8PP(FFmpegPostProcessor):
         if self.get_audio_codec(filename) == 'aac':
             temp_filename = prepend_extension(filename, 'temp')
 
-            options = ['-c', 'copy', '-f', 'mp4', '-bsf:a', 'aac_adtstoasc']
-            self._downloader.to_screen('[ffmpeg] Fixing malformed AAC bitstream in "%s"' % filename)
+            options = ['-c', 'copy', '-map', '0', '-dn', '-f', 'mp4', '-bsf:a', 'aac_adtstoasc']
+            self.to_screen('Fixing malformed AAC bitstream in "%s"' % filename)
             self.run_ffmpeg(filename, temp_filename, options)
 
             os.remove(encodeFilename(filename))
@@ -601,33 +717,36 @@ class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):
 
     def run(self, info):
         subs = info.get('requested_subtitles')
-        filename = info['filepath']
         new_ext = self.format
         new_format = new_ext
         if new_format == 'vtt':
             new_format = 'webvtt'
         if subs is None:
-            self._downloader.to_screen('[ffmpeg] There aren\'t any subtitles to convert')
+            self.to_screen('There aren\'t any subtitles to convert')
             return [], info
-        self._downloader.to_screen('[ffmpeg] Converting subtitles')
+        self.to_screen('Converting subtitles')
         sub_filenames = []
         for lang, sub in subs.items():
             ext = sub['ext']
             if ext == new_ext:
-                self._downloader.to_screen(
-                    '[ffmpeg] Subtitle file for %s is already in the requested format' % new_ext)
+                self.to_screen('Subtitle file for %s is already in the requested format' % new_ext)
+                continue
+            elif ext == 'json':
+                self.to_screen(
+                    'You have requested to convert json subtitles into another format, '
+                    'which is currently not possible')
                 continue
-            old_file = subtitles_filename(filename, lang, ext, info.get('ext'))
+            old_file = sub['filepath']
             sub_filenames.append(old_file)
-            new_file = subtitles_filename(filename, lang, new_ext, info.get('ext'))
+            new_file = replace_extension(old_file, new_ext)
 
             if ext in ('dfxp', 'ttml', 'tt'):
-                self._downloader.report_warning(
+                self.report_warning(
                     'You have requested to convert dfxp (TTML) subtitles into another format, '
                     'which results in style information loss')
 
                 dfxp_file = old_file
-                srt_file = subtitles_filename(filename, lang, 'srt', info.get('ext'))
+                srt_file = replace_extension(old_file, 'srt')
 
                 with open(dfxp_file, 'rb') as f:
                     srt_data = dfxp2srt(f.read())
@@ -638,7 +757,8 @@ class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):
 
                 subs[lang] = {
                     'ext': 'srt',
-                    'data': srt_data
+                    'data': srt_data,
+                    'filepath': srt_file,
                 }
 
                 if new_ext == 'srt':
@@ -652,6 +772,81 @@ class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):
                 subs[lang] = {
                     'ext': new_ext,
                     'data': f.read(),
+                    'filepath': new_file,
+                    'filename': new_file,
                 }
 
+            info['__files_to_move'][new_file] = replace_extension(
+                info['__files_to_move'][old_file], new_ext)
+
         return sub_filenames, info
+
+
+class FFmpegThumbnailsConvertorPP(FFmpegPostProcessor):
+    def __init__(self, downloader=None, format=None):
+        super(FFmpegThumbnailsConvertorPP, self).__init__(downloader)
+        self.format = format
+
+    @staticmethod
+    def is_webp(path):
+        with open(encodeFilename(path), 'rb') as f:
+            b = f.read(12)
+        return b[0:4] == b'RIFF' and b[8:] == b'WEBP'
+
+    def fixup_webp(self, info, idx=-1):
+        thumbnail_filename = info['thumbnails'][idx]['filename']
+        _, thumbnail_ext = os.path.splitext(thumbnail_filename)
+        if thumbnail_ext:
+            thumbnail_ext = thumbnail_ext[1:].lower()
+            if thumbnail_ext != 'webp' and self.is_webp(thumbnail_filename):
+                self.to_screen('Correcting thumbnail "%s" extension to webp' % thumbnail_filename)
+                webp_filename = replace_extension(thumbnail_filename, 'webp')
+                if os.path.exists(webp_filename):
+                    os.remove(webp_filename)
+                os.rename(encodeFilename(thumbnail_filename), encodeFilename(webp_filename))
+                info['thumbnails'][idx]['filename'] = webp_filename
+                info['__files_to_move'][webp_filename] = replace_extension(
+                    info['__files_to_move'].pop(thumbnail_filename), 'webp')
+
+    def convert_thumbnail(self, thumbnail_filename, ext):
+        if ext != 'jpg':
+            raise FFmpegPostProcessorError('Only conversion to jpg is currently supported')
+        # NB: % is supposed to be escaped with %% but this does not work
+        # for input files so working around with standard substitution
+        escaped_thumbnail_filename = thumbnail_filename.replace('%', '#')
+        os.rename(encodeFilename(thumbnail_filename), encodeFilename(escaped_thumbnail_filename))
+        escaped_thumbnail_jpg_filename = replace_extension(escaped_thumbnail_filename, 'jpg')
+        self.to_screen('Converting thumbnail "%s" to JPEG' % escaped_thumbnail_filename)
+        self.run_ffmpeg(escaped_thumbnail_filename, escaped_thumbnail_jpg_filename, ['-bsf:v', 'mjpeg2jpeg'])
+        thumbnail_jpg_filename = replace_extension(thumbnail_filename, 'jpg')
+        # Rename back to unescaped
+        os.rename(encodeFilename(escaped_thumbnail_filename), encodeFilename(thumbnail_filename))
+        os.rename(encodeFilename(escaped_thumbnail_jpg_filename), encodeFilename(thumbnail_jpg_filename))
+        return thumbnail_jpg_filename
+
+    def run(self, info):
+        if self.format != 'jpg':
+            raise FFmpegPostProcessorError('Only conversion to jpg is currently supported')
+        files_to_delete = []
+        has_thumbnail = False
+
+        for idx, thumbnail_dict in enumerate(info['thumbnails']):
+            if 'filepath' not in thumbnail_dict:
+                continue
+            has_thumbnail = True
+            self.fixup_webp(info, idx)
+            original_thumbnail = thumbnail_dict['filename']
+            _, thumbnail_ext = os.path.splitext(original_thumbnail)
+            if thumbnail_ext:
+                thumbnail_ext = thumbnail_ext[1:].lower()
+            if thumbnail_ext == self.format:
+                self.to_screen('Thumbnail "%s" is already in the requested format' % original_thumbnail)
+                continue
+            thumbnail_dict['filename'] = self.convert_thumbnail(original_thumbnail, self.format)
+            files_to_delete.append(original_thumbnail)
+            info['__files_to_move'][thumbnail_dict['filename']] = replace_extension(
+                info['__files_to_move'][original_thumbnail], self.format)
+
+        if not has_thumbnail:
+            self.to_screen('There aren\'t any thumbnails to convert')
+        return files_to_delete, info
diff --git a/youtube_dl/postprocessor/sponskrub.py b/youtube_dl/postprocessor/sponskrub.py
new file mode 100644
index 000000000..8d1341846
--- /dev/null
+++ b/youtube_dl/postprocessor/sponskrub.py
@@ -0,0 +1,95 @@
+from __future__ import unicode_literals
+import os
+import subprocess
+
+from .common import PostProcessor
+from ..compat import compat_shlex_split
+from ..utils import (
+    check_executable,
+    cli_option,
+    encodeArgument,
+    encodeFilename,
+    shell_quote,
+    str_or_none,
+    PostProcessingError,
+    prepend_extension,
+    process_communicate_or_kill,
+)
+
+
+class SponSkrubPP(PostProcessor):
+    _temp_ext = 'spons'
+    _exe_name = 'sponskrub'
+
+    def __init__(self, downloader, path='', args=None, ignoreerror=False, cut=False, force=False):
+        PostProcessor.__init__(self, downloader)
+        self.force = force
+        self.cutout = cut
+        self.args = str_or_none(args) or ''  # For backward compatibility
+        self.path = self.get_exe(path)
+
+        if not ignoreerror and self.path is None:
+            if path:
+                raise PostProcessingError('sponskrub not found in "%s"' % path)
+            else:
+                raise PostProcessingError('sponskrub not found. Please install or provide the path using --sponskrub-path.')
+
+    def get_exe(self, path=''):
+        if not path or not check_executable(path, ['-h']):
+            path = os.path.join(path, self._exe_name)
+            if not check_executable(path, ['-h']):
+                return None
+        return path
+
+    def run(self, information):
+        if self.path is None:
+            return [], information
+
+        filename = information['filepath']
+        if not os.path.exists(encodeFilename(filename)):  # no download
+            return [], information
+
+        if information['extractor_key'].lower() != 'youtube':
+            self.to_screen('Skipping sponskrub since it is not a YouTube video')
+            return [], information
+        if self.cutout and not self.force and not information.get('__real_download', False):
+            self.report_warning(
+                'Skipping sponskrub since the video was already downloaded. '
+                'Use --sponskrub-force to run sponskrub anyway')
+            return [], information
+
+        self.to_screen('Trying to %s sponsor sections' % ('remove' if self.cutout else 'mark'))
+        if self.cutout:
+            self.report_warning('Cutting out sponsor segments will cause the subtitles to go out of sync.')
+            if not information.get('__real_download', False):
+                self.report_warning('If sponskrub is run multiple times, unintended parts of the video could be cut out.')
+
+        temp_filename = prepend_extension(filename, self._temp_ext)
+        if os.path.exists(encodeFilename(temp_filename)):
+            os.remove(encodeFilename(temp_filename))
+
+        cmd = [self.path]
+        if not self.cutout:
+            cmd += ['-chapter']
+        cmd += cli_option(self.params, '-proxy', 'proxy')
+        cmd += compat_shlex_split(self.args)  # For backward compatibility
+        cmd += self._configuration_args(self._exe_name, use_compat=False)
+        cmd += ['--', information['id'], filename, temp_filename]
+        cmd = [encodeArgument(i) for i in cmd]
+
+        self.write_debug('sponskrub command line: %s' % shell_quote(cmd))
+        pipe = None if self.get_param('verbose') else subprocess.PIPE
+        p = subprocess.Popen(cmd, stdout=pipe)
+        stdout = process_communicate_or_kill(p)[0]
+
+        if p.returncode == 0:
+            os.remove(encodeFilename(filename))
+            os.rename(encodeFilename(temp_filename), encodeFilename(filename))
+            self.to_screen('Sponsor sections have been %s' % ('removed' if self.cutout else 'marked'))
+        elif p.returncode == 3:
+            self.to_screen('No segments in the SponsorBlock database')
+        else:
+            msg = stdout.decode('utf-8', 'replace').strip() if stdout else ''
+            msg = msg.split('\n')[0 if msg.lower().startswith('unrecognised') else -1]
+            raise PostProcessingError(msg if msg else 'sponskrub failed with error code %s' % p.returncode)
+        return [], information
diff --git a/youtube_dl/update.py b/youtube_dl/update.py
index 84c964617..bb2638e96 100644
--- a/youtube_dl/update.py
+++ b/youtube_dl/update.py
@@ -13,6 +13,10 @@ from .compat import compat_realpath
 from .utils import encode_compat_str
 
 from .version import __version__
+try:
+    from .build_config import variant
+except ImportError:
+    variant = None
 
 
 def rsa_verify(message, signature, key):
@@ -32,10 +36,9 @@ def rsa_verify(message, signature, key):
 def update_self(to_screen, verbose, opener):
     """Update the program file with the latest version from the repository"""
 
-    UPDATE_URL = 'https://yt-dl.org/update/'
+    UPDATE_URL = 'https://ytdl-patched.github.io/ytdl-patched/'
     VERSION_URL = UPDATE_URL + 'LATEST_VERSION'
     JSON_URL = UPDATE_URL + 'versions.json'
-    UPDATES_RSA_KEY = (0x9d60ee4d8f805312fdb15a62f87b95bd66177b91df176765d13514a0f1754bcd2057295c5b6f1d35daa6742c3ffc9a82d3e118861c207995a8031e151d863c9927e304576bc80692bc8e094896fcf11b66f3e29e04e3a71e9a11558558acea1840aec37fc396fb6b65dc81a1c4144e03bd1c011de62e3f1357b327d08426fe93, 65537)
 
     if not isinstance(globals().get('__loader__'), zipimporter) and not hasattr(sys, 'frozen'):
         to_screen('It looks like you installed youtube-dl with a package manager, pip, setup.py or a tarball. Please use that to update.')
@@ -62,14 +65,8 @@ def update_self(to_screen, verbose, opener):
             to_screen(encode_compat_str(traceback.format_exc()))
         to_screen('ERROR: can\'t obtain versions info. Please try again later.')
         return
-    if 'signature' not in versions_info:
-        to_screen('ERROR: the versions file is not signed or corrupted. Aborting.')
-        return
-    signature = versions_info['signature']
-    del versions_info['signature']
-    if not rsa_verify(json.dumps(versions_info, sort_keys=True).encode('utf-8'), signature, UPDATES_RSA_KEY):
-        to_screen('ERROR: the versions file signature is invalid. Aborting.')
-        return
+
+    # Don't test signatures for now
 
     version_id = versions_info['latest']
 
@@ -93,16 +90,23 @@ def update_self(to_screen, verbose, opener):
         to_screen('ERROR: no write permissions on %s' % filename)
         return
 
-    # Py2EXE
-    if hasattr(sys, 'frozen'):
+    # PyInstaller
+    if hasattr(sys, 'frozen') and os.name == 'nt':
         exe = filename
         directory = os.path.dirname(exe)
         if not os.access(directory, os.W_OK):
             to_screen('ERROR: no write permissions on %s' % directory)
             return
 
+        version_data = None
+        if variant:
+            version_data = version.get('exe-%s' % variant)
+        if not version_data:
+            version_data = version['exe']
+        assert version_data
+
         try:
-            urlh = opener.open(version['exe'][0])
+            urlh = opener.open(version_data[0])
             newcontent = urlh.read()
             urlh.close()
         except (IOError, OSError):
@@ -112,7 +116,7 @@ def update_self(to_screen, verbose, opener):
             return
 
         newcontent_hash = hashlib.sha256(newcontent).hexdigest()
-        if newcontent_hash != version['exe'][1]:
+        if newcontent_hash != version_data[1]:
             to_screen('ERROR: the downloaded file hash does not match. Aborting.')
             return
 
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index e722eed58..83d00f188 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -72,12 +72,14 @@ from .socks import (
     sockssocket,
 )
 
+from .chrome_versions import versions as _CHROME_VERSIONS
+
 
 def register_socks_protocols():
     # "Register" SOCKS protocols
     # In Python < 2.6.5, urlsplit() suffers from bug https://bugs.python.org/issue7904
     # URLs with protocols not in urlparse.uses_netloc are not handled correctly
-    for scheme in ('socks', 'socks4', 'socks4a', 'socks5'):
+    for scheme in ('socks', 'socks4', 'socks4a', 'socks5', 'socks5h'):
         if scheme not in compat_urlparse.uses_netloc:
             compat_urlparse.uses_netloc.append(scheme)
 
@@ -87,1586 +89,14 @@ compiled_regex_type = type(re.compile(''))
 
 
 def random_user_agent():
-    _USER_AGENT_TPL = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/%s Safari/537.36'
-    _CHROME_VERSIONS = (
-        '74.0.3729.129',
-        '76.0.3780.3',
-        '76.0.3780.2',
-        '74.0.3729.128',
-        '76.0.3780.1',
-        '76.0.3780.0',
-        '75.0.3770.15',
-        '74.0.3729.127',
-        '74.0.3729.126',
-        '76.0.3779.1',
-        '76.0.3779.0',
-        '75.0.3770.14',
-        '74.0.3729.125',
-        '76.0.3778.1',
-        '76.0.3778.0',
-        '75.0.3770.13',
-        '74.0.3729.124',
-        '74.0.3729.123',
-        '73.0.3683.121',
-        '76.0.3777.1',
-        '76.0.3777.0',
-        '75.0.3770.12',
-        '74.0.3729.122',
-        '76.0.3776.4',
-        '75.0.3770.11',
-        '74.0.3729.121',
-        '76.0.3776.3',
-        '76.0.3776.2',
-        '73.0.3683.120',
-        '74.0.3729.120',
-        '74.0.3729.119',
-        '74.0.3729.118',
-        '76.0.3776.1',
-        '76.0.3776.0',
-        '76.0.3775.5',
-        '75.0.3770.10',
-        '74.0.3729.117',
-        '76.0.3775.4',
-        '76.0.3775.3',
-        '74.0.3729.116',
-        '75.0.3770.9',
-        '76.0.3775.2',
-        '76.0.3775.1',
-        '76.0.3775.0',
-        '75.0.3770.8',
-        '74.0.3729.115',
-        '74.0.3729.114',
-        '76.0.3774.1',
-        '76.0.3774.0',
-        '75.0.3770.7',
-        '74.0.3729.113',
-        '74.0.3729.112',
-        '74.0.3729.111',
-        '76.0.3773.1',
-        '76.0.3773.0',
-        '75.0.3770.6',
-        '74.0.3729.110',
-        '74.0.3729.109',
-        '76.0.3772.1',
-        '76.0.3772.0',
-        '75.0.3770.5',
-        '74.0.3729.108',
-        '74.0.3729.107',
-        '76.0.3771.1',
-        '76.0.3771.0',
-        '75.0.3770.4',
-        '74.0.3729.106',
-        '74.0.3729.105',
-        '75.0.3770.3',
-        '74.0.3729.104',
-        '74.0.3729.103',
-        '74.0.3729.102',
-        '75.0.3770.2',
-        '74.0.3729.101',
-        '75.0.3770.1',
-        '75.0.3770.0',
-        '74.0.3729.100',
-        '75.0.3769.5',
-        '75.0.3769.4',
-        '74.0.3729.99',
-        '75.0.3769.3',
-        '75.0.3769.2',
-        '75.0.3768.6',
-        '74.0.3729.98',
-        '75.0.3769.1',
-        '75.0.3769.0',
-        '74.0.3729.97',
-        '73.0.3683.119',
-        '73.0.3683.118',
-        '74.0.3729.96',
-        '75.0.3768.5',
-        '75.0.3768.4',
-        '75.0.3768.3',
-        '75.0.3768.2',
-        '74.0.3729.95',
-        '74.0.3729.94',
-        '75.0.3768.1',
-        '75.0.3768.0',
-        '74.0.3729.93',
-        '74.0.3729.92',
-        '73.0.3683.117',
-        '74.0.3729.91',
-        '75.0.3766.3',
-        '74.0.3729.90',
-        '75.0.3767.2',
-        '75.0.3767.1',
-        '75.0.3767.0',
-        '74.0.3729.89',
-        '73.0.3683.116',
-        '75.0.3766.2',
-        '74.0.3729.88',
-        '75.0.3766.1',
-        '75.0.3766.0',
-        '74.0.3729.87',
-        '73.0.3683.115',
-        '74.0.3729.86',
-        '75.0.3765.1',
-        '75.0.3765.0',
-        '74.0.3729.85',
-        '73.0.3683.114',
-        '74.0.3729.84',
-        '75.0.3764.1',
-        '75.0.3764.0',
-        '74.0.3729.83',
-        '73.0.3683.113',
-        '75.0.3763.2',
-        '75.0.3761.4',
-        '74.0.3729.82',
-        '75.0.3763.1',
-        '75.0.3763.0',
-        '74.0.3729.81',
-        '73.0.3683.112',
-        '75.0.3762.1',
-        '75.0.3762.0',
-        '74.0.3729.80',
-        '75.0.3761.3',
-        '74.0.3729.79',
-        '73.0.3683.111',
-        '75.0.3761.2',
-        '74.0.3729.78',
-        '74.0.3729.77',
-        '75.0.3761.1',
-        '75.0.3761.0',
-        '73.0.3683.110',
-        '74.0.3729.76',
-        '74.0.3729.75',
-        '75.0.3760.0',
-        '74.0.3729.74',
-        '75.0.3759.8',
-        '75.0.3759.7',
-        '75.0.3759.6',
-        '74.0.3729.73',
-        '75.0.3759.5',
-        '74.0.3729.72',
-        '73.0.3683.109',
-        '75.0.3759.4',
-        '75.0.3759.3',
-        '74.0.3729.71',
-        '75.0.3759.2',
-        '74.0.3729.70',
-        '73.0.3683.108',
-        '74.0.3729.69',
-        '75.0.3759.1',
-        '75.0.3759.0',
-        '74.0.3729.68',
-        '73.0.3683.107',
-        '74.0.3729.67',
-        '75.0.3758.1',
-        '75.0.3758.0',
-        '74.0.3729.66',
-        '73.0.3683.106',
-        '74.0.3729.65',
-        '75.0.3757.1',
-        '75.0.3757.0',
-        '74.0.3729.64',
-        '73.0.3683.105',
-        '74.0.3729.63',
-        '75.0.3756.1',
-        '75.0.3756.0',
-        '74.0.3729.62',
-        '73.0.3683.104',
-        '75.0.3755.3',
-        '75.0.3755.2',
-        '73.0.3683.103',
-        '75.0.3755.1',
-        '75.0.3755.0',
-        '74.0.3729.61',
-        '73.0.3683.102',
-        '74.0.3729.60',
-        '75.0.3754.2',
-        '74.0.3729.59',
-        '75.0.3753.4',
-        '74.0.3729.58',
-        '75.0.3754.1',
-        '75.0.3754.0',
-        '74.0.3729.57',
-        '73.0.3683.101',
-        '75.0.3753.3',
-        '75.0.3752.2',
-        '75.0.3753.2',
-        '74.0.3729.56',
-        '75.0.3753.1',
-        '75.0.3753.0',
-        '74.0.3729.55',
-        '73.0.3683.100',
-        '74.0.3729.54',
-        '75.0.3752.1',
-        '75.0.3752.0',
-        '74.0.3729.53',
-        '73.0.3683.99',
-        '74.0.3729.52',
-        '75.0.3751.1',
-        '75.0.3751.0',
-        '74.0.3729.51',
-        '73.0.3683.98',
-        '74.0.3729.50',
-        '75.0.3750.0',
-        '74.0.3729.49',
-        '74.0.3729.48',
-        '74.0.3729.47',
-        '75.0.3749.3',
-        '74.0.3729.46',
-        '73.0.3683.97',
-        '75.0.3749.2',
-        '74.0.3729.45',
-        '75.0.3749.1',
-        '75.0.3749.0',
-        '74.0.3729.44',
-        '73.0.3683.96',
-        '74.0.3729.43',
-        '74.0.3729.42',
-        '75.0.3748.1',
-        '75.0.3748.0',
-        '74.0.3729.41',
-        '75.0.3747.1',
-        '73.0.3683.95',
-        '75.0.3746.4',
-        '74.0.3729.40',
-        '74.0.3729.39',
-        '75.0.3747.0',
-        '75.0.3746.3',
-        '75.0.3746.2',
-        '74.0.3729.38',
-        '75.0.3746.1',
-        '75.0.3746.0',
-        '74.0.3729.37',
-        '73.0.3683.94',
-        '75.0.3745.5',
-        '75.0.3745.4',
-        '75.0.3745.3',
-        '75.0.3745.2',
-        '74.0.3729.36',
-        '75.0.3745.1',
-        '75.0.3745.0',
-        '75.0.3744.2',
-        '74.0.3729.35',
-        '73.0.3683.93',
-        '74.0.3729.34',
-        '75.0.3744.1',
-        '75.0.3744.0',
-        '74.0.3729.33',
-        '73.0.3683.92',
-        '74.0.3729.32',
-        '74.0.3729.31',
-        '73.0.3683.91',
-        '75.0.3741.2',
-        '75.0.3740.5',
-        '74.0.3729.30',
-        '75.0.3741.1',
-        '75.0.3741.0',
-        '74.0.3729.29',
-        '75.0.3740.4',
-        '73.0.3683.90',
-        '74.0.3729.28',
-        '75.0.3740.3',
-        '73.0.3683.89',
-        '75.0.3740.2',
-        '74.0.3729.27',
-        '75.0.3740.1',
-        '75.0.3740.0',
-        '74.0.3729.26',
-        '73.0.3683.88',
-        '73.0.3683.87',
-        '74.0.3729.25',
-        '75.0.3739.1',
-        '75.0.3739.0',
-        '73.0.3683.86',
-        '74.0.3729.24',
-        '73.0.3683.85',
-        '75.0.3738.4',
-        '75.0.3738.3',
-        '75.0.3738.2',
-        '75.0.3738.1',
-        '75.0.3738.0',
-        '74.0.3729.23',
-        '73.0.3683.84',
-        '74.0.3729.22',
-        '74.0.3729.21',
-        '75.0.3737.1',
-        '75.0.3737.0',
-        '74.0.3729.20',
-        '73.0.3683.83',
-        '74.0.3729.19',
-        '75.0.3736.1',
-        '75.0.3736.0',
-        '74.0.3729.18',
-        '73.0.3683.82',
-        '74.0.3729.17',
-        '75.0.3735.1',
-        '75.0.3735.0',
-        '74.0.3729.16',
-        '73.0.3683.81',
-        '75.0.3734.1',
-        '75.0.3734.0',
-        '74.0.3729.15',
-        '73.0.3683.80',
-        '74.0.3729.14',
-        '75.0.3733.1',
-        '75.0.3733.0',
-        '75.0.3732.1',
-        '74.0.3729.13',
-        '74.0.3729.12',
-        '73.0.3683.79',
-        '74.0.3729.11',
-        '75.0.3732.0',
-        '74.0.3729.10',
-        '73.0.3683.78',
-        '74.0.3729.9',
-        '74.0.3729.8',
-        '74.0.3729.7',
-        '75.0.3731.3',
-        '75.0.3731.2',
-        '75.0.3731.0',
-        '74.0.3729.6',
-        '73.0.3683.77',
-        '73.0.3683.76',
-        '75.0.3730.5',
-        '75.0.3730.4',
-        '73.0.3683.75',
-        '74.0.3729.5',
-        '73.0.3683.74',
-        '75.0.3730.3',
-        '75.0.3730.2',
-        '74.0.3729.4',
-        '73.0.3683.73',
-        '73.0.3683.72',
-        '75.0.3730.1',
-        '75.0.3730.0',
-        '74.0.3729.3',
-        '73.0.3683.71',
-        '74.0.3729.2',
-        '73.0.3683.70',
-        '74.0.3729.1',
-        '74.0.3729.0',
-        '74.0.3726.4',
-        '73.0.3683.69',
-        '74.0.3726.3',
-        '74.0.3728.0',
-        '74.0.3726.2',
-        '73.0.3683.68',
-        '74.0.3726.1',
-        '74.0.3726.0',
-        '74.0.3725.4',
-        '73.0.3683.67',
-        '73.0.3683.66',
-        '74.0.3725.3',
-        '74.0.3725.2',
-        '74.0.3725.1',
-        '74.0.3724.8',
-        '74.0.3725.0',
-        '73.0.3683.65',
-        '74.0.3724.7',
-        '74.0.3724.6',
-        '74.0.3724.5',
-        '74.0.3724.4',
-        '74.0.3724.3',
-        '74.0.3724.2',
-        '74.0.3724.1',
-        '74.0.3724.0',
-        '73.0.3683.64',
-        '74.0.3723.1',
-        '74.0.3723.0',
-        '73.0.3683.63',
-        '74.0.3722.1',
-        '74.0.3722.0',
-        '73.0.3683.62',
-        '74.0.3718.9',
-        '74.0.3702.3',
-        '74.0.3721.3',
-        '74.0.3721.2',
-        '74.0.3721.1',
-        '74.0.3721.0',
-        '74.0.3720.6',
-        '73.0.3683.61',
-        '72.0.3626.122',
-        '73.0.3683.60',
-        '74.0.3720.5',
-        '72.0.3626.121',
-        '74.0.3718.8',
-        '74.0.3720.4',
-        '74.0.3720.3',
-        '74.0.3718.7',
-        '74.0.3720.2',
-        '74.0.3720.1',
-        '74.0.3720.0',
-        '74.0.3718.6',
-        '74.0.3719.5',
-        '73.0.3683.59',
-        '74.0.3718.5',
-        '74.0.3718.4',
-        '74.0.3719.4',
-        '74.0.3719.3',
-        '74.0.3719.2',
-        '74.0.3719.1',
-        '73.0.3683.58',
-        '74.0.3719.0',
-        '73.0.3683.57',
-        '73.0.3683.56',
-        '74.0.3718.3',
-        '73.0.3683.55',
-        '74.0.3718.2',
-        '74.0.3718.1',
-        '74.0.3718.0',
-        '73.0.3683.54',
-        '74.0.3717.2',
-        '73.0.3683.53',
-        '74.0.3717.1',
-        '74.0.3717.0',
-        '73.0.3683.52',
-        '74.0.3716.1',
-        '74.0.3716.0',
-        '73.0.3683.51',
-        '74.0.3715.1',
-        '74.0.3715.0',
-        '73.0.3683.50',
-        '74.0.3711.2',
-        '74.0.3714.2',
-        '74.0.3713.3',
-        '74.0.3714.1',
-        '74.0.3714.0',
-        '73.0.3683.49',
-        '74.0.3713.1',
-        '74.0.3713.0',
-        '72.0.3626.120',
-        '73.0.3683.48',
-        '74.0.3712.2',
-        '74.0.3712.1',
-        '74.0.3712.0',
-        '73.0.3683.47',
-        '72.0.3626.119',
-        '73.0.3683.46',
-        '74.0.3710.2',
-        '72.0.3626.118',
-        '74.0.3711.1',
-        '74.0.3711.0',
-        '73.0.3683.45',
-        '72.0.3626.117',
-        '74.0.3710.1',
-        '74.0.3710.0',
-        '73.0.3683.44',
-        '72.0.3626.116',
-        '74.0.3709.1',
-        '74.0.3709.0',
-        '74.0.3704.9',
-        '73.0.3683.43',
-        '72.0.3626.115',
-        '74.0.3704.8',
-        '74.0.3704.7',
-        '74.0.3708.0',
-        '74.0.3706.7',
-        '74.0.3704.6',
-        '73.0.3683.42',
-        '72.0.3626.114',
-        '74.0.3706.6',
-        '72.0.3626.113',
-        '74.0.3704.5',
-        '74.0.3706.5',
-        '74.0.3706.4',
-        '74.0.3706.3',
-        '74.0.3706.2',
-        '74.0.3706.1',
-        '74.0.3706.0',
-        '73.0.3683.41',
-        '72.0.3626.112',
-        '74.0.3705.1',
-        '74.0.3705.0',
-        '73.0.3683.40',
-        '72.0.3626.111',
-        '73.0.3683.39',
-        '74.0.3704.4',
-        '73.0.3683.38',
-        '74.0.3704.3',
-        '74.0.3704.2',
-        '74.0.3704.1',
-        '74.0.3704.0',
-        '73.0.3683.37',
-        '72.0.3626.110',
-        '72.0.3626.109',
-        '74.0.3703.3',
-        '74.0.3703.2',
-        '73.0.3683.36',
-        '74.0.3703.1',
-        '74.0.3703.0',
-        '73.0.3683.35',
-        '72.0.3626.108',
-        '74.0.3702.2',
-        '74.0.3699.3',
-        '74.0.3702.1',
-        '74.0.3702.0',
-        '73.0.3683.34',
-        '72.0.3626.107',
-        '73.0.3683.33',
-        '74.0.3701.1',
-        '74.0.3701.0',
-        '73.0.3683.32',
-        '73.0.3683.31',
-        '72.0.3626.105',
-        '74.0.3700.1',
-        '74.0.3700.0',
-        '73.0.3683.29',
-        '72.0.3626.103',
-        '74.0.3699.2',
-        '74.0.3699.1',
-        '74.0.3699.0',
-        '73.0.3683.28',
-        '72.0.3626.102',
-        '73.0.3683.27',
-        '73.0.3683.26',
-        '74.0.3698.0',
-        '74.0.3696.2',
-        '72.0.3626.101',
-        '73.0.3683.25',
-        '74.0.3696.1',
-        '74.0.3696.0',
-        '74.0.3694.8',
-        '72.0.3626.100',
-        '74.0.3694.7',
-        '74.0.3694.6',
-        '74.0.3694.5',
-        '74.0.3694.4',
-        '72.0.3626.99',
-        '72.0.3626.98',
-        '74.0.3694.3',
-        '73.0.3683.24',
-        '72.0.3626.97',
-        '72.0.3626.96',
-        '72.0.3626.95',
-        '73.0.3683.23',
-        '72.0.3626.94',
-        '73.0.3683.22',
-        '73.0.3683.21',
-        '72.0.3626.93',
-        '74.0.3694.2',
-        '72.0.3626.92',
-        '74.0.3694.1',
-        '74.0.3694.0',
-        '74.0.3693.6',
-        '73.0.3683.20',
-        '72.0.3626.91',
-        '74.0.3693.5',
-        '74.0.3693.4',
-        '74.0.3693.3',
-        '74.0.3693.2',
-        '73.0.3683.19',
-        '74.0.3693.1',
-        '74.0.3693.0',
-        '73.0.3683.18',
-        '72.0.3626.90',
-        '74.0.3692.1',
-        '74.0.3692.0',
-        '73.0.3683.17',
-        '72.0.3626.89',
-        '74.0.3687.3',
-        '74.0.3691.1',
-        '74.0.3691.0',
-        '73.0.3683.16',
-        '72.0.3626.88',
-        '72.0.3626.87',
-        '73.0.3683.15',
-        '74.0.3690.1',
-        '74.0.3690.0',
-        '73.0.3683.14',
-        '72.0.3626.86',
-        '73.0.3683.13',
-        '73.0.3683.12',
-        '74.0.3689.1',
-        '74.0.3689.0',
-        '73.0.3683.11',
-        '72.0.3626.85',
-        '73.0.3683.10',
-        '72.0.3626.84',
-        '73.0.3683.9',
-        '74.0.3688.1',
-        '74.0.3688.0',
-        '73.0.3683.8',
-        '72.0.3626.83',
-        '74.0.3687.2',
-        '74.0.3687.1',
-        '74.0.3687.0',
-        '73.0.3683.7',
-        '72.0.3626.82',
-        '74.0.3686.4',
-        '72.0.3626.81',
-        '74.0.3686.3',
-        '74.0.3686.2',
-        '74.0.3686.1',
-        '74.0.3686.0',
-        '73.0.3683.6',
-        '72.0.3626.80',
-        '74.0.3685.1',
-        '74.0.3685.0',
-        '73.0.3683.5',
-        '72.0.3626.79',
-        '74.0.3684.1',
-        '74.0.3684.0',
-        '73.0.3683.4',
-        '72.0.3626.78',
-        '72.0.3626.77',
-        '73.0.3683.3',
-        '73.0.3683.2',
-        '72.0.3626.76',
-        '73.0.3683.1',
-        '73.0.3683.0',
-        '72.0.3626.75',
-        '71.0.3578.141',
-        '73.0.3682.1',
-        '73.0.3682.0',
-        '72.0.3626.74',
-        '71.0.3578.140',
-        '73.0.3681.4',
-        '73.0.3681.3',
-        '73.0.3681.2',
-        '73.0.3681.1',
-        '73.0.3681.0',
-        '72.0.3626.73',
-        '71.0.3578.139',
-        '72.0.3626.72',
-        '72.0.3626.71',
-        '73.0.3680.1',
-        '73.0.3680.0',
-        '72.0.3626.70',
-        '71.0.3578.138',
-        '73.0.3678.2',
-        '73.0.3679.1',
-        '73.0.3679.0',
-        '72.0.3626.69',
-        '71.0.3578.137',
-        '73.0.3678.1',
-        '73.0.3678.0',
-        '71.0.3578.136',
-        '73.0.3677.1',
-        '73.0.3677.0',
-        '72.0.3626.68',
-        '72.0.3626.67',
-        '71.0.3578.135',
-        '73.0.3676.1',
-        '73.0.3676.0',
-        '73.0.3674.2',
-        '72.0.3626.66',
-        '71.0.3578.134',
-        '73.0.3674.1',
-        '73.0.3674.0',
-        '72.0.3626.65',
-        '71.0.3578.133',
-        '73.0.3673.2',
-        '73.0.3673.1',
-        '73.0.3673.0',
-        '72.0.3626.64',
-        '71.0.3578.132',
-        '72.0.3626.63',
-        '72.0.3626.62',
-        '72.0.3626.61',
-        '72.0.3626.60',
-        '73.0.3672.1',
-        '73.0.3672.0',
-        '72.0.3626.59',
-        '71.0.3578.131',
-        '73.0.3671.3',
-        '73.0.3671.2',
-        '73.0.3671.1',
-        '73.0.3671.0',
-        '72.0.3626.58',
-        '71.0.3578.130',
-        '73.0.3670.1',
-        '73.0.3670.0',
-        '72.0.3626.57',
-        '71.0.3578.129',
-        '73.0.3669.1',
-        '73.0.3669.0',
-        '72.0.3626.56',
-        '71.0.3578.128',
-        '73.0.3668.2',
-        '73.0.3668.1',
-        '73.0.3668.0',
-        '72.0.3626.55',
-        '71.0.3578.127',
-        '73.0.3667.2',
-        '73.0.3667.1',
-        '73.0.3667.0',
-        '72.0.3626.54',
-        '71.0.3578.126',
-        '73.0.3666.1',
-        '73.0.3666.0',
-        '72.0.3626.53',
-        '71.0.3578.125',
-        '73.0.3665.4',
-        '73.0.3665.3',
-        '72.0.3626.52',
-        '73.0.3665.2',
-        '73.0.3664.4',
-        '73.0.3665.1',
-        '73.0.3665.0',
-        '72.0.3626.51',
-        '71.0.3578.124',
-        '72.0.3626.50',
-        '73.0.3664.3',
-        '73.0.3664.2',
-        '73.0.3664.1',
-        '73.0.3664.0',
-        '73.0.3663.2',
-        '72.0.3626.49',
-        '71.0.3578.123',
-        '73.0.3663.1',
-        '73.0.3663.0',
-        '72.0.3626.48',
-        '71.0.3578.122',
-        '73.0.3662.1',
-        '73.0.3662.0',
-        '72.0.3626.47',
-        '71.0.3578.121',
-        '73.0.3661.1',
-        '72.0.3626.46',
-        '73.0.3661.0',
-        '72.0.3626.45',
-        '71.0.3578.120',
-        '73.0.3660.2',
-        '73.0.3660.1',
-        '73.0.3660.0',
-        '72.0.3626.44',
-        '71.0.3578.119',
-        '73.0.3659.1',
-        '73.0.3659.0',
-        '72.0.3626.43',
-        '71.0.3578.118',
-        '73.0.3658.1',
-        '73.0.3658.0',
-        '72.0.3626.42',
-        '71.0.3578.117',
-        '73.0.3657.1',
-        '73.0.3657.0',
-        '72.0.3626.41',
-        '71.0.3578.116',
-        '73.0.3656.1',
-        '73.0.3656.0',
-        '72.0.3626.40',
-        '71.0.3578.115',
-        '73.0.3655.1',
-        '73.0.3655.0',
-        '72.0.3626.39',
-        '71.0.3578.114',
-        '73.0.3654.1',
-        '73.0.3654.0',
-        '72.0.3626.38',
-        '71.0.3578.113',
-        '73.0.3653.1',
-        '73.0.3653.0',
-        '72.0.3626.37',
-        '71.0.3578.112',
-        '73.0.3652.1',
-        '73.0.3652.0',
-        '72.0.3626.36',
-        '71.0.3578.111',
-        '73.0.3651.1',
-        '73.0.3651.0',
-        '72.0.3626.35',
-        '71.0.3578.110',
-        '73.0.3650.1',
-        '73.0.3650.0',
-        '72.0.3626.34',
-        '71.0.3578.109',
-        '73.0.3649.1',
-        '73.0.3649.0',
-        '72.0.3626.33',
-        '71.0.3578.108',
-        '73.0.3648.2',
-        '73.0.3648.1',
-        '73.0.3648.0',
-        '72.0.3626.32',
-        '71.0.3578.107',
-        '73.0.3647.2',
-        '73.0.3647.1',
-        '73.0.3647.0',
-        '72.0.3626.31',
-        '71.0.3578.106',
-        '73.0.3635.3',
-        '73.0.3646.2',
-        '73.0.3646.1',
-        '73.0.3646.0',
-        '72.0.3626.30',
-        '71.0.3578.105',
-        '72.0.3626.29',
-        '73.0.3645.2',
-        '73.0.3645.1',
-        '73.0.3645.0',
-        '72.0.3626.28',
-        '71.0.3578.104',
-        '72.0.3626.27',
-        '72.0.3626.26',
-        '72.0.3626.25',
-        '72.0.3626.24',
-        '73.0.3644.0',
-        '73.0.3643.2',
-        '72.0.3626.23',
-        '71.0.3578.103',
-        '73.0.3643.1',
-        '73.0.3643.0',
-        '72.0.3626.22',
-        '71.0.3578.102',
-        '73.0.3642.1',
-        '73.0.3642.0',
-        '72.0.3626.21',
-        '71.0.3578.101',
-        '73.0.3641.1',
-        '73.0.3641.0',
-        '72.0.3626.20',
-        '71.0.3578.100',
-        '72.0.3626.19',
-        '73.0.3640.1',
-        '73.0.3640.0',
-        '72.0.3626.18',
-        '73.0.3639.1',
-        '71.0.3578.99',
-        '73.0.3639.0',
-        '72.0.3626.17',
-        '73.0.3638.2',
-        '72.0.3626.16',
-        '73.0.3638.1',
-        '73.0.3638.0',
-        '72.0.3626.15',
-        '71.0.3578.98',
-        '73.0.3635.2',
-        '71.0.3578.97',
-        '73.0.3637.1',
-        '73.0.3637.0',
-        '72.0.3626.14',
-        '71.0.3578.96',
-        '71.0.3578.95',
-        '72.0.3626.13',
-        '71.0.3578.94',
-        '73.0.3636.2',
-        '71.0.3578.93',
-        '73.0.3636.1',
-        '73.0.3636.0',
-        '72.0.3626.12',
-        '71.0.3578.92',
-        '73.0.3635.1',
-        '73.0.3635.0',
-        '72.0.3626.11',
-        '71.0.3578.91',
-        '73.0.3634.2',
-        '73.0.3634.1',
-        '73.0.3634.0',
-        '72.0.3626.10',
-        '71.0.3578.90',
-        '71.0.3578.89',
-        '73.0.3633.2',
-        '73.0.3633.1',
-        '73.0.3633.0',
-        '72.0.3610.4',
-        '72.0.3626.9',
-        '71.0.3578.88',
-        '73.0.3632.5',
-        '73.0.3632.4',
-        '73.0.3632.3',
-        '73.0.3632.2',
-        '73.0.3632.1',
-        '73.0.3632.0',
-        '72.0.3626.8',
-        '71.0.3578.87',
-        '73.0.3631.2',
-        '73.0.3631.1',
-        '73.0.3631.0',
-        '72.0.3626.7',
-        '71.0.3578.86',
-        '72.0.3626.6',
-        '73.0.3630.1',
-        '73.0.3630.0',
-        '72.0.3626.5',
-        '71.0.3578.85',
-        '72.0.3626.4',
-        '73.0.3628.3',
-        '73.0.3628.2',
-        '73.0.3629.1',
-        '73.0.3629.0',
-        '72.0.3626.3',
-        '71.0.3578.84',
-        '73.0.3628.1',
-        '73.0.3628.0',
-        '71.0.3578.83',
-        '73.0.3627.1',
-        '73.0.3627.0',
-        '72.0.3626.2',
-        '71.0.3578.82',
-        '71.0.3578.81',
-        '71.0.3578.80',
-        '72.0.3626.1',
-        '72.0.3626.0',
-        '71.0.3578.79',
-        '70.0.3538.124',
-        '71.0.3578.78',
-        '72.0.3623.4',
-        '72.0.3625.2',
-        '72.0.3625.1',
-        '72.0.3625.0',
-        '71.0.3578.77',
-        '70.0.3538.123',
-        '72.0.3624.4',
-        '72.0.3624.3',
-        '72.0.3624.2',
-        '71.0.3578.76',
-        '72.0.3624.1',
-        '72.0.3624.0',
-        '72.0.3623.3',
-        '71.0.3578.75',
-        '70.0.3538.122',
-        '71.0.3578.74',
-        '72.0.3623.2',
-        '72.0.3610.3',
-        '72.0.3623.1',
-        '72.0.3623.0',
-        '72.0.3622.3',
-        '72.0.3622.2',
-        '71.0.3578.73',
-        '70.0.3538.121',
-        '72.0.3622.1',
-        '72.0.3622.0',
-        '71.0.3578.72',
-        '70.0.3538.120',
-        '72.0.3621.1',
-        '72.0.3621.0',
-        '71.0.3578.71',
-        '70.0.3538.119',
-        '72.0.3620.1',
-        '72.0.3620.0',
-        '71.0.3578.70',
-        '70.0.3538.118',
-        '71.0.3578.69',
-        '72.0.3619.1',
-        '72.0.3619.0',
-        '71.0.3578.68',
-        '70.0.3538.117',
-        '71.0.3578.67',
-        '72.0.3618.1',
-        '72.0.3618.0',
-        '71.0.3578.66',
-        '70.0.3538.116',
-        '72.0.3617.1',
-        '72.0.3617.0',
-        '71.0.3578.65',
-        '70.0.3538.115',
-        '72.0.3602.3',
-        '71.0.3578.64',
-        '72.0.3616.1',
-        '72.0.3616.0',
-        '71.0.3578.63',
-        '70.0.3538.114',
-        '71.0.3578.62',
-        '72.0.3615.1',
-        '72.0.3615.0',
-        '71.0.3578.61',
-        '70.0.3538.113',
-        '72.0.3614.1',
-        '72.0.3614.0',
-        '71.0.3578.60',
-        '70.0.3538.112',
-        '72.0.3613.1',
-        '72.0.3613.0',
-        '71.0.3578.59',
-        '70.0.3538.111',
-        '72.0.3612.2',
-        '72.0.3612.1',
-        '72.0.3612.0',
-        '70.0.3538.110',
-        '71.0.3578.58',
-        '70.0.3538.109',
-        '72.0.3611.2',
-        '72.0.3611.1',
-        '72.0.3611.0',
-        '71.0.3578.57',
-        '70.0.3538.108',
-        '72.0.3610.2',
-        '71.0.3578.56',
-        '71.0.3578.55',
-        '72.0.3610.1',
-        '72.0.3610.0',
-        '71.0.3578.54',
-        '70.0.3538.107',
-        '71.0.3578.53',
-        '72.0.3609.3',
-        '71.0.3578.52',
-        '72.0.3609.2',
-        '71.0.3578.51',
-        '72.0.3608.5',
-        '72.0.3609.1',
-        '72.0.3609.0',
-        '71.0.3578.50',
-        '70.0.3538.106',
-        '72.0.3608.4',
-        '72.0.3608.3',
-        '72.0.3608.2',
-        '71.0.3578.49',
-        '72.0.3608.1',
-        '72.0.3608.0',
-        '70.0.3538.105',
-        '71.0.3578.48',
-        '72.0.3607.1',
-        '72.0.3607.0',
-        '71.0.3578.47',
-        '70.0.3538.104',
-        '72.0.3606.2',
-        '72.0.3606.1',
-        '72.0.3606.0',
-        '71.0.3578.46',
-        '70.0.3538.103',
-        '70.0.3538.102',
-        '72.0.3605.3',
-        '72.0.3605.2',
-        '72.0.3605.1',
-        '72.0.3605.0',
-        '71.0.3578.45',
-        '70.0.3538.101',
-        '71.0.3578.44',
-        '71.0.3578.43',
-        '70.0.3538.100',
-        '70.0.3538.99',
-        '71.0.3578.42',
-        '72.0.3604.1',
-        '72.0.3604.0',
-        '71.0.3578.41',
-        '70.0.3538.98',
-        '71.0.3578.40',
-        '72.0.3603.2',
-        '72.0.3603.1',
-        '72.0.3603.0',
-        '71.0.3578.39',
-        '70.0.3538.97',
-        '72.0.3602.2',
-        '71.0.3578.38',
-        '71.0.3578.37',
-        '72.0.3602.1',
-        '72.0.3602.0',
-        '71.0.3578.36',
-        '70.0.3538.96',
-        '72.0.3601.1',
-        '72.0.3601.0',
-        '71.0.3578.35',
-        '70.0.3538.95',
-        '72.0.3600.1',
-        '72.0.3600.0',
-        '71.0.3578.34',
-        '70.0.3538.94',
-        '72.0.3599.3',
-        '72.0.3599.2',
-        '72.0.3599.1',
-        '72.0.3599.0',
-        '71.0.3578.33',
-        '70.0.3538.93',
-        '72.0.3598.1',
-        '72.0.3598.0',
-        '71.0.3578.32',
-        '70.0.3538.87',
-        '72.0.3597.1',
-        '72.0.3597.0',
-        '72.0.3596.2',
-        '71.0.3578.31',
-        '70.0.3538.86',
-        '71.0.3578.30',
-        '71.0.3578.29',
-        '72.0.3596.1',
-        '72.0.3596.0',
-        '71.0.3578.28',
-        '70.0.3538.85',
-        '72.0.3595.2',
-        '72.0.3591.3',
-        '72.0.3595.1',
-        '72.0.3595.0',
-        '71.0.3578.27',
-        '70.0.3538.84',
-        '72.0.3594.1',
-        '72.0.3594.0',
-        '71.0.3578.26',
-        '70.0.3538.83',
-        '72.0.3593.2',
-        '72.0.3593.1',
-        '72.0.3593.0',
-        '71.0.3578.25',
-        '70.0.3538.82',
-        '72.0.3589.3',
-        '72.0.3592.2',
-        '72.0.3592.1',
-        '72.0.3592.0',
-        '71.0.3578.24',
-        '72.0.3589.2',
-        '70.0.3538.81',
-        '70.0.3538.80',
-        '72.0.3591.2',
-        '72.0.3591.1',
-        '72.0.3591.0',
-        '71.0.3578.23',
-        '70.0.3538.79',
-        '71.0.3578.22',
-        '72.0.3590.1',
-        '72.0.3590.0',
-        '71.0.3578.21',
-        '70.0.3538.78',
-        '70.0.3538.77',
-        '72.0.3589.1',
-        '72.0.3589.0',
-        '71.0.3578.20',
-        '70.0.3538.76',
-        '71.0.3578.19',
-        '70.0.3538.75',
-        '72.0.3588.1',
-        '72.0.3588.0',
-        '71.0.3578.18',
-        '70.0.3538.74',
-        '72.0.3586.2',
-        '72.0.3587.0',
-        '71.0.3578.17',
-        '70.0.3538.73',
-        '72.0.3586.1',
-        '72.0.3586.0',
-        '71.0.3578.16',
-        '70.0.3538.72',
-        '72.0.3585.1',
-        '72.0.3585.0',
-        '71.0.3578.15',
-        '70.0.3538.71',
-        '71.0.3578.14',
-        '72.0.3584.1',
-        '72.0.3584.0',
-        '71.0.3578.13',
-        '70.0.3538.70',
-        '72.0.3583.2',
-        '71.0.3578.12',
-        '72.0.3583.1',
-        '72.0.3583.0',
-        '71.0.3578.11',
-        '70.0.3538.69',
-        '71.0.3578.10',
-        '72.0.3582.0',
-        '72.0.3581.4',
-        '71.0.3578.9',
-        '70.0.3538.67',
-        '72.0.3581.3',
-        '72.0.3581.2',
-        '72.0.3581.1',
-        '72.0.3581.0',
-        '71.0.3578.8',
-        '70.0.3538.66',
-        '72.0.3580.1',
-        '72.0.3580.0',
-        '71.0.3578.7',
-        '70.0.3538.65',
-        '71.0.3578.6',
-        '72.0.3579.1',
-        '72.0.3579.0',
-        '71.0.3578.5',
-        '70.0.3538.64',
-        '71.0.3578.4',
-        '71.0.3578.3',
-        '71.0.3578.2',
-        '71.0.3578.1',
-        '71.0.3578.0',
-        '70.0.3538.63',
-        '69.0.3497.128',
-        '70.0.3538.62',
-        '70.0.3538.61',
-        '70.0.3538.60',
-        '70.0.3538.59',
-        '71.0.3577.1',
-        '71.0.3577.0',
-        '70.0.3538.58',
-        '69.0.3497.127',
-        '71.0.3576.2',
-        '71.0.3576.1',
-        '71.0.3576.0',
-        '70.0.3538.57',
-        '70.0.3538.56',
-        '71.0.3575.2',
-        '70.0.3538.55',
-        '69.0.3497.126',
-        '70.0.3538.54',
-        '71.0.3575.1',
-        '71.0.3575.0',
-        '71.0.3574.1',
-        '71.0.3574.0',
-        '70.0.3538.53',
-        '69.0.3497.125',
-        '70.0.3538.52',
-        '71.0.3573.1',
-        '71.0.3573.0',
-        '70.0.3538.51',
-        '69.0.3497.124',
-        '71.0.3572.1',
-        '71.0.3572.0',
-        '70.0.3538.50',
-        '69.0.3497.123',
-        '71.0.3571.2',
-        '70.0.3538.49',
-        '69.0.3497.122',
-        '71.0.3571.1',
-        '71.0.3571.0',
-        '70.0.3538.48',
-        '69.0.3497.121',
-        '71.0.3570.1',
-        '71.0.3570.0',
-        '70.0.3538.47',
-        '69.0.3497.120',
-        '71.0.3568.2',
-        '71.0.3569.1',
-        '71.0.3569.0',
-        '70.0.3538.46',
-        '69.0.3497.119',
-        '70.0.3538.45',
-        '71.0.3568.1',
-        '71.0.3568.0',
-        '70.0.3538.44',
-        '69.0.3497.118',
-        '70.0.3538.43',
-        '70.0.3538.42',
-        '71.0.3567.1',
-        '71.0.3567.0',
-        '70.0.3538.41',
-        '69.0.3497.117',
-        '71.0.3566.1',
-        '71.0.3566.0',
-        '70.0.3538.40',
-        '69.0.3497.116',
-        '71.0.3565.1',
-        '71.0.3565.0',
-        '70.0.3538.39',
-        '69.0.3497.115',
-        '71.0.3564.1',
-        '71.0.3564.0',
-        '70.0.3538.38',
-        '69.0.3497.114',
-        '71.0.3563.0',
-        '71.0.3562.2',
-        '70.0.3538.37',
-        '69.0.3497.113',
-        '70.0.3538.36',
-        '70.0.3538.35',
-        '71.0.3562.1',
-        '71.0.3562.0',
-        '70.0.3538.34',
-        '69.0.3497.112',
-        '70.0.3538.33',
-        '71.0.3561.1',
-        '71.0.3561.0',
-        '70.0.3538.32',
-        '69.0.3497.111',
-        '71.0.3559.6',
-        '71.0.3560.1',
-        '71.0.3560.0',
-        '71.0.3559.5',
-        '71.0.3559.4',
-        '70.0.3538.31',
-        '69.0.3497.110',
-        '71.0.3559.3',
-        '70.0.3538.30',
-        '69.0.3497.109',
-        '71.0.3559.2',
-        '71.0.3559.1',
-        '71.0.3559.0',
-        '70.0.3538.29',
-        '69.0.3497.108',
-        '71.0.3558.2',
-        '71.0.3558.1',
-        '71.0.3558.0',
-        '70.0.3538.28',
-        '69.0.3497.107',
-        '71.0.3557.2',
-        '71.0.3557.1',
-        '71.0.3557.0',
-        '70.0.3538.27',
-        '69.0.3497.106',
-        '71.0.3554.4',
-        '70.0.3538.26',
-        '71.0.3556.1',
-        '71.0.3556.0',
-        '70.0.3538.25',
-        '71.0.3554.3',
-        '69.0.3497.105',
-        '71.0.3554.2',
-        '70.0.3538.24',
-        '69.0.3497.104',
-        '71.0.3555.2',
-        '70.0.3538.23',
-        '71.0.3555.1',
-        '71.0.3555.0',
-        '70.0.3538.22',
-        '69.0.3497.103',
-        '71.0.3554.1',
-        '71.0.3554.0',
-        '70.0.3538.21',
-        '69.0.3497.102',
-        '71.0.3553.3',
-        '70.0.3538.20',
-        '69.0.3497.101',
-        '71.0.3553.2',
-        '69.0.3497.100',
-        '71.0.3553.1',
-        '71.0.3553.0',
-        '70.0.3538.19',
-        '69.0.3497.99',
-        '69.0.3497.98',
-        '69.0.3497.97',
-        '71.0.3552.6',
-        '71.0.3552.5',
-        '71.0.3552.4',
-        '71.0.3552.3',
-        '71.0.3552.2',
-        '71.0.3552.1',
-        '71.0.3552.0',
-        '70.0.3538.18',
-        '69.0.3497.96',
-        '71.0.3551.3',
-        '71.0.3551.2',
-        '71.0.3551.1',
-        '71.0.3551.0',
-        '70.0.3538.17',
-        '69.0.3497.95',
-        '71.0.3550.3',
-        '71.0.3550.2',
-        '71.0.3550.1',
-        '71.0.3550.0',
-        '70.0.3538.16',
-        '69.0.3497.94',
-        '71.0.3549.1',
-        '71.0.3549.0',
-        '70.0.3538.15',
-        '69.0.3497.93',
-        '69.0.3497.92',
-        '71.0.3548.1',
-        '71.0.3548.0',
-        '70.0.3538.14',
-        '69.0.3497.91',
-        '71.0.3547.1',
-        '71.0.3547.0',
-        '70.0.3538.13',
-        '69.0.3497.90',
-        '71.0.3546.2',
-        '69.0.3497.89',
-        '71.0.3546.1',
-        '71.0.3546.0',
-        '70.0.3538.12',
-        '69.0.3497.88',
-        '71.0.3545.4',
-        '71.0.3545.3',
-        '71.0.3545.2',
-        '71.0.3545.1',
-        '71.0.3545.0',
-        '70.0.3538.11',
-        '69.0.3497.87',
-        '71.0.3544.5',
-        '71.0.3544.4',
-        '71.0.3544.3',
-        '71.0.3544.2',
-        '71.0.3544.1',
-        '71.0.3544.0',
-        '69.0.3497.86',
-        '70.0.3538.10',
-        '69.0.3497.85',
-        '70.0.3538.9',
-        '69.0.3497.84',
-        '71.0.3543.4',
-        '70.0.3538.8',
-        '71.0.3543.3',
-        '71.0.3543.2',
-        '71.0.3543.1',
-        '71.0.3543.0',
-        '70.0.3538.7',
-        '69.0.3497.83',
-        '71.0.3542.2',
-        '71.0.3542.1',
-        '71.0.3542.0',
-        '70.0.3538.6',
-        '69.0.3497.82',
-        '69.0.3497.81',
-        '71.0.3541.1',
-        '71.0.3541.0',
-        '70.0.3538.5',
-        '69.0.3497.80',
-        '71.0.3540.1',
-        '71.0.3540.0',
-        '70.0.3538.4',
-        '69.0.3497.79',
-        '70.0.3538.3',
-        '71.0.3539.1',
-        '71.0.3539.0',
-        '69.0.3497.78',
-        '68.0.3440.134',
-        '69.0.3497.77',
-        '70.0.3538.2',
-        '70.0.3538.1',
-        '70.0.3538.0',
-        '69.0.3497.76',
-        '68.0.3440.133',
-        '69.0.3497.75',
-        '70.0.3537.2',
-        '70.0.3537.1',
-        '70.0.3537.0',
-        '69.0.3497.74',
-        '68.0.3440.132',
-        '70.0.3536.0',
-        '70.0.3535.5',
-        '70.0.3535.4',
-        '70.0.3535.3',
-        '69.0.3497.73',
-        '68.0.3440.131',
-        '70.0.3532.8',
-        '70.0.3532.7',
-        '69.0.3497.72',
-        '69.0.3497.71',
-        '70.0.3535.2',
-        '70.0.3535.1',
-        '70.0.3535.0',
-        '69.0.3497.70',
-        '68.0.3440.130',
-        '69.0.3497.69',
-        '68.0.3440.129',
-        '70.0.3534.4',
-        '70.0.3534.3',
-        '70.0.3534.2',
-        '70.0.3534.1',
-        '70.0.3534.0',
-        '69.0.3497.68',
-        '68.0.3440.128',
-        '70.0.3533.2',
-        '70.0.3533.1',
-        '70.0.3533.0',
-        '69.0.3497.67',
-        '68.0.3440.127',
-        '70.0.3532.6',
-        '70.0.3532.5',
-        '70.0.3532.4',
-        '69.0.3497.66',
-        '68.0.3440.126',
-        '70.0.3532.3',
-        '70.0.3532.2',
-        '70.0.3532.1',
-        '69.0.3497.60',
-        '69.0.3497.65',
-        '69.0.3497.64',
-        '70.0.3532.0',
-        '70.0.3531.0',
-        '70.0.3530.4',
-        '70.0.3530.3',
-        '70.0.3530.2',
-        '69.0.3497.58',
-        '68.0.3440.125',
-        '69.0.3497.57',
-        '69.0.3497.56',
-        '69.0.3497.55',
-        '69.0.3497.54',
-        '70.0.3530.1',
-        '70.0.3530.0',
-        '69.0.3497.53',
-        '68.0.3440.124',
-        '69.0.3497.52',
-        '70.0.3529.3',
-        '70.0.3529.2',
-        '70.0.3529.1',
-        '70.0.3529.0',
-        '69.0.3497.51',
-        '70.0.3528.4',
-        '68.0.3440.123',
-        '70.0.3528.3',
-        '70.0.3528.2',
-        '70.0.3528.1',
-        '70.0.3528.0',
-        '69.0.3497.50',
-        '68.0.3440.122',
-        '70.0.3527.1',
-        '70.0.3527.0',
-        '69.0.3497.49',
-        '68.0.3440.121',
-        '70.0.3526.1',
-        '70.0.3526.0',
-        '68.0.3440.120',
-        '69.0.3497.48',
-        '69.0.3497.47',
-        '68.0.3440.119',
-        '68.0.3440.118',
-        '70.0.3525.5',
-        '70.0.3525.4',
-        '70.0.3525.3',
-        '68.0.3440.117',
-        '69.0.3497.46',
-        '70.0.3525.2',
-        '70.0.3525.1',
-        '70.0.3525.0',
-        '69.0.3497.45',
-        '68.0.3440.116',
-        '70.0.3524.4',
-        '70.0.3524.3',
-        '69.0.3497.44',
-        '70.0.3524.2',
-        '70.0.3524.1',
-        '70.0.3524.0',
-        '70.0.3523.2',
-        '69.0.3497.43',
-        '68.0.3440.115',
-        '70.0.3505.9',
-        '69.0.3497.42',
-        '70.0.3505.8',
-        '70.0.3523.1',
-        '70.0.3523.0',
-        '69.0.3497.41',
-        '68.0.3440.114',
-        '70.0.3505.7',
-        '69.0.3497.40',
-        '70.0.3522.1',
-        '70.0.3522.0',
-        '70.0.3521.2',
-        '69.0.3497.39',
-        '68.0.3440.113',
-        '70.0.3505.6',
-        '70.0.3521.1',
-        '70.0.3521.0',
-        '69.0.3497.38',
-        '68.0.3440.112',
-        '70.0.3520.1',
-        '70.0.3520.0',
-        '69.0.3497.37',
-        '68.0.3440.111',
-        '70.0.3519.3',
-        '70.0.3519.2',
-        '70.0.3519.1',
-        '70.0.3519.0',
-        '69.0.3497.36',
-        '68.0.3440.110',
-        '70.0.3518.1',
-        '70.0.3518.0',
-        '69.0.3497.35',
-        '69.0.3497.34',
-        '68.0.3440.109',
-        '70.0.3517.1',
-        '70.0.3517.0',
-        '69.0.3497.33',
-        '68.0.3440.108',
-        '69.0.3497.32',
-        '70.0.3516.3',
-        '70.0.3516.2',
-        '70.0.3516.1',
-        '70.0.3516.0',
-        '69.0.3497.31',
-        '68.0.3440.107',
-        '70.0.3515.4',
-        '68.0.3440.106',
-        '70.0.3515.3',
-        '70.0.3515.2',
-        '70.0.3515.1',
-        '70.0.3515.0',
-        '69.0.3497.30',
-        '68.0.3440.105',
-        '68.0.3440.104',
-        '70.0.3514.2',
-        '70.0.3514.1',
-        '70.0.3514.0',
-        '69.0.3497.29',
-        '68.0.3440.103',
-        '70.0.3513.1',
-        '70.0.3513.0',
-        '69.0.3497.28',
+    _USER_AGENT_TPL = 'Mozilla/5.0 (Windows NT %s; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/%s Safari/537.36'
+    _WINDOWS_VERSIONS = (
+        '6.1',  # 7
+        '6.2',  # 8
+        '6.3',  # 8.1
+        '10.0',
     )
-    return _USER_AGENT_TPL % random.choice(_CHROME_VERSIONS)
+    return _USER_AGENT_TPL % (random.choice(_WINDOWS_VERSIONS), random.choice(_CHROME_VERSIONS))
 
 
 std_headers = {
@@ -1753,6 +183,9 @@ DATE_FORMATS = (
     '%b %d %Y at %H:%M:%S',
     '%B %d %Y at %H:%M',
     '%B %d %Y at %H:%M:%S',
+    '%Y%m%d',
+    '%Y%m%d%H%M',
+    '%Y%m%d%H%M%S',
 )
 
 DATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)
@@ -2019,10 +452,10 @@ def extract_attributes(html_element):
     return parser.attrs
 
 
-def clean_html(html):
+def clean_html(html, strip_script=False):
     """Clean an HTML snippet into a readable string"""
 
-    if html is None:  # Convenience for sanitizing descriptions etc.
+    if not html:  # Convenience for sanitizing descriptions etc.
         return html
 
     # Newline vs <br />
@@ -2030,10 +463,13 @@ def clean_html(html):
     html = re.sub(r'(?u)\s*<\s*br\s*/?\s*>\s*', '\n', html)
     html = re.sub(r'(?u)<\s*/\s*p\s*>\s*<\s*p[^>]*>', '\n', html)
     # Strip html tags
+    if strip_script:
+        html = re.sub(r'<(script|style)>.*?</\1>', '', html)
     html = re.sub('<.*?>', '', html)
     # Replace html entities
     html = unescapeHTML(html)
-    return html.strip()
+    if html:
+        return html.strip()
 
 
 def sanitize_open(filename, open_mode):
@@ -2204,7 +640,7 @@ def _htmlentity_transform(entity_with_semicolon):
 
 
 def unescapeHTML(s):
-    if s is None:
+    if not s:
         return None
     assert type(s) == compat_str
 
@@ -2321,7 +757,7 @@ def bug_reports_message():
         update_cmd = 'type  youtube-dl -U  to update'
     else:
         update_cmd = 'see  https://yt-dl.org/update  on how to update'
-    msg = '; please report this issue on https://yt-dl.org/bug .'
+    msg = '; please report this issue on https://git.io/JOq1c .'
     msg += ' Make sure you are using the latest version; %s.' % update_cmd
     msg += ' Be sure to call youtube-dl with the --verbose flag and include its complete output.'
     return msg
@@ -2674,7 +1110,7 @@ def make_socks_conn_class(base_class, socks_proxy):
         compat_http_client.HTTPConnection, compat_http_client.HTTPSConnection))
 
     url_components = compat_urlparse.urlparse(socks_proxy)
-    if url_components.scheme.lower() == 'socks5':
+    if url_components.scheme.lower() in ('socks5', 'socks5h'):
         socks_type = ProxyType.SOCKS5
     elif url_components.scheme.lower() in ('socks', 'socks4'):
         socks_type = ProxyType.SOCKS4
@@ -2875,7 +1311,15 @@ class YoutubeDLCookieProcessor(compat_urllib_request.HTTPCookieProcessor):
         #                 response.headers[set_cookie_header] = set_cookie_escaped
         return compat_urllib_request.HTTPCookieProcessor.http_response(self, request, response)
 
-    https_request = compat_urllib_request.HTTPCookieProcessor.http_request
+    def http_request(self, request):
+        # If the URL contains non-ASCII characters, the cookies
+        # are lost before the request reaches YoutubeDLHandler.
+        # So we percent encode the url before adding cookies
+        # See: https://github.com/yt-dlp/yt-dlp/issues/263
+        request = update_Request(request, url=escape_url(request.get_full_url()))
+        return compat_urllib_request.HTTPCookieProcessor.http_request(self, request)
+
+    https_request = http_request
     https_response = http_response
 
 
@@ -4651,12 +3095,27 @@ def cli_valueless_option(params, command_option, param, expected_value=True):
     return [command_option] if param == expected_value else []
 
 
-def cli_configuration_args(params, param, default=[]):
-    ex_args = params.get(param)
-    if ex_args is None:
+def cli_configuration_args(argdict, keys, default=[], use_compat=True):
+    if isinstance(argdict, (list, tuple)):  # for backward compatibility
+        if use_compat:
+            return argdict
+        else:
+            argdict = None
+    if argdict is None:
         return default
-    assert isinstance(ex_args, list)
-    return ex_args
+    assert isinstance(argdict, dict)
+
+    if not isinstance(keys, (list, tuple)):
+        keys = [keys]
+    for key_list in keys:
+        if isinstance(key_list, compat_str):
+            key_list = (key_list,)
+        arg_list = list(filter(
+            lambda x: x is not None,
+            [argdict.get(key.lower()) for key in key_list]))
+        if arg_list:
+            return [arg for args in arg_list for arg in args]
+    return default
 
 
 class ISO639Utils(object):
@@ -5401,7 +3860,7 @@ class PerRequestProxyHandler(compat_urllib_request.ProxyHandler):
 
         if proxy == '__noproxy__':
             return None  # No Proxy
-        if compat_urlparse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):
+        if compat_urlparse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5', 'socks5h'):
             req.add_header('Ytdl-socks-proxy', proxy)
             # youtube-dl's http/https handlers do wrapping the socket with socks
             return None
@@ -5757,6 +4216,63 @@ def random_birthday(year_field, month_field, day_field):
     }
 
 
+def bytes_to_scalar(value):
+    if isinstance(value, compat_str):
+        value = value.decode('utf8')
+    result = 0
+    for b in value:
+        result *= 256
+        result += b
+    return result
+
+
+def decode_base(value, digits):
+    # This will convert given base-x string to scalar (long or int)
+    table = {char: index for index, char in enumerate(digits)}
+    result = 0
+    base = len(digits)
+    for chr in value:
+        result *= base
+        result += table[chr]
+    return result
+
+
+def scalar_to_bytes(scalar):
+    if not scalar:
+        return b''
+    array = []
+    while scalar:
+        scalar, idx = divmod(scalar, 256)
+        array.insert(0, idx)
+    return intlist_to_bytes(array)
+
+
+def encode_base(scalar, digits):
+    # This will convert scalar (long or int) to base-x string
+    if not scalar:
+        return ''
+    base = len(digits)
+    result = ''
+    while scalar:
+        scalar, idx = divmod(scalar, base)
+        result = digits[idx] + result
+    return result
+
+
+def char_replace(base, replace, string):
+    # character-by-character replacing
+    if not string:
+        return ''
+    assert len(base) == len(replace)
+    table = {b: r for b, r in zip(base, replace) if b != r}
+    if not table:
+        return string
+    result = ''
+    for i in string:
+        result += table.get(i, i)
+    return result
+
+
 def clean_podcast_url(url):
     return re.sub(r'''(?x)
         (?:
@@ -5772,3 +4288,65 @@ def clean_podcast_url(url):
                 st\.fm # https://podsights.com/docs/
             )/e
         )/''', '', url)
+
+
+def to_str(value):
+    if isinstance(value, bytes):
+        value = value.decode(preferredencoding())
+    return value
+
+
+def process_communicate_or_kill(p, *args, **kwargs):
+    try:
+        return p.communicate(*args, **kwargs)
+    except BaseException:  # Including KeyboardInterrupt
+        p.kill()
+        p.wait()
+        raise
+
+
+def dig_object_type(obj, prefix='', lines=[]):
+    if isinstance(obj, dict):
+        for k, v in obj.items():
+            dig_object_type(v, prefix + '.' + str(k), lines)
+    elif isinstance(obj, tuple(x for x in (list, tuple, map, filter) if isinstance(x, type))):
+        for i, v in enumerate(obj):
+            dig_object_type(v, prefix + '[' + str(i) + ']', lines)
+    else:
+        lines.append(prefix + ': ' + str(type(obj)))
+    return lines
+
+
+class PrintJsonEncoder(json.JSONEncoder):
+    def default(self, obj):
+        if isinstance(obj, bytes):
+            try:
+                return obj.decode('utf-8')
+            except BaseException:
+                return None
+        else:
+            return json.JSONEncoder.default(self, obj)
+
+
+def time_millis():
+    return round(time.time() * 1000)
+
+
+def traverse_dict(dictn, keys, casesense=True):
+    keys = list(keys)[::-1]
+    while keys:
+        key = keys.pop()
+        if isinstance(dictn, dict):
+            if not casesense:
+                dictn = {k.lower(): v for k, v in dictn.items()}
+                key = key.lower()
+            dictn = dictn.get(key)
+        elif isinstance(dictn, (list, tuple, compat_str)):
+            if ':' in key:
+                key = slice(*map(int_or_none, key.split(':')))
+            else:
+                key = int_or_none(key)
+            dictn = try_get(dictn, lambda x: x[key])
+        else:
+            return None
+    return dictn
diff --git a/youtube_dl/websocket/__init__.py b/youtube_dl/websocket/__init__.py
new file mode 100644
index 000000000..f97472e4e
--- /dev/null
+++ b/youtube_dl/websocket/__init__.py
@@ -0,0 +1,74 @@
+from __future__ import unicode_literals
+
+HAVE_WEBSOCKET = False
+WebSocket = None
+
+# WebSocket: (URI, header={'Accept': 'nothing', 'X-Magic-Number': '42'})->WebSocket
+# only send, recv, close are guaranteed to exist
+
+HAVE_WS_WEBSOCKET_CLIENT, HAVE_WS_WEBSOCKETS, HAVE_WS_WEBSOCAT, HAVE_WS_NODEJS_WS_WRAPPER, HAVE_WS_NODEJS_WEBSOCKET_WRAPPER = (False, ) * 5
+
+try:
+    from websocket import create_connection, WebSocket
+
+    def _enter(self):
+        return self
+
+    def _exit(self, type, value, traceback):
+        self.close()
+
+    WebSocket.__enter__ = _enter
+    WebSocket.__exit__ = _exit
+
+    def WebSocketClientWrapper(url, headers={}):
+        return create_connection(url, headers=['%s: %s' % kv for kv in headers.items()])
+
+    HAVE_WS_WEBSOCKET_CLIENT = True
+    HAVE_WEBSOCKET = True
+except (ImportError, ValueError, SyntaxError):
+    WebSocketClientWrapper = None
+
+try:
+    from .websockets import WebSocketsWrapper
+    HAVE_WS_WEBSOCKETS = True
+    HAVE_WEBSOCKET = True
+except (ImportError, ValueError, SyntaxError):
+    WebSocketsWrapper = None
+
+try:
+    from .websocat import AVAILABLE
+
+    if AVAILABLE:
+        from .websocat import WebsocatWrapper
+        HAVE_WS_WEBSOCAT = True
+        HAVE_WEBSOCKET = True
+    else:
+        WebsocatWrapper = None
+except (ImportError, ValueError, SyntaxError):
+    WebsocatWrapper = None
+
+try:
+    from .nodejs import NPM_IS_SANE
+
+    if NPM_IS_SANE:
+        from .nodejs import HAVE_NODEJS_WEBSOCKET_WRAPPER, HAVE_NODEJS_WS_WRAPPER
+
+        if HAVE_NODEJS_WEBSOCKET_WRAPPER:
+            from .nodejs import NodeJsWebsocketWrapper
+            HAVE_WS_NODEJS_WEBSOCKET_WRAPPER = True
+            HAVE_WEBSOCKET = True
+        else:
+            NodeJsWebsocketWrapper = None
+
+        if HAVE_NODEJS_WS_WRAPPER:
+            from .nodejs import NodeJsWsWrapper
+            HAVE_WS_NODEJS_WS_WRAPPER = True
+            HAVE_WEBSOCKET = True
+        else:
+            NodeJsWsWrapper = None
+    else:
+        NodeJsWebsocketWrapper, NodeJsWsWrapper = None, None
+except (ImportError, ValueError, SyntaxError):
+    NodeJsWebsocketWrapper, NodeJsWsWrapper = None, None
+
+WebSocket = WebSocketClientWrapper or WebSocketsWrapper or WebsocatWrapper or NodeJsWebsocketWrapper or NodeJsWsWrapper
diff --git a/youtube_dl/websocket/nodejs.py b/youtube_dl/websocket/nodejs.py
new file mode 100644
index 000000000..1fec7c1eb
--- /dev/null
+++ b/youtube_dl/websocket/nodejs.py
@@ -0,0 +1,188 @@
+from __future__ import unicode_literals
+
+import codecs
+import json
+
+from ..utils import check_executable, to_str
+from ..compat import compat_str
+from subprocess import Popen, PIPE
+from os.path import join
+
+
+# (mostly joke) wrappers for NodeJS WebSocket packages
+
+if check_executable('node', ['-v']) and check_executable('npm', ['-v']):
+    npm_prefix = Popen(
+        ['npm', 'prefix', '-g'],
+        stdout=PIPE, stderr=PIPE, stdin=PIPE)
+
+    NPM_GLOBAL_PATH = npm_prefix.communicate()[0].decode().strip()
+    if NPM_GLOBAL_PATH:
+        NPM_GLOBAL_PATH = join(NPM_GLOBAL_PATH, 'lib/node_modules')
+    NPM_IS_SANE = bool(NPM_GLOBAL_PATH)
+else:
+    NPM_IS_SANE = False
+
+if NPM_IS_SANE:
+    def start_node_process(args):
+        return Popen(
+            ['node'] + args,
+            stdout=PIPE, stderr=PIPE, stdin=PIPE,
+            env={'NODE_PATH': NPM_GLOBAL_PATH})
+
+    def test_package_existence(pkg):
+        try:
+            assert isinstance(pkg, compat_str)
+            p = start_node_process(['-e', 'require("%s")' % pkg])
+            p.wait()
+            return p.returncode == 0
+        except OSError:
+            return False
+
+    """
+    Details of protocol between Node.js process(N) and NodeJsWrapperBase(P):
+
+    0. Every tokens are send via stdin/stdout by line-by-line manner, terminated by LF.
+    1. When N has started, prints "OPENED" after opening connection. P will wait for it.
+    2. All inbound frames must be converted into HEX and printed to N's stdout.
+    3. All outbound frames are converted into HEX and printed to N's stdin.
+    """
+    class NodeJsWrapperBase():
+        def __init__(self, url, headers={}):
+            self.proc = start_node_process(['-e', self.EVAL_CODE % (json.dumps(url), json.dumps({
+                # any JSON is valid for JS object/array/string/number
+                'headers': headers,
+            }))])
+            while True:
+                if to_str(self.proc.stdout.readline()).strip() == 'OPENED':
+                    return
+
+        def send(self, data):
+            if isinstance(data, compat_str):
+                data = data.encode('utf-8')
+            data = codecs.encode(data, "hex")
+            self.proc.stdin.write(data)
+            self.proc.stdin.write(b'\n')
+            self.proc.stdin.flush()
+
+        def recv(self):
+            ret = self.proc.stdout.readline().strip()
+            ret = codecs.decode(ret, "hex")
+            if isinstance(ret, bytes):
+                ret = ret.decode('utf-8')
+            return ret
+
+        def close(self):
+            self.proc.kill()
+            self.proc.terminate()
+            self.proc = None
+
+        def __enter__(self):
+            return self
+
+        def __exit__(self, type, value, traceback):
+            self.close()
+
+    class NodeJsWsWrapper(NodeJsWrapperBase):
+        EVAL_CODE = '''
+            const readline = require("readline");
+            const rl = readline.createInterface({
+                input: process.stdin,
+                output: process.stdout
+            });
+
+            const WebSocket = require('ws');
+
+            const ws = new WebSocket(%s, %s);
+
+            rl.on("line", function(line){
+                ws.send(Buffer.from(line, "hex").toString("utf8"));
+            });
+
+            ws.on('open', function() {
+                console.log('OPENED');
+            });
+
+            ws.on('error', function(err) {
+                const util = require('util');
+                process.stderr.write(util.inspect(err));
+                process.stderr.write("\\n");
+                process.exit(1);
+            });
+
+            // https://github.com/websockets/ws/blob/HEAD/doc/ws.md#event-message
+            ws.on('message', function(data) {
+                if(typeof data === 'string'){
+                    data = [Buffer.from(data, 'utf8')];
+                }else if(Buffer.isBuffer(data)){
+                    data = [data];
+                }else if(Array.isArray(data) && data.length){
+                    if (Buffer.isBuffer(data[0])) {
+                        // pass, expected type
+                    }else{
+                        // ArrayBuffer
+                        data = [Buffer.from(data, 'utf8')];
+                    }
+                }else{
+                    // unknown type, do toString() here
+                    data = [Buffer.from(`${data}`, 'utf8')];
+                }
+
+                for (d of data) {
+                    console.log(d.toString('hex'));
+                }
+            });
+        '''.strip()
+
+    class NodeJsWebsocketWrapper(NodeJsWrapperBase):
+        EVAL_CODE = '''
+            const readline = require("readline");
+            const rl = readline.createInterface({
+                input: process.stdin,
+                output: process.stdout
+            });
+
+            const WebSocket = require('websocket').client;
+
+            const ws = new WebSocket();
+
+            ws.on('connect', function(conn) {
+                console.log('OPENED');
+                rl.on("line", function(line){
+                    conn.sendUTF(Buffer.from(line, "hex").toString("utf8"));
+                });
+                conn.on('message', function(message) {
+                    let buf;
+                    if(message.type == 'utf8'){
+                        buf = Buffer.from(message.utf8Data, 'utf8');
+                    }else if(message.type == 'binary'){
+                        buf = Buffer.from(message.binaryDataBuffer);
+                    }else{
+                        return;
+                    }
+
+                    console.log(buf.toString('hex'));
+                });
+                conn.on('error', function(err) {
+                    const util = require('util');
+                    process.stderr.write(util.inspect(err));
+                    process.stderr.write("\\n");
+                    process.exit(1);
+                });
+            });
+
+            ws.on('error', function(err) {
+                const util = require('util');
+                process.stderr.write(util.inspect(err));
+                process.stderr.write("\\n");
+                process.exit(1);
+            });
+
+            ws.connect(%s, null, null, %s);
+        '''.strip()
+
+    HAVE_NODEJS_WS_WRAPPER = test_package_existence('ws')
+    HAVE_NODEJS_WEBSOCKET_WRAPPER = test_package_existence('websocket')
+else:
+    NodeJsWsWrapper, NodeJsWebsocketWrapper = (None, ) * 2
+    HAVE_NODEJS_WS_WRAPPER, HAVE_NODEJS_WEBSOCKET_WRAPPER = (False, ) * 2
diff --git a/youtube_dl/websocket/websocat.py b/youtube_dl/websocket/websocat.py
new file mode 100644
index 000000000..c8d639c6b
--- /dev/null
+++ b/youtube_dl/websocket/websocat.py
@@ -0,0 +1,59 @@
+from __future__ import unicode_literals
+
+from ..utils import check_executable
+from ..compat import compat_str
+from subprocess import Popen, PIPE
+
+
+class WebsocatWrapper():
+    "Wraps websocat command to use in non-async scopes"
+
+    def __init__(self, url, headers={}):
+        self.proc = Popen(
+            ['websocat', '-t', *('-H=%s: %s' % kv for kv in headers.items()), url],
+            stdout=PIPE, stdin=PIPE, stderr=PIPE)
+
+    def __read_stderr(func):
+        def cback(self, *args):
+            try:
+                return func(self, *args)
+            except BaseException as ex:
+                e = Exception()
+                if self.proc:
+                    e.msg = self.proc.stderr.read()
+                e.cause = ex
+                raise e
+        return cback
+
+    @__read_stderr
+    def send(self, data):
+        if isinstance(data, compat_str):
+            data = data.encode('utf-8')
+        self.proc.stdin.write(data)
+        self.proc.stdin.write(b'\n')
+        self.proc.stdin.flush()
+
+    @__read_stderr
+    def recv(self):
+        while True:
+            ret = self.proc.stdout.readline()
+            if isinstance(ret, bytes):
+                ret = ret.decode('utf-8')
+            ret = ret.strip()
+            if ret:
+                return ret
+
+    @__read_stderr
+    def close(self):
+        self.proc.kill()
+        self.proc.terminate()
+        self.proc = None
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type, value, traceback):
+        self.close()
+
+
+AVAILABLE = bool(check_executable('websocat', ['-h']))
diff --git a/youtube_dl/websocket/websockets.py b/youtube_dl/websocket/websockets.py
new file mode 100644
index 000000000..488fc1675
--- /dev/null
+++ b/youtube_dl/websocket/websockets.py
@@ -0,0 +1,72 @@
+from __future__ import unicode_literals
+
+import asyncio
+import websockets
+
+
+# taken from https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py with modifications
+def run_with_loop(main, loop):
+    if asyncio.events._get_running_loop() is not None:
+        raise RuntimeError(
+            "asyncio.run() cannot be called from a running event loop")
+
+    if not asyncio.coroutines.iscoroutine(main):
+        raise ValueError("a coroutine was expected, got {!r}".format(main))
+
+    try:
+        # asyncio.events.set_event_loop(loop)
+        return loop.run_until_complete(main)
+    finally:
+        loop.run_until_complete(loop.shutdown_asyncgens())
+        if hasattr(loop, 'shutdown_default_executor'):
+            loop.run_until_complete(loop.shutdown_default_executor())
+
+
+def _cancel_all_tasks(loop):
+    to_cancel = asyncio.tasks.all_tasks(loop)
+    if not to_cancel:
+        return
+
+    for task in to_cancel:
+        task.cancel()
+
+    loop.run_until_complete(
+        asyncio.tasks.gather(*to_cancel, loop=loop, return_exceptions=True))
+
+    for task in to_cancel:
+        if task.cancelled():
+            continue
+        if task.exception() is not None:
+            loop.call_exception_handler({
+                'message': 'unhandled exception during asyncio.run() shutdown',
+                'exception': task.exception(),
+                'task': task,
+            })
+
+
+class WebSocketsWrapper():
+    "Wraps websockets module to use in non-async scopes"
+
+    def __init__(self, url, headers=None):
+        # self.loop = asyncio.events.get_event_loop() or asyncio.events.new_event_loop()
+        self.loop = asyncio.events.new_event_loop()
+        self.conn = websockets.connect(
+            url, extra_headers=headers, ping_interval=None,
+            close_timeout=float('inf'), loop=self.loop, ping_timeout=float('inf'))
+
+    def __enter__(self):
+        self.pool = run_with_loop(self.conn.__aenter__(), self.loop)
+        return self
+
+    def send(self, *args):
+        run_with_loop(self.pool.send(*args), self.loop)
+
+    def recv(self, *args):
+        return run_with_loop(self.pool.recv(*args), self.loop)
+
+    def __exit__(self, type, value, traceback):
+        try:
+            return run_with_loop(self.conn.__aexit__(type, value, traceback), self.loop)
+        finally:
+            self.loop.close()
+            _cancel_all_tasks(self.loop)
diff --git a/ytdl.py b/ytdl.py
new file mode 100755
index 000000000..9db04b981
--- /dev/null
+++ b/ytdl.py
@@ -0,0 +1,24 @@
+#!/usr/bin/env python3
+# coding: utf-8
+from __future__ import unicode_literals
+
+# Quick script for starting youtube-dl within this repository
+# DO NOT USE THIS IN PRODUCTION
+
+import os
+import os.path
+import sys
+import random
+sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
+
+# assign sys.argv[0] anything better if it's None or ''
+#  or it somehow break Jython
+if not sys.argv[0]:
+    sys.argv[0] = 'youtube-dl'
+
+if random.randint(0, 10) == 7:
+    sys.stderr.write('REMAINDER: This script is for testing purposes. To suppress this message, use distributed binaries.\n')
+
+import youtube_dl
+
+youtube_dl.main()
